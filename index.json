
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    [{"authors":null,"categories":null,"content":"通过 OAuth2 实现单点登陆和应用授权。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前置条件 以下内容全部运行于 kubernetes 环境。\n安装 OpenID Connect Provider 这里使用 dex 作为 OpenID Connect Provider 并且通过 helm 进行安装。\nhelm repo add dex https://charts.dexidp.io helm upgrade --install dex dex/dex --create-namespace --namespace dex helm upgrade dex dex/dex --namespace dex --values dex.yaml dex.yaml 的主要内容是 dex 的配置文件，可以在这个文件中对资源进行更多定义。\nimage: tag: v2.38.0 config: issuer: http://dex.lan/ storage: type: memory oauth2: skipApprovalScreen: true responseTypes: [\u0026#34;code\u0026#34;, \u0026#34;id_token\u0026#34;, \u0026#34;token\u0026#34;] alwaysShowLoginScreen: false connectors: - type: ldap id: ldap name: ldap config: host: openldap-service.openldap-namespace:1389 insecureNoSSL: true bindDN: cn=users,dc=example,dc=com bindPW: password usernamePrompt: Username userSearch: baseDN: cn=users,dc=example,dc=com filter: \u0026#34;(objectClass=person)\u0026#34; username: cn idAttr: uid emailAttr: mail nameAttr: name preferredUsernameAttr: cn staticClients: - id: oauth2-proxy redirectURIs: - \u0026#34;http://app.lan/oauth2/callback\u0026#34; name: \u0026#34;oauth2-proxy\u0026#34; secret: REDACTED v2.39.1 版本的 dex 似乎无法正常实现 skipApprovalScreen 配置，降级到 v2.38.0 版本使用，可以参考这个 issue 。\n具体的用户数据从 openldap 中获取，可以参考这里快速搭建 openldap 。\n安装 OAuth2 proxy 应用本身不具备 oauth2 的认证功能的情况下，需要额外部署应用来实现这部分功能。\nhelm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests helm install oauth2-proxy oauth2-proxy/oauth2-proxy --create-namespace --namespace oauth2-proxy helm upgrade oauth2-proxy oauth2-proxy/oauth2-proxy --namespace oauth2-proxy --values oauth2-proxy.yml oauth2-proxy.yml 的主要内容如下：\nconfig: clientID: \u0026#34;oauth2-proxy\u0026#34; clientSecret: \u0026#34;REDACTED\u0026#34; configFile: |- redirect_url = \u0026#34;http://app.lan/oauth2/callback\u0026#34; provider = \u0026#34;oidc\u0026#34; oidc_issuer_url = \u0026#34;http://dex.lan/\u0026#34; upstreams = [ \u0026#34;http://app-service.app-namespace:8080\u0026#34; ] email_domains = [ \u0026#34;*\u0026#34; ] cookie_secure = false cookie_secret = \u0026#34;REDACTED\u0026#34; proxy_websockets = true 这里的 upstream 是集群中已有的服务，除了 dex 之外，也可以选择不同的 OpenID Connect Provider ，根据 oauth2-proxy 的官方文档，使用 dex 作为 provider 时需要关闭 cookie_secure 。\n可以看到这里用到了两个域名 dex.lan 和 app.lan ，其中 dex.lan 是专门留给 dex 对外服务使用的，因为在用户没有经过登陆授权时，需要导向到 dex 进行用户登陆，所以这个服务需要对外暴露。而 app.lan 则是实际应用的入口，外部流量经由它代理到具体的上游服务。\n","date":1720695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"a8af250dd536c6b06614acf03798dea1","permalink":"https://yuweizzz.github.io/post/using_oauth2_for_authorization/","publishdate":"2024-07-11T11:00:00Z","relpermalink":"/post/using_oauth2_for_authorization/","section":"post","summary":"通过 OAuth2 实现单点登陆和应用授权。\n","tags":["Kubernetes","OAuth2","OpenID Connect"],"title":"通过 OAuth2 进行授权","type":"post"},{"authors":null,"categories":null,"content":"姐姐，今夜我在德令哈，夜色笼罩\n姐姐，今夜我只有戈壁\n草原尽头我两手空空\n悲痛时握不住一颗泪滴\n姐姐，今夜我在德令哈\n这是雨水中一座荒凉的城\n除了那些路过的和居住的\n德令哈……今夜\n这是惟一的，最后的，抒情\n这是惟一的，最后的，草原\n我把石头还给石头\n让胜利的胜利\n今夜青稞只属于她自己\n一切都在生长\n今夜我只有美丽的戈壁空空\n姐姐，今夜我不关心人类，我只想你\nIn memory of 2024/6/25\n","date":1719351885,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"f84f82c2bf8f720cf9acde8486018030","permalink":"https://yuweizzz.github.io/post/tonight_i_am_in_delingha/","publishdate":"2024-06-25T21:44:45Z","relpermalink":"/post/tonight_i_am_in_delingha/","section":"post","summary":"","tags":["Life"],"title":"今夜我在德令哈","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 tekton 在 kubernetes 集群中的基本使用。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 安装 tekton # install tekton pipeline curl https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml -o pipeline.yaml kubectl apply -f pipeline.yaml # install tekton dashboard curl https://storage.googleapis.com/tekton-releases/dashboard/latest/release-full.yaml -o dashboard.yaml kubectl apply -f dashboard.yaml # install tekton triggers curl https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml -o triggers.yaml curl https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml -o interceptors.yaml kubectl apply -f triggers.yaml kubectl apply -f interceptors.yaml 使用 task 和 taskrun 运行单次任务 最基本的运行资源是由 task 和 taskrun 组成的，比如我们可以通过 tekton hub 提供的 git-clone task ，来执行单次代码克隆。\ngit-clone.yaml # git-clone v0.7 # https://hub.tekton.dev/tekton/task/git-clone apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: git-clone labels: app.kubernetes.io/version: \u0026#34;0.7\u0026#34; annotations: tekton.dev/pipelines.minVersion: \u0026#34;0.29.0\u0026#34; tekton.dev/categories: Git tekton.dev/tags: git tekton.dev/displayName: \u0026#34;git clone\u0026#34; tekton.dev/platforms: \u0026#34;linux/amd64,linux/s390x,linux/ppc64le,linux/arm64\u0026#34; spec: description: \u0026gt;- These Tasks are Git tasks to work with repositories used by other tasks in your Pipeline. The git-clone Task will clone a repo from the provided url into the output Workspace. By default the repo will be cloned into the root of your Workspace. You can clone into a subdirectory by setting this Task\u0026#39;s subdirectory param. This Task also supports sparse checkouts. To perform a sparse checkout, pass a list of comma separated directory patterns to this Task\u0026#39;s sparseCheckoutDirectories param. workspaces: - name: output description: The git repo will be cloned onto the volume backing this Workspace. - name: ssh-directory optional: true description: | A .ssh directory with private key, known_hosts, config, etc. Copied to the user\u0026#39;s home before git commands are executed. Used to authenticate with the git remote when performing the clone. Binding a Secret to this Workspace is strongly recommended over other volume types. - name: basic-auth optional: true description: | A Workspace containing a .gitconfig and .git-credentials file. These will be copied to the user\u0026#39;s home before any git commands are run. Any other files in this Workspace are ignored. It is strongly recommended to use ssh-directory over basic-auth whenever possible and to bind a Secret to this Workspace over other volume types. - name: ssl-ca-directory optional: true description: | A workspace containing CA certificates, this will be used by Git to verify the peer with when fetching or pushing over HTTPS. params: - name: url description: Repository URL to clone from. type: string - name: revision description: Revision to checkout. (branch, tag, sha, ref, etc...) type: string default: \u0026#34;\u0026#34; - name: refspec description: Refspec to fetch before checking out revision. default: \u0026#34;\u0026#34; - name: submodules description: Initialize and fetch git submodules. type: string default: \u0026#34;true\u0026#34; - name: depth description: Perform a shallow clone, fetching only the most recent N commits. type: string default: \u0026#34;1\u0026#34; - name: sslVerify description: Set the `http.sslVerify` global git config. Setting this to `false` is not advised unless you are sure that you trust your git remote. type: string default: \u0026#34;true\u0026#34; - name: crtFileName description: file name of mounted crt using ssl-ca-directory workspace. default value is ca-bundle.crt. type: string default: \u0026#34;ca-bundle.crt\u0026#34; - name: subdirectory description: Subdirectory inside the `output` Workspace to clone the repo into. type: string default: \u0026#34;\u0026#34; - name: sparseCheckoutDirectories description: Define the directory patterns to match or exclude when performing a sparse checkout. type: string default: \u0026#34;\u0026#34; - name: deleteExisting description: Clean out the contents of the destination directory if it already exists before cloning. type: string default: \u0026#34;true\u0026#34; - name: httpProxy description: HTTP proxy server for non-SSL requests. type: string default: \u0026#34;\u0026#34; - name: httpsProxy description: HTTPS proxy server for SSL requests. type: string default: \u0026#34;\u0026#34; - name: noProxy description: Opt out of proxying HTTP/HTTPS requests. type: string default: \u0026#34;\u0026#34; - name: verbose description: Log the commands that are executed during `git-clone`\u0026#39;s operation. type: string default: \u0026#34;true\u0026#34; - name: gitInitImage description: The image providing the …","date":1715271285,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"0e6c3aa96a017c406cc3e951004de375","permalink":"https://yuweizzz.github.io/post/use_tekton_to_run_pipelines_in_kubernetes_cluster/","publishdate":"2024-05-09T16:14:45Z","relpermalink":"/post/use_tekton_to_run_pipelines_in_kubernetes_cluster/","section":"post","summary":"这篇笔记用来记录 tekton 在 kubernetes 集群中的基本使用。\n","tags":["Tekton","Kubernetes"],"title":"使用 tekton 在 kubernetes 集群中运行流水线","type":"post"},{"authors":null,"categories":null,"content":"搭建 k3s 集群，同时使用 Cilium 和 APISIX Ingress Controller 。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 安装 k3s # 使用 etcd 作为配置中心 apt install etcd-server etcd-client # 安装 k3s # 禁用网络策略 --disable-network-policy # 修改配置中心 --datastore-endpoint # 启用内置 iptables 替换系统使用的版本 --prefer-bundled-bin # 禁用组件 kube-proxy, flannel, traefik, metrics-server, servicelb export INSTALL_K3S_EXEC=\u0026#39;--datastore-endpoint=http://127.0.0.1:2379 --disable-network-policy --flannel-backend=none --disable-kube-proxy --disable=traefik --disable=servicelb --disable=metrics-server --prefer-bundled-bin\u0026#39; # 禁用 metrics-server 可能会导致 HPA 不可用，应该根据项目情况来禁用这一选项。 # 执行安装 curl -sfL https://get.k3s.io | sh - # 中国用户可以通过以下命令加速安装，避免出现下载失败的问题 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh - 因为后续会使用 cilium 替换默认的 flannel 网络插件，需要先使用 --disable-network-policy 和 --flannel-backend=none 禁止 k3s 自动安装网络插件。同时会通过 cilium 来彻底替换 kube-proxy ，所以会使用 --disable-kube-proxy 禁止 k3s 启动相关的组件。\ntraefik 的功能由 Apisix Ingress 接管， servicelb 的功能则通过 cilium 的 L2 Announcement 实现。\n相应的 --datastore-endpoint 具体信息记录在 /var/lib/rancher/k3s/server/db ，而 k3s 集群创建的 local-path 类型的 PVC 则存储在 /var/lib/rancher/k3s/storage 。\n通过修改 k3s registries 配置来解决 docker 被墙后的镜像拉取错误问题。\n# 添加 /etc/rancher/k3s/registries.yaml cat /etc/rancher/k3s/registries.yaml mirrors: docker.io: endpoint: - \u0026#34;https://docker.m.daocloud.io\u0026#34; # 重启 k3s systemctl restart k3s 安装 cilium # 安装 helm curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | sh - # 安装 cilium helm repo add cilium https://helm.cilium.io/ helm repo update export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm upgrade --install cilium cilium/cilium --version 1.15.1 \\ --create-namespace --namespace cilium \\ --set operator.replicas=1 \\ --set ipam.mode=cluster-pool \\ --set ipam.operator.clusterPoolIPv4PodCIDRList=10.0.0.0/16 \\ --set routingMode=native \\ --set autoDirectNodeRoutes=true \\ --set ipv4NativeRoutingCIDR=10.0.0.0/16 \\ --set bpf.masquerade=true \\ --set kubeProxyReplacement=true \\ --set loadBalancer.mode=dsr \\ --set loadBalancer.acceleration=native \\ --set hubble.relay.enabled=true \\ --set hubble.ui.enabled=true \\ --set installNoConntrackIptablesRules=true cilium helm 参数可以参考以下信息：\noperator.replicas 的默认值是 2 ，这里使用 1 个副本就足够了。 ipam.mode 默认就是 cluster-pool 模式，相应的要通过 ipam.operator.clusterPoolIPv4PodCIDRList 用来设置允许分配的 Pod CIDR 范围。 设置之后可以通过 CiliumNode 查看具体的 CIDR 分配，具体节点上的 CIDR 由 cilium operator 分配。\nroutingMode 使用 native 模式，也就是 cilium 的 Native-Routing 模式， native 模式的性能高于默认的 tunnel 模式， ipv4NativeRoutingCIDR 用于声明 Pod CIDR ， cilium 会认为这部分的流量是可以由系统网络直接路由，不需要使用 SNAT ， autoDirectNodeRoutes 允许 cilium 修改节点的路由信息，使得 Pod CIDR 可以被直接路由，适用于所有 worker 节点处在同一 L2 网络。 bpf.masquerade 设置为 true 时，会使用 ebpf 来实现地址伪装，否则默认是通过 iptables 去实现的，主要针对 Pod 内部和集群外部之间的流量。 kubeProxyReplacement 设置为 true 允许 cilium 使用内置功能替换 kube-proxy 。 loadBalancer.mode 设置为 dsr 是针对 NodePort 类型的 ebpf 增强，允许通过 ebpf 修改数据包，使得 Pod 不在当前节点的流量可以直接返回客户端，而不是通过 SNAT 进行跳转。 loadBalancer.acceleration 是优化选项，设置为 native 允许数据包通过 XDP 直接在网卡驱动部分处理，而不是系统内核，可以提高性能。 虚拟机上的某些网卡驱动可能不支持 XDP ，这时 loadBalancer.acceleration 的 native 模式不可用，直接去掉即可。\ninstallNoConntrackIptablesRules 设置为 true 可以禁用流量的连接跟踪，也就是 iptables 提供的流量追踪功能，可以提高性能。 hubble.relay.enabled 和 hubble.ui.enabled 是可观测性的相关部分，它们都是默认关闭的，这里重新启用了它们。其中 ui 是对应的图形界面， relay 是 hubble 的中继服务，用来沟通不同 hubble 实例和提供更多的 API 支持，开启 hubble 会略微降低一些性能。 关于性能优化的部分可以参考官方文档 ，关于 Host-Routing ， installNoConntrackIptablesRules 等性能优化的选项都可以找到对应说明。\n正常安装后通过 cilium pod 查看安装信息和配置情况。\n# 查看 cilium 状态 kubectl exec -it -n cilium cilium-5r2nh -- cilium status --verbose # 以下是命令的输出结果，已经省略了部分内容 KubeProxyReplacement: True [ens192 192.168.10.100 fe80::20c:29ff:fe89:c031 (Direct Routing)] Cilium: Ok 1.15.1 (v1.15.1-a368c8f0) # Host Routing 在内核支持 ebpf 和使用 KubeProxyReplacement 等条件下会使用 BPF ，默认是 Legacy Host Routing: BPF # bpf.masquerade 需要设置为 true ，否则此处使用 iptables Masquerading: BPF [ens192] 10.0.0.0/16 [IPv4: Enabled, IPv6: Disabled] # 开启 Hubble Hubble: Ok Current/Max Flows: 4095/4095 (100.00%), Flows/s: 11.27 Metrics: Disabled KubeProxyReplacement Details: # kubeProxyReplacement 需要设置为 true Status: True Socket LB: Enabled Socket LB Tracing: Enabled Socket LB Coverage: Full Devices: ens192 192.168.10.100 fe80::20c:29ff:fe89:c031 (Direct Routing) # loadBalancer.mode 需要设置为 dsr Mode: DSR DSR Dispatch Mode: IP Option/Extension Backend Selection: Random Session Affinity: Enabled Graceful Termination: Enabled NAT46/64 Support: Disabled # 如果 loadBalancer.acceleration 设置为 native ，此处应该会被启用 XDP Acceleration: …","date":1713455085,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"c9daabef181ae2aeb20291095b93c84c","permalink":"https://yuweizzz.github.io/post/install_k3s_cluster/","publishdate":"2024-04-18T15:44:45Z","relpermalink":"/post/install_k3s_cluster/","section":"post","summary":"搭建 k3s 集群，同时使用 Cilium 和 APISIX Ingress Controller 。\n","tags":["K3s","Cilium","Apisix"],"title":"搭建 k3s 集群","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Openresty 开发的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 定义 lua_package_path 在引入一系列自定义的 lua package 的时候比较有用，事实上大部分基于 openresty 的网关项目都是这样做的。\n# nginx.conf http { lua_package_path \u0026#39;/usr/local/src/lua-resty-http/?.lua;;\u0026#39;; lua_package_cpath \u0026#39;/usr/local/src/lua-cjson/?.so;;\u0026#39;; } body_filter_by_lua_block 的流式处理 openresty 请求响应可能会经过多次 body_filter_by_lua_block 阶段。其中在 body_filter_by_lua_block 阶段的 ngx.arg[1] 和 ngx.arg[2] 参数分别代表响应内容和响应结束标记。\n-- apisix brotli plugin -- body_filter 函数会在 body_filter_by_lua_block 中执行 function _M.body_filter(conf, ctx) if not ctx.brotli_matched then return end local chunk, eof = ngx.arg[1], ngx.arg[2] if type(chunk) == \u0026#34;string\u0026#34; and chunk ~= \u0026#34;\u0026#34; then local encode_chunk = ctx.compressor:compress(chunk) ngx.arg[1] = encode_chunk .. ctx.compressor:flush() end if eof then ngx.arg[1] = ngx.arg[1] .. ctx.compressor:finish() end end eof 标记为 true 时 chunk 内容不一定为空，此时的 chunk 就是最后的响应内容。在某些响应体内容修改的场景中，可能需要将所有 ngx.arg[1] 内容进行合并后在处理，并且应该在 header_filter_by_lua_block 阶段将 ngx.header.content_length 置空。\n-- 合并 ngx.arg[1] 可以参考 apisix hold_body_chunk 函数 -- apisix/core/response.lua local arg = ngx.arg function _M.hold_body_chunk(ctx, hold_the_copy) local body_buffer local chunk, eof = arg[1], arg[2] if not ctx._body_buffer then ctx._body_buffer = {} end if type(chunk) == \u0026#34;string\u0026#34; and chunk ~= \u0026#34;\u0026#34; then body_buffer = ctx._body_buffer[ctx._plugin_name] if not body_buffer then body_buffer = { chunk, n = 1 } ctx._body_buffer[ctx._plugin_name] = body_buffer else local n = body_buffer.n + 1 body_buffer.n = n body_buffer[n] = chunk end end if eof then body_buffer = ctx._body_buffer[ctx._plugin_name] if not body_buffer then return chunk end body_buffer = concat_tab(body_buffer, \u0026#34;\u0026#34;, 1, body_buffer.n) ctx._body_buffer[ctx._plugin_name] = nil return body_buffer end if not hold_the_copy then -- flush the origin body chunk arg[1] = nil end return nil end proxy_next_upstream 和 balancer_by_lua_block proxy_next_upstream 是 ngx_http_proxy_module 提供的关键字，具体用法参考 proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 | http_403 | http_404 | http_429 | non_idempotent | off ...; ，默认定义是 proxy_next_upstream error timeout; 。它定义了是否在对应的状态下将请求发送到下一个 upstream ，实际上就是 nginx 的重试机制。\n在 openresty 中，这部分可以配合 balancer_by_lua_block 来自定义上游的选择。但是只有在访问上游出现的错误符合所定义的条件时，才会再次进入 balancer_by_lua_block 执行阶段。\n# openresty balancer_by_lua_block doc http { upstream backend { server 0.0.0.1; # just an invalid address as a place holder balancer_by_lua_block { local balancer = require \u0026#34;ngx.balancer\u0026#34; -- well, usually we calculate the peer\u0026#39;s host and port -- according to some balancing policies instead of using -- hard-coded values like below local host = \u0026#34;127.0.0.2\u0026#34; local port = 8080 local ok, err = balancer.set_current_peer(host, port) if not ok then ngx.log(ngx.ERR, \u0026#34;failed to set the current peer: \u0026#34;, err) return ngx.exit(500) end } keepalive 10; # connection pool } server { # this is the real entry point listen 80; location / { # make use of the upstream named \u0026#34;backend\u0026#34; defined above: proxy_pass http://backend/fake; } } server { # this server is just for mocking up a backend peer here... listen 127.0.0.2:8080; location = /fake { echo \u0026#34;this is the fake backend peer...\u0026#34;; } } } 进入 balancer_by_lua_block 阶段后可以通过 ngx.balancer.set_more_tries 来设置新增的重试次数，会在原有的重试次数上增加。所以应该额外设置 proxy_next_upstream_tries 或者 proxy_next_upstream_timeout ，否则可能出现无限重试的情况。而 ngx.balancer.get_last_failure 可以拿到进入本次 balancer_by_lua_block 阶段的原因以及状态码。\nLuajit string buffer Luajit 提供了一个高效的 string buffer 模块，可以实现 FIFO 的字符缓存队列。\nstr_buffer = require(\u0026#34;string.buffer\u0026#34;) strings = \u0026#34;abcdefghij\u0026#34; -- new 方法可以指定给定的内存空间大小，但无论是否给定，后续内存空间都会自动适应增长 -- set 方法可以直接设置数据内容 buffer = str_buffer.new():set(strings) print(buffer:get(5)) -- get 方法会将数据从队列中取出 -- out: -- abcde print(buffer:tostring()) -- tostring 方法可以输出当前队列内容，并且不修改原有队列 -- out: -- fghij print(buffer:get(5)) -- out: -- fghij -- 取完所有数据，后续的 get 和 tostring 都会返回 \u0026#34;\u0026#34; if buffer:get(1) == \u0026#34;\u0026#34; then print([[get \u0026#34;\u0026#34;]]) --- out: --- get \u0026#34;\u0026#34; end -- buffer 可以复用 buffer:put(\u0026#34;12345\u0026#34;) print(buffer:get(5)) -- out: -- 12345 -- reset 方法可以将 buffer 置空，相当于取出所有数据，但内存空间不会释放 buffer:reset() -- free 方法可以主动释放内存空间 buffer:free() ngx.re module 在 openresty 中，正则相关的处理一般使用 ngx.re 模块。这个模块提供的函数和 Lua string 非常接近，同样都有 find ， match ， gmatch ， sub ， gsub 这几个函数。\nlocation /find { content_by_lua_block { local regex = [[\\d+]] -- find 同样是找到匹配式在目标字符串中的起止位置 local from, to, err = ngx.re.find(\u0026#34;hello, 1234\u0026#34;, regex, \u0026#34;jo\u0026#34;) if err then ngx.log(ngx.ERR, \u0026#34;error: \u0026#34; .. …","date":1712173485,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"f1a8d05c6f69e241316451b4016c7e1c","permalink":"https://yuweizzz.github.io/post/study_notes_about_openresty/","publishdate":"2024-04-03T19:44:45Z","relpermalink":"/post/study_notes_about_openresty/","section":"post","summary":"这篇笔记用来记录一些 Openresty 开发的相关知识。\n","tags":["Lua","Openresty","Apisix"],"title":"openresty 学习笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记主要记录如何自行编写 Prometheus exporter 。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 本篇根据网络上找到的样板修改而来，首先通过 go mod init myexporter 来创建模块，具体的代码结构如下：\n. ├── collector │ └── http.go ├── go.mod ├── go.sum └── main.go http.go 代码内容如下：\npackage collector import ( \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type StatusCollector struct { requestDesc *prometheus.Desc mutex sync.Mutex } func NewStatusCollector() prometheus.Collector { return \u0026amp;StatusCollector{ requestDesc: prometheus.NewDesc( \u0026#34;http_request_status\u0026#34;, \u0026#34;The Status Code of http request\u0026#34;, nil, prometheus.Labels{\u0026#34;url\u0026#34;: \u0026#34;https://www.baidu.com\u0026#34;}), } } func (n *StatusCollector) Describe(ch chan\u0026lt;- *prometheus.Desc) { ch \u0026lt;- n.requestDesc } func (n *StatusCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { n.mutex.Lock() client := http.Client{ Timeout: 5 * time.Second, } resp, err := client.Head(\u0026#34;https://www.baidu.com\u0026#34;) if err != nil { ch \u0026lt;- prometheus.MustNewConstMetric(n.requestDesc, prometheus.GaugeValue, 0) n.mutex.Unlock() return } ch \u0026lt;- prometheus.MustNewConstMetric(n.requestDesc, prometheus.GaugeValue, float64(resp.StatusCode)) n.mutex.Unlock() } main.go 代码内容如下：\npackage main import ( \u0026#34;myexporter/collector\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { r := prometheus.NewRegistry() r.MustRegister(collector.NewStatusCollector()) handler := promhttp.HandlerFor(r, promhttp.HandlerOpts{}) http.Handle(\u0026#34;/metrics\u0026#34;, handler) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { fmt.Printf(\u0026#34;Error occur when start server %v\u0026#34;, err) } } 代码编写完成后，直接通过 go get 和 go build 下载依赖模块和编译代码，就可以得到可执行文件。\n建议通过 CGO_ENABLED=0 go build -ldflags=\u0026#34;-w -s\u0026#34; main.go 进行编译，可以得到静态链接的可执行文件并且文件体积比较小。\n","date":1711891560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"164bee6b72db941b9ea34be5ce50dacb","permalink":"https://yuweizzz.github.io/post/example_of_prometheus_exporter/","publishdate":"2024-03-31T13:26:00Z","relpermalink":"/post/example_of_prometheus_exporter/","section":"post","summary":"这篇笔记主要记录如何自行编写 Prometheus exporter 。\n","tags":["Prometheus","Go"],"title":"Prometheus exporter 代码参考","type":"post"},{"authors":null,"categories":null,"content":"通过 cloudflare API 更新 DNS 记录的 python 脚本。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ import urllib.parse import urllib.request import json headers = { \u0026#39;Authorization\u0026#39;: \u0026#39;Bearer TOKEN_CONTENT\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, } def get_zone_id(domain): url = \u0026#39;https://api.cloudflare.com/client/v4/zones\u0026#39; req = urllib.request.Request(url, data=None, headers=headers) with urllib.request.urlopen(req) as response: body = json.loads(response.read().decode(\u0026#39;utf-8\u0026#39;)) for i in body.get(\u0026#39;result\u0026#39;): if i.get(\u0026#39;name\u0026#39;) == domain: return i.get(\u0026#39;id\u0026#39;) def get_dns_record_id(domain_id, record): url = f\u0026#39;https://api.cloudflare.com/client/v4/zones/{domain_id}/dns_records\u0026#39; req = urllib.request.Request(url, data=None, headers=headers) with urllib.request.urlopen(req) as response: body = json.loads(response.read().decode(\u0026#39;utf-8\u0026#39;)) for i in body.get(\u0026#39;result\u0026#39;): if i.get(\u0026#39;name\u0026#39;) == record: return i.get(\u0026#39;id\u0026#39;) def put_dns_record_A(domain_id, record_id, name, content): url = f\u0026#39;https://api.cloudflare.com/client/v4/zones/{domain_id}/dns_records/{record_id}\u0026#39; data = { \u0026#39;type\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;name\u0026#39;: name, \u0026#39;content\u0026#39;: content, \u0026#39;ttl\u0026#39;: 60, \u0026#39;proxied\u0026#39;: False, } req = urllib.request.Request(url, data=json.dumps(data).encode(), headers=headers, method=\u0026#39;PUT\u0026#39;) with urllib.request.urlopen(req) as response: body = json.loads(response.read().decode(\u0026#39;utf-8\u0026#39;)) return body # def get_ip_addr() # req = urllib.request.Request(\u0026#39;http://4.ipw.cn\u0026#39;) # with urllib.request.urlopen(req) as response: # body = response.read().decode(\u0026#39;utf-8\u0026#39;) # return body zone = \u0026#39;yourdomain.com\u0026#39; record_name = \u0026#39;sub.yourdomain.com\u0026#39; # new_addr = get_ip_addr() new_addr = \u0026#39;1.1.1.1\u0026#39; zone_id = get_zone_id(zone) dns_record_id = get_dns_record_id(zone_id, record_name) put_dns_record_A(zone_id, dns_record_id, record_name, new_addr) ","date":1708268400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5d05e8772d710f70e1c09edd58feefea","permalink":"https://yuweizzz.github.io/post/update_dns_record_via_cloudflare_api/","publishdate":"2024-02-18T15:00:00Z","relpermalink":"/post/update_dns_record_via_cloudflare_api/","section":"post","summary":"通过 cloudflare API 更新 DNS 记录的 python 脚本。\n","tags":["cloudflare","dns","python"],"title":"通过 cloudflare api 更新 DNS 记录","type":"post"},{"authors":null,"categories":null,"content":"不见昨夜雨湿处，聊以新颜待今朝。\n","date":1707523201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"d7a1c51809ad83377677e6e28298b923","permalink":"https://yuweizzz.github.io/post/hello_2024/","publishdate":"2024-02-10T00:00:01Z","relpermalink":"/post/hello_2024/","section":"post","summary":"不见昨夜雨湿处，聊以新颜待今朝。","tags":null,"title":"Hello 2024","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Debian 系统中的一些实用技巧。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 配置 Debian 源 dpkg 和 apt 是 Debian 系列 Linux 发行版的软件包管理器，其中 dpkg 是比较底层的软件包工具，而 apt 则是更高层级的管理工具，两者之间的关系类似于 rpm 和 yum 的关系。\n# 修改 Debian 10 buster 镜像源 $ echo \u0026#39;deb http://mirrors.ustc.edu.cn/debian buster main contrib non-free deb http://mirrors.ustc.edu.cn/debian buster-updates main contrib non-free deb http://mirrors.ustc.edu.cn/debian buster-backports main contrib non-free deb http://mirrors.ustc.edu.cn/debian-security/ buster/updates main contrib non-free\u0026#39; \\ \u0026gt; /etc/apt/sources.list # 更新软件列表 $ apt update 配置不稳定镜像源获取最新版本软件 为了能够使用一些版本比较高的软件，有时候可能需要将镜像源配置为不稳定版本，在 Debian 中不稳定版本的镜像源代号为 sid ，这个镜像源通常包括了一些最新版本但是还不稳定的软件。\n# 添加 Debian sid 镜像源 $ echo deb https://mirrors.aliyun.com/debian/ sid main contrib non-free non-free-firmware \\ \u0026gt; /etc/apt/sources.list.d/sid.list # 更新软件列表 $ apt update 配置镜像源的优先级 虽然可以通过添加 sid 镜像源来获取最新软件，但是这个操作是有一定危险性的，所以在添加 sid 源后千万不要使用 apt upgrade 更新所有软件，最好在安装完需要的某部分软件后，把 sid 源的优先级降低。\n# 添加镜像源的优先级配置 $ cat /etc/apt/preferences.d/sid Package: * Pin: release o=Debian,a=unstable,n=sid Pin-Priority: 50 # 更新软件列表 $ apt update # 默认的镜像源优先级是 500 ，越大的优先级会被优先使用 # 以下输出是一台调整过优先级的 Debian 11 系统的运行信息，可以看到 sid 源的优先级已经被降低 $ apt-cache policy Package files: 100 /var/lib/dpkg/status release a=now 50 https://mirrors.aliyun.com/debian sid/non-free-firmware amd64 Packages release o=Debian,a=unstable,n=sid,l=Debian,c=non-free-firmware,b=amd64 origin mirrors.aliyun.com 50 https://mirrors.aliyun.com/debian sid/non-free amd64 Packages release o=Debian,a=unstable,n=sid,l=Debian,c=non-free,b=amd64 origin mirrors.aliyun.com 50 https://mirrors.aliyun.com/debian sid/contrib amd64 Packages release o=Debian,a=unstable,n=sid,l=Debian,c=contrib,b=amd64 origin mirrors.aliyun.com 50 https://mirrors.aliyun.com/debian sid/main amd64 Packages release o=Debian,a=unstable,n=sid,l=Debian,c=main,b=amd64 origin mirrors.aliyun.com 500 http://deb.debian.org/debian bullseye-updates/main amd64 Packages release v=11-updates,o=Debian,a=oldstable-updates,n=bullseye-updates,l=Debian,c=main,b=amd64 origin deb.debian.org 500 http://security.debian.org/debian-security bullseye-security/main amd64 Packages release v=11,o=Debian,a=oldstable-security,n=bullseye-security,l=Debian-Security,c=main,b=amd64 origin security.debian.org 500 http://deb.debian.org/debian bullseye/main amd64 Packages release v=11.8,o=Debian,a=oldstable,n=bullseye,l=Debian,c=main,b=amd64 origin deb.debian.org 一些重要软件的独立镜像源 这里会记录一些重要软件的独立镜像源，以便需要时能够快速安装。\n# jenkins Debian LTS release $ curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key \\ \u0026gt; /usr/share/keyrings/jenkins-keyring.asc $ echo \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/\u0026#34; \\ \u0026gt; /etc/apt/sources.list.d/jenkins.list $ apt update $ apt install jenkins # docker-ce $ curl -fsSL https://download.docker.com/linux/debian/gpg \\ \u0026gt; /usr/share/keyrings/docker-ce-keyring.asc $ echo \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/usr/share/keyrings/docker-ce-keyring.asc] \\ https://download.docker.com/linux/debian \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; \\ \u0026gt; /etc/apt/sources.list.d/docker-ce.list $ apt update $ apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin # openresty $ curl -fsSL https://openresty.org/package/pubkey.gpg \\ \u0026gt; /usr/share/keyrings/openresty-keyring.asc $ echo \u0026#34;deb [signed-by=/usr/share/keyrings/openresty-keyring.asc] http://openresty.org/package/debian \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; openresty\u0026#34; \\ \u0026gt; /etc/apt/sources.list.d/openresty.list $ apt update $ apt install openresty # mysql $ wget https://dev.mysql.com/get/mysql-apt-config_0.8.15-1_all.deb # mysql-apt-config 实际上就是安装对应的 apt 镜像源和签名信息的步骤 # 0.8.15 版本的 mysql-apt-config 可以下载安装 mysql 5.7 和 mysql 8.0 # 新版本的 mysql-apt-config 只能下载安装 mysql 8.0 $ dpkg -i mysql-apt-config_0.8.15-1_all.deb # 安装完成后会跳出图形界面，可以自行选择需要的镜像源，这里选择的是 mysql 5.7 # 配置完成后想要修改的话可以通过以下命令重新打开 $ dpkg-reconfigure mysql-apt-config # 执行实际的安装步骤 $ apt update $ apt install mysql-community-server 调整网卡配置 Debian 系统使用的网卡配置文件是 /etc/network/interfaces 和 /etc/network/interfaces.d/* ，一般会在这里调整网卡的 IP 地址获取行为。\n# 默认的 /etc/network/interfaces 配置文件 $ cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary …","date":1703343600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"d49d324c852be92ddd8a55b4679c8cd4","permalink":"https://yuweizzz.github.io/post/practical_tips_in_debian/","publishdate":"2023-12-23T15:00:00Z","relpermalink":"/post/practical_tips_in_debian/","section":"post","summary":"这篇笔记用来记录 Debian 系统中的一些实用技巧。\n","tags":["Linux","Debian"],"title":"Debian 系统实用技巧笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些用到的 VictoriaMetrics API 和常用的 PromQL 。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ VictoriaMetrics API 这里主要指的是 VictoriaMetrics 单机版所使用的 API 。\n后续内容均使用以下 metrics 作为例子：\nhttp_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query\u0026#34;,instance=\u0026#34;localhost:9090\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 1 http_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query_range\u0026#34;,instance=\u0026#34;localhost:9090\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 0 查询相关的 API 有即时查询和范围查询两种：\n# 即时查询 curl http://localhost:8428/prometheus/api/v1/query \\ -d \u0026#39;query=http_requests_total\u0026#39; # 范围查询 curl http://localhost:8428/prometheus/api/v1/query_range \\ -d \u0026#39;query=http_requests_total\u0026#39; \\ -d \u0026#39;start=-1d\u0026#39; \\ -d \u0026#39;step=1h\u0026#39; 即时查询一般会返回最新的数据样本，但是在指明某些时间节点时，如果没有直接命中对应的数据样本，会返回距离该时间最近的数据节点。\n范围查询和即时查询相似，但是它可以返回多个数据样本，并且同样会尽量返回某些接近的数据样本来替换一些缺失数据。\n所以两者返回的数据都不一定是实际时间对应的数据，可能是一个替换值。\n删除相关的 API ：\ncurl http://localhost:8428/api/v1/admin/tsdb/delete_series \\ -d \u0026#39;match[]=http_requests_total\u0026#39; 该操作会将整个指标都直接删除，并且占用的存储空间不会立即释放。\n常用的 PromQL 这里会介绍一些比较常用的 PromQL 。\nlabel_replace label_replace 的具体用法可以参考这个公式： label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string) 。\n主要用来替换某些标签内容，实际用例参考：\nbefore: http_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query\u0026#34;,instance=\u0026#34;localhost:9090\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 1 http_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query_range\u0026#34;,instance=\u0026#34;localhost:9090\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 0 expr: label_replace(http_requests_total{instance=\u0026#34;localhost:9090\u0026#34;},\u0026#34;instance\u0026#34;,\u0026#34;$1\u0026#34;,\u0026#34;job\u0026#34;,\u0026#34;(.*)\u0026#34;) after: http_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query\u0026#34;,instance=\u0026#34;prometheus\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 1 http_requests_total{code=\u0026#34;200\u0026#34;,handler=\u0026#34;query_range\u0026#34;,instance=\u0026#34;prometheus\u0026#34;,job=\u0026#34;prometheus\u0026#34;,method=\u0026#34;get\u0026#34;} 0 increase increase 用来计算区间向量的增长量，以区间向量的第一个元素和最后一个元素进行计算，实际用例参考：\nbefore: http_requests_total{code=\u0026#34;200\u0026#34;} 100375 @1708045914.967 http_requests_total{code=\u0026#34;200\u0026#34;} 100377 @1708045924.967 http_requests_total{code=\u0026#34;200\u0026#34;} 100378 @1708045934.967 http_requests_total{code=\u0026#34;200\u0026#34;} 100379 @1708045944.967 http_requests_total{code=\u0026#34;200\u0026#34;} 100381 @1708045954.967 http_requests_total{code=\u0026#34;200\u0026#34;} 100381 @1708045964.967 expr: increase(http_requests_total{code=\u0026#34;200\u0026#34;}[1m]) after: http_requests_total{code=\u0026#34;200\u0026#34;} 7 increase 存在数据外推现象，所以实际使用的计算数据可能不是简单的区间向量的第一个元素和最后一个元素直接运算，而会通过外推后的值来平衡计算。\nrate 和 irate rate 和 irate 都可以用来计算区间向量中数据的增长率，但是 rate 是以区间向量的第一个元素和最后一个元素进行计算，而 irate 是以区间向量的最新两个元素进行计算： rate(http_requests_total{code=\u0026#34;200\u0026#34;}[10m]) 和 irate(http_requests_total{code=\u0026#34;200\u0026#34;}[10m]) 。\nrate 存在数据外推现象，它基本上是基于 increase 的计算值再和时间进行计算。而 irate 只会取最近的两个元素进行计算，所以不会存在数据外推。\ngroup_left 和 group_right group_left 和 group_right 是用于向量匹配的关键字，允许不同向量之间的多对一或者一对多的匹配。\n参考公式：\n\u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) group_left(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) group_right(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; on(\u0026lt;label list\u0026gt;) group_left(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; on(\u0026lt;label list\u0026gt;) group_right(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; 实际用例参考：\nbefore: method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;} 24 method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;} 30 method_code:http_errors:rate5m{method=\u0026#34;put\u0026#34;, code=\u0026#34;501\u0026#34;} 3 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;} 6 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;} 21 method:http_requests:rate5m{method=\u0026#34;get\u0026#34;} 600 method:http_requests:rate5m{method=\u0026#34;del\u0026#34;} 34 method:http_requests:rate5m{method=\u0026#34;post\u0026#34;} 120 expr: method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m after: {method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;} 0.04 // 24 / 600 {method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;} 0.05 // 30 / 600 {method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;} 0.05 // 6 / 120 {method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;} 0.175 // 21 / 120 在上述例子中，我们需要先明确在计算表达式中，多的对象指的是 method_code:http_errors:rate5m ，而一的对象指的是 method:http_requests:rate5m 。它是由向量处于 group_left 或者 group_right 的左右位置决定的。\non 或者 ignoring 是对多所指的对象而言的， ignoring 用来忽略这个标签， on 用来匹配这个标签。由于这个例子中， code 标签被忽略，所以使用 method 标签进行匹配。\ngroup_left 实际上还可以声明额外的标签，以允许使用一对象中的标签覆盖最终结果。由于这个例子中，只有单一的 method 标签，可以通过下面这里扩展例子来理解：\nbefore: method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;, url=\u0026#34;endpoint/500\u0026#34;} 24 method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;, url=\u0026#34;endpoint/404\u0026#34;} 30 method_code:http_errors:rate5m{method=\u0026#34;put\u0026#34;, code=\u0026#34;501\u0026#34;, url=\u0026#34;endpoint/501\u0026#34;} 3 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;, url=\u0026#34;endpoint/500\u0026#34;} 6 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;, url=\u0026#34;endpoint/404\u0026#34;} 21 method:http_requests:rate5m{method=\u0026#34;get\u0026#34;, url=\u0026#34;endpoint\u0026#34;} 600 …","date":1702720800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"3612f5540d5e8fbdd0bd194a35592554","permalink":"https://yuweizzz.github.io/post/useful_victoriametrics_api_and_prmoql/","publishdate":"2023-12-16T10:00:00Z","relpermalink":"/post/useful_victoriametrics_api_and_prmoql/","section":"post","summary":"这篇笔记用来记录一些用到的 VictoriaMetrics API 和常用的 PromQL 。\n","tags":["VictoriaMetrics","Prometheus"],"title":"VictoriaMetrics API 和常用的 PromQL","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录如何解决使用 VictoriaMetrics 过程中查询端出现 connection reset by peer 报错的问题。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 在使用 VictoriaMetrics 作为时序数据源时，偶尔发现查询端的日志中存在 connection reset by peer 的报错。\n在发现这个问题后，我第一时间检查了 VictoriaMetrics 的日志，但是对应的日志中并没有关于连接重置的错误日志，所以这应该不是 VictoriaMetrics 服务本身在请求过程中发生问题导致的连接重置。\n由于服务端的信息比较少，所以只能从查询端入手排查，根据查询端的代码，整个查询过程中共用一个封装好的查询结构体，深扒一下代码，原来这个结构体是基于 http.Client 来实现的。\n由于 VictoriaMetrics 和查询端都是 Go 语言编写的，所以尽量查找一些 Go 在网络相关方面的错误案例，很快就有了线索，那就是 http.Client 的连接管理问题。\n在 http.Client 结构体中的 Transport 部分维护着一个连接池，实际上可以通过 DisableKeepAlives 选项来禁用长连接，不过在查询端中为了提高效率，并没有使用这个选项，还是选择使用这个连接池来执行具体的查询请求。\n因为 VictoriaMetrics 默认的关闭空闲连接的时间是 1m ，而查询端是周期性发起查询请求，其中大部分都是 30s 查询一次，这样就有可能正好撞上 VictoriaMetrics 的空闲连接关闭时间，此时上游已经关闭连接，而查询端仍然向这个连接发起查询请求，从而产生 connection reset by peer 的错误。\n要解决这个问题有两个方法，一个是完全禁用长连接，这样可以从根本解决问题，但是要修改查询端的代码，而且这样就不能复用连接资源了，另一个方法是控制上游的连接超时时间，使得长连接在适当的时间回收。\n这里选择的是第二个方法， VictoriaMetrics 提供了控制连接超时时间的选项 -http.idleConnTimeout ，将这个时间从默认的 1m 调整到 100s 后，就可以避免大量查询周期为 30s 的请求产生错误。这个时间的调节应该根据实际情况出发，不论是选择更长或者更短的空闲等待时间都是可以的，只要能够避开查询端的请求周期就没问题。但还是推荐尽量使用更长的时间，这样可以节省发起连接的资源损耗。\n","date":1702634400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"b9d3309a65d65fd65b99b0f552b4a977","permalink":"https://yuweizzz.github.io/post/fix_connection_reset_error_when_query_from_victoriametrics/","publishdate":"2023-12-15T10:00:00Z","relpermalink":"/post/fix_connection_reset_error_when_query_from_victoriametrics/","section":"post","summary":"这篇笔记用来记录如何解决使用 VictoriaMetrics 过程中查询端出现 connection reset by peer 报错的问题。\n","tags":["VictoriaMetrics","Go"],"title":"解决使用 VictoriaMetrics 时的连接重置问题","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Linux 系统下一些网络协议的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 路由管理 在 Linux 系统中一般通过 iproute2 对路由进行管理。\n$ ip route default via 10.233.0.1 dev ens33 10.233.0.0/16 dev ens33 proto kernel scope link src 10.233.0.2 虽然通过 ip route 大部分情况就可以满足基本的场景需求，但是可以使用多张路由表来实现更复杂的流量控制。\n# Linux 系统默认定义的路由表，其中 ip route 默认使用的就是 main ，也是大多数情况下默认使用的路由表 # /etc/iproute2/rt_tables 用来定义 route table 和 id 的映射关系 $ cat /etc/iproute2/rt_tables # # reserved values # 255 local 254 main 253 default 0 unspec # # local # #1 inr.ruhep # 路由表有优先级定义，一般来说 local 表的优先级应该是最高的 # 注意这里的数字表示的是 prio 而不是 table id $ ip rule show 0: from all lookup local 32766: from all lookup main 32767: from all lookup default # 添加新的路由表 $ echo \u0026#34;101 custom\u0026#34; \u0026gt;\u0026gt; /etc/iproute2/rt_tables # 预先定义路由表的映射关系 $ ip rule add from 10.10.10.0/24 table custom prio 101 $ ip rule show 0: from all lookup local 101: from 10.10.10.0/24 lookup custom 32766: from all lookup main 32767: from all lookup default # 可以不定义映射关系，直接通过 id 创建路由表 $ ip rule add from 10.10.11.0/24 table 102 prio 102 $ ip rule show 0: from all lookup local 101: from 10.10.10.0/24 lookup custom 102: from 10.10.11.0/24 lookup 102 32766: from all lookup main 32767: from all lookup default # 往新路由表中添加路由规则 # 当定义路由的 via 或者 src 时，指定的 IP 地址应该是本机持有的地址 $ ip route add default via 10.10.11.1 dev ens33 table 102 # 默认网关 $ ip route add 10.10.11.0/24 dev ens33 src 10.10.11.1 table 102 # 普通路由规则 # 查看添加规则后的路由表，不指定 table 则默认使用 main 表 $ ip route show table 102 # 删掉路由表 $ ip rule del table 102 $ ip rule del table custom # 有映射关系的可以使用 id 或者名称 多张路由表可以用在多网络出口和特殊子网管理的场景，使用单独的路由表来控制对应的流量，在网络情况复杂的时候会很有用。\n# 通过 iptables 标记流量来指定处理的路由表，在容器网络管理方面经常使用 $ iptables -t mangle -A FORWARD -i ens33 -j MARK --set-mark 1 $ iptables -t mangle -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -A FORWARD -i ens33 -j MARK --set-xmark 0x1/0xffffffff # 添加处理对应流量标记的路由表 $ ip rule add fwmark 1 table 103 prio 103 $ ip rule show 0: from all lookup local 101: from 10.10.10.0/24 lookup custom 102: from 10.10.11.0/24 lookup 102 103: from all fwmark 0x1 lookup 103 32766: from all lookup main 32767: from all lookup default # cilium 实际上也使用到了流量标记，以下输出是来自一台运行了 cilium 的系统 $ ip rule show 9: from all fwmark 0x200/0xf00 lookup 2004 100: from all lookup local 32766: from all lookup main 32767: from all lookup default MTU 和 MSS 在使用 openconnect 搭建 VPN 服务时，比较有意思的地方就是 tunnel 的 MTU 设定。\n# 搭建 VPN 服务时用到的防火墙配置，其中 10.10.10.0/24 是 VPN 服务所使用的网段 $ iptables -t filter -A FORWARD -s 10.10.10.0/24 -j ACCEPT $ iptables -t filter -A FORWARD -o vpns+ -j ACCEPT $ iptables -t filter -A FORWARD -i vpns+ -j ACCEPT $ iptables -t nat -A POSTROUTING -s 10.10.10.0/24 -o eth0 -j MASQUERADE # 有一些说法指出防火墙还需要配置下面这条规则 $ iptables -A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu # 连接 VPN 后查看新建立的 tunnel $ ip tuntap show tun0: tun $ ip addr show tun0 27: tun0: \u0026lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u0026gt; mtu 1472 qdisc pfifo_fast state UNKNOWN group default qlen 500 link/none inet 10.10.10.101/32 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::ec6a:a1a5:de04:9aa3/64 scope link flags 800 valid_lft forever preferred_lft forever openconnect 使用 tun 设备来连接客户端和服务端， tun 设备是 Linux 系统下的虚拟三层网络设备，可以看到这里使用的 MTU 值是 1472 。\nMTU 是网络层的概念，它定义了网络传输中允许数据包通过的最大值，这个值的计算已经包括数据包标头在内，而 TCP MSS 是除去标头部分后 TCP 协议传输实际数据内容的最大值，是传输层的概念。因为标准以太网接口的 MTU 是 1500 bytes ，而 TCP 协议标头和 IP 协议标头都是 20 bytes ，所以标准以太网下对应的 TCP MSS 就是 1460 bytes 。\nMTU 和 TCP MSS 的重要区别是，当数据包超过目标设备的 MTU 则会导致数据的分包或者丢弃，这种情况会降低传输效率。而在 TCP 协议的传输过程中，双方可以进行 MSS 通告，然后根据双方提供的 MSS 值中的最小值确定连接的具体 MSS 值。\n一般来说， VPN 软件可能会添加一些自定义的数据包标头，所以为了保险起见，在创建 tun 设备时，会尽量使用低于 1500 的 MTU 值，这样当隧道中的数据包驮载到以太网设备中传输时，尽量做到保持数据包大小在标准以太网接口 MTU 之内。这一点可以参考 gre tunnel 的实现， gre tunnel 经常使用 1476 的 MTU 值，因为它需要 24 bytes 作为数据包标头。在 gre tunnel 的数据包中，包括了均为 20 bytes 的 TCP 协议标头和 IP 协议标头，这样它的 MSS 应该设置为 1436 bytes 是最合理，当数据包从 tunnel 中发出时，还需要添加 24 bytes 的数据包标头，最后总共就是 1500 bytes 的数据包格式。更多的详细信息可以参考思科的官方文档。\n那么根据推断， openconnect 的数据包标头应该是 28 bytes ，其中 20 bytes 应该是标准 IP 协议的标头， 而 8 bytes 应该是自定义标头，如果使用的是 IPv6 协议，这个总值应该是 48 bytes ，这里可以通过在 openconnect 服务端显式设置 MTU 值来验证，在连接过程中你可以看到对应的 HTTP header 已经包含了这部分信息，其中 X-CSTP-Base-MTU 就是服务端的 tunnel 预设值，一般会使用低于 1500 的 MTU 值，而 X-CSTP-MTU 就是去除标头后的实际 MTU 值，也就是最后 tunnel 实际使用的 MTU 值。\n防火墙的 --clamp-mss-to-pmtu 一般是用来协调使用不同 MTU 值的网络接口传输过程中的 TCP MSS ，在 openconnect 中，服务端的流量出入口通常都是同个网卡，这个设置实际上是可以省去的，如果网络环境比较复杂可能才需要这个设置。此外除了自动调整，也可以将 TCP MSS 设定为一个固定值，通过 iptables -A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS …","date":1699560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"4103a09cc52a7837efbceae06395ae49","permalink":"https://yuweizzz.github.io/post/knowledge_about_linux_network/","publishdate":"2023-11-09T20:00:00Z","relpermalink":"/post/knowledge_about_linux_network/","section":"post","summary":"这篇笔记用来记录 Linux 系统下一些网络协议的相关知识。\n","tags":["Linux","iproute2","iptables","openconnect"],"title":"Linux 网络协议知识笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录如何使用 Kaniko 在 Kubernetes 集群中构建镜像。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 使用 kaniko 构建镜像 kaniko 是 Google 开发的镜像构建工具，它的特点在于不需要守护进程，直接运行在容器或者 Kubernetes 集群。\n我们可以编写相关的 YAML 文件直接应用到 Kubernetes 集群中。\n# from https://github.com/GoogleContainerTools/kaniko apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: - \u0026#34;--dockerfile=\u0026lt;path to Dockerfile within the build context\u0026gt;\u0026#34; - \u0026#34;--context=gs://\u0026lt;GCS bucket\u0026gt;/\u0026lt;path to .tar.gz\u0026gt;\u0026#34; - \u0026#34;--destination=\u0026lt;gcr.io/$PROJECT/$IMAGE:$TAG\u0026gt;\u0026#34; volumeMounts: - name: kaniko-secret mountPath: /secret env: - name: GOOGLE_APPLICATION_CREDENTIALS value: /secret/kaniko-secret.json restartPolicy: Never volumes: - name: kaniko-secret secret: secretName: kaniko-secret 主要通过容器参数来定义构建内容，主要的参数信息参考：\n--context ：容器构建的上下文信息，一般就是源代码路径。 --dockerfile ： dockerfile 文件所在路径，一般来说保持 dockerfile 在源代码目录并且命名为 Dockerfile 可以不需要这个参数。 --destination ：镜像的命名相关信息。 可以看到这个 pod 还额外挂载了 secret ，这部分其实是用来存放拉取源码和构建完成后推送镜像的一些认证信息。\n通过 hostPath 进行本地构建 以下是用来做本地测试时的 YAML 文件，需要注意只是用来测试 kaniko 的功能，在实际使用时应该确保 pod 可以正常调度到对应存放着源代码的节点上。\napiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: - \u0026#34;--context=/source/\u0026#34; - \u0026#34;--no-push\u0026#34; - \u0026#34;--destination=gcr.io/project/image:latest\u0026#34; - \u0026#34;--tar-path=/source/image.tar\u0026#34; volumeMounts: - name: source mountPath: /source restartPolicy: Never volumes: - name: source hostPath: path: /path/to/source/ 新出现的参数信息参考：\n--no-push ：构建镜像完成后不进行推送。 --tar-path ：通过指定路径将镜像通过 tar 文件进行保存。 虽然这里没有涉及 docker 镜像构建中的 --build-arg ，但是它在 kaniko 中同样支持，具体用法和 docker build 相似。\n使用 --build-arg 的 YAML 文件参考 apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest env: # for build arg with space - name: IFS value: \u0026#34;\u0026#34; args: - \u0026#34;--context=/source/\u0026#34; - \u0026#34;--no-push\u0026#34; - \u0026#34;--destination=gcr.io/project/image:latest\u0026#34; - \u0026#34;--tar-path=/source/image.tar\u0026#34; - \u0026#34;--build-arg=ARG_1=VALUE_1\u0026#34; - \u0026#34;--build-arg=ARG_2=VALUE_2\u0026#34; - \u0026#34;--build-arg=ARG_3=\u0026#39;VALUE WITH SPACE\u0026#39;\u0026#34; volumeMounts: - name: source mountPath: /source restartPolicy: Never volumes: - name: source hostPath: path: /path/to/source/ 编辑完成后直接通过 kubectl 启动 pod 来执行构建。\n# 保存 yaml 文件后就可以直接执行构建 kubectl apply -f kaniko.yaml # 可以追踪构建过程 kubectl logs kaniko -f # 构建完成后应该可以在对应目录找到 image.tar ，将它导入到 containerd 纳管 ctr -n k8s.io i import image.tar ctr i ls 构建完成后推送到镜像仓库 这里会涉及本地仓库的搭建，测试环境可以使用 docker registry ，如果是生产环境可以考虑 harbor 。\n搭建 docker registry 的资源文件 # docker registry apiVersion: apps/v1 kind: Deployment metadata: labels: app: registry name: registry spec: replicas: 1 selector: matchLabels: app: registry template: metadata: labels: app: registry spec: containers: - image: registry:latest name: registry volumeMounts: - name: configfile mountPath: /etc/docker/registry/ volumes: - name: configfile configMap: name: config --- apiVersion: v1 kind: ConfigMap metadata: name: config data: config.yml: |+ version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry delete: enabled: true http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 --- apiVersion: v1 kind: Service metadata: name: registry spec: selector: app: registry ports: - protocol: TCP port: 5000 targetPort: 5000 docker registry 搭建完成后，我们只需要对前面的本地构建所使用的 YAML 做出一些小修改就可以，主要还是 kaniko 的运行参数：\n不再需要 \u0026#34;--no-push\u0026#34; 。 保存 tar 文件的参数 \u0026#34;--tar-path=/source/image.tar\u0026#34; 是可选项，按需选择保留或者去除。 将目标镜像 \u0026#34;--destination=gcr.io/project/image:latest\u0026#34; 修改为 \u0026#34;--destination=registry:5000/project/image:latest\u0026#34; 指向本地镜像仓库。 由于搭建的 docker registry 没有启用 https ，所以需要增加 \u0026#34;--insecure\u0026#34; 参数，这样 kaniko 才能正常进行镜像推送。 修改后的 kaniko 具体实例 apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: - \u0026#34;--context=/source/\u0026#34; - \u0026#34;--destination=registry:5000/project/image:latest\u0026#34; - \u0026#34;--insecure\u0026#34; volumeMounts: - name: source mountPath: /source restartPolicy: Never volumes: - name: source hostPath: path: /path/to/source/ 以下是一些可能用到的 docker registry API ：\n# 查看仓库中的镜像信息 curl \u0026#34;\u0026lt;registry:5000\u0026gt;/v2/_catalog\u0026#34; # 根据仓库给出的镜像信息，可以查到某个镜像所有的 tags curl \u0026#34;\u0026lt;registry:5000\u0026gt;/v2/\u0026lt;project/image\u0026gt;/tags/list\u0026#34; # 删除镜像时需要查询具体签名才能执行 # 使用 docker 进行镜像构建，可能需要使用下面的 header 进行查询 curl -I -H \u0026#34;Accept: application/vnd.docker.distribution.manifest.v2+json\u0026#34; \u0026#34;\u0026lt;registry:5000\u0026gt;/v2/\u0026lt;project/image\u0026gt;/manifests/\u0026lt;latest\u0026gt;\u0026#34; # 使用 kaniko 进行镜像构建，镜像使用的是 oci 标准格式 curl -I -H …","date":1697541405,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"98ed55c3fa20736ae3af1c1e5e38fab8","permalink":"https://yuweizzz.github.io/post/use_kaniko_to_build_images_in_kubernetes_cluster/","publishdate":"2023-10-17T11:16:45Z","relpermalink":"/post/use_kaniko_to_build_images_in_kubernetes_cluster/","section":"post","summary":"这篇笔记用来记录如何使用 Kaniko 在 Kubernetes 集群中构建镜像。\n","tags":["kaniko","distroless","container","docker registry"],"title":"使用 Kaniko 在 Kubernetes 集群中构建镜像","type":"post"},{"authors":null,"categories":null,"content":"一个由游戏汉化项目引起的思考。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 基于日文开发的游戏无法正常在使用中文的 Windows 系统上显示文本。\n如果你使用的是 Windows 操作系统，那么你可以在当前系统通过 cmd 或者 PowerShell 执行 chcp 命令，得到系统正在使用的活动代码页，活动代码页实际上就是 windows codepage ，中文的系统通常返回会是 936 ，也就是 CP936 。当你把系统语言设置为日文之后，就可以正常打开这些游戏，此时检查 codepage ，你会发现返回的代码应该是 932 ，也就是 CP932 。\ncodepage 是 Windows 系统用来转换编码格式的重要依据，在业务范围面向世界的 Windows 系统中，语言本地化是非常重要的环节，当使用了各种各样编码格式的文本要在 Windows 系统中能够以使用者熟悉的语言正常显示，就需要使用正确的编码格式来读取。\n在这个过程中 Unicode 是一个不可忽视的编码标准。 Unicode 通过使用 U+0000 格式的码位来表示字符，这个格式可以覆盖非常大的数据范围，几乎足以包括了世界上所有语言中的字符。但是实际使用过程中很少直接使用对应的码位来表示字符，而是使用更高效的编码方式，也就是 utf-8 或者 utf-16 这两种。所以可以说 Unicode 是一个支持全部语言字符的通用性字符表，而 utf-8 和 utf-16 是基于这个字符表实现的更高效的表示方法。\n实际上 Windows 系统也是通过 Unicode 来实现语言的多样化，在底层操作中使用 Unicode 可以避免很多问题，但是在本地化场景下，由于无法确定用户使用的语言，所以系统需要通过设置 codepage 来适应这个场景，这样当用户使用特殊的编码格式时，操作系统就可以通过 codepage 映射到 Unicode 中，保持底层操作的一致性。\nUnicode 字符编码 在这里我们只关注 utf-8 和 utf-16 两种编码格式，因为他们的使用频率最高。\n首先是大小端的问题，因为硬件平台的差异，在处理多字节数据时，数据的高低字节读取顺序就很重要，应该保持写入时的顺序，否则读出来的数据就会截然不同。\n以二进制数据 0x1234 为例子，此时高字节是 0x12 ，而低字节是 0x34 ，大小端的数据处理情况是这样的：\n大端：高字节存放在内存低地址，低字节存放在内存高地址，假设从左到右表示内存地址增长，那么数据会是 0x1234 。 小端：高字节存放在内存高地址，低字节存放在内存低地址，假设从左到右表示内存地址增长，那么数据会是 0x3412 。 所以实际使用的 utf-16 还分为 utf-16 LE 和 utf-16 BE 两种，对应小端和大端的不同处理场景。\n由于字节序会影响实际的读取，使用 utf-8 和 utf-16 编码格式的文件还需要通过字节顺序标记来记录对应的字节序，也就是在调整编码时经常会出现的 BOM 概念。这个标记在跨平台的时候非常重要，它是文件正常解码的关键。\nutf-8 和 utf-16 的 BOM 会出现在文件开头，然后才是真正的文件内容， BOM 的具体内容是这样的：\nutf-8 ： 0xEFBBBF utf-16 BE ： 0xFEFF utf-16 LE ： 0xFFFE 使用 utf-16 编码的文件必须带有 BOM ，否则会读取错误，而 utf-8 实际上可以不需要 BOM ，因为它是字节序无关的，但是使用 utf-8 编码格式的文件依然可以添加 BOM 作为标记。在 Windows 系统和 Linux 系统对 utf-8 BOM 的处理方式不同，在跨系统处理文件的时候可能要注意这个问题。\nutf-8 编码可以使用一到四个字节来表示单个字符，英文字符只需要用到一个字节，而中文字符一般要用到三个字节，这样看起来中文字符或者使用多字节表示的字符似乎会受到大小端问题的影响，实际上 utf-8 虽然也用到了多字节，但是不同于 utf-16 这样的编码方式， utf-16 因为用到了两个字节和四个字节来表示字符，在使用两个字节的情况下相当于直接使用类似于 U+0000 的 Unicode 码位，此时字节序就会明显影响读取值所指向的具体码位。而 utf-8 在多字节的情况下，则需要通过第一个字节判断出具体的使用的字节数量，然后顺序读取后续的数据将它们当作整块数据来处理。\n// from Standard library unicode/utf8 func DecodeRune(p []byte) (r rune, size int) { n := len(p) if n \u0026lt; 1 { return RuneError, 0 } // as 和 first 都是常量 // const as = 0xF0 // first 是记录了 utf-8 第一个字节编码情况的数组 p0 := p[0] x := first[p0] // 判断是否使用英文字符，即使用单个字节的情况 if x \u0026gt;= as { // The following code simulates an additional check for x == xx and // handling the ASCII and invalid cases accordingly. This mask-and-or // approach prevents an additional branch. mask := rune(x) \u0026lt;\u0026lt; 31 \u0026gt;\u0026gt; 31 // Create 0x0000 or 0xFFFF. return rune(p[0])\u0026amp;^mask | RuneError\u0026amp;mask, 1 } // 使用多字节的情况 // 通过第一个字节判断具体使用的字节数量 sz := int(x \u0026amp; 7) accept := acceptRanges[x\u0026gt;\u0026gt;4] if n \u0026lt; sz { return RuneError, 1 } b1 := p[1] if b1 \u0026lt; accept.lo || accept.hi \u0026lt; b1 { return RuneError, 1 } // 使用两个字节的情况 if sz \u0026lt;= 2 { // \u0026lt;= instead of == to help the compiler eliminate some bounds checks return rune(p0\u0026amp;mask2)\u0026lt;\u0026lt;6 | rune(b1\u0026amp;maskx), 2 } b2 := p[2] if b2 \u0026lt; locb || hicb \u0026lt; b2 { return RuneError, 1 } // 使用三个字节的情况 if sz \u0026lt;= 3 { return rune(p0\u0026amp;mask3)\u0026lt;\u0026lt;12 | rune(b1\u0026amp;maskx)\u0026lt;\u0026lt;6 | rune(b2\u0026amp;maskx), 3 } b3 := p[3] if b3 \u0026lt; locb || hicb \u0026lt; b3 { return RuneError, 1 } // 使用四个字节的情况 return rune(p0\u0026amp;mask4)\u0026lt;\u0026lt;18 | rune(b1\u0026amp;maskx)\u0026lt;\u0026lt;12 | rune(b2\u0026amp;maskx)\u0026lt;\u0026lt;6 | rune(b3\u0026amp;maskx), 4 } 这是用来测试的 utf-16 LE 编码转换为 utf-8 编码的代码：\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;os\u0026#34; \u0026#34;golang.org/x/text/encoding/unicode\u0026#34; \u0026#34;golang.org/x/text/transform\u0026#34; ) func main() { filename := os.Args[1] file, err := os.Open(filename) if err != nil { panic(err) } f, err := os.Create(\u0026#34;./convert_\u0026#34; + filename) defer f.Close() w := bufio.NewWriter(f) scanner := bufio.NewScanner(transform.NewReader(file, unicode.UTF16(unicode.LittleEndian, unicode.UseBOM).NewDecoder())) for scanner.Scan() { line := append(scanner.Bytes(), []byte(\u0026#34;\\n\u0026#34;)...) w.Write(line) } w.Flush() } 这里其实是通过 utf-16 解码之后默认使用 utf-8 编码来实现的。如果想转换其他编码格式还需要重新定义编码器，使用新的编码器将 Scan 得到的数据转换为 string 类型写入。\n常见语言的编码格式 GB2312 是比较早的中文字符编码，而后由于 Unicode 的制定和汉字收录拓展的关系，发展出了 GBK 编码格式，它向下兼容 GB2312 ，也基本实现了对当时版本 Unicode 收录汉字范围的覆盖，现代 Windows 系统使用的 CP936 就是 GBK 编码格式。\nShift JIS 则是日文字符编码，也就是 CP932 所使用的编码格式。\nBig5 是繁体中文字符编码，对应的 codepage 是 CP950 ，多用于港澳台地区，而 GBK 主要是简体中文字符。\n# 不同编码格式的读写测试 # 通过二进制编辑器来读取文件内容 raw = \u0026#34;\u0026#34;\u0026#34; c4e3 bac3 \u0026#34;\u0026#34;\u0026#34; raw = raw.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) raw = raw.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) bins = b\u0026#39;\u0026#39;.join([ bytes.fromhex(raw[i:i+2]) for i in range(len(raw))[::2] ]) # 以 GBK 编码格式来解码，可以输出实际的文本内容 bins.decode(\u0026#34;gbk\u0026#34;) # 输出乱码可能是小端模式的显示问题，尝试互换位置再进行解码 (b\u0026#39;\u0026#39;.join([ bytes.fromhex(raw[i+2:i+4] + raw[i:i+2]) for i in range(len(raw))[::4] ])).decode(\u0026#34;gbk\u0026#34;) # 计算字节数量，通过十六进制显示 hex(int(len(raw)/2)) # 以 Shift JIS 编码格式来解码，可以用来解码一些在中文系统下的日文乱码文件 bins.decode(\u0026#34;shift-jis\u0026#34;) # 指定某个编码格式将文本内容进行编码，通过十 …","date":1695741720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"9e40b0c636a5974412815d1c6e715062","permalink":"https://yuweizzz.github.io/post/study_notes_about_character_encoding_formats/","publishdate":"2023-09-26T15:22:00Z","relpermalink":"/post/study_notes_about_character_encoding_formats/","section":"post","summary":"一个由游戏汉化项目引起的思考。\n","tags":["unicode","utf-8"],"title":"探究字符编码格式","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来分析 Apisix 是如何实现负载均衡的。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ Route 和 Upstream 在 Apisix 中， Route 是最基础的资源对象，我们通过它来定义不同类型的请求，而 Upstream 是 Route 中请求的实际处理者， Apisix 会根据 Upstream 中的定义，对服务节点进行负载均衡，这一步也是这里想要分析的内容。\n源码解析 熟悉 Openresty 的都应该知道它是根据请求的不同阶段来进行逻辑处理，同样地我们将 Apisix 中的 access_by_lua_block 和 balancer_by_lua_block 作为我们的分析入口。根据 apisix/cli/ngx_tpl.lua 可以找到这两个 block 分别对应了 apisix.http_access_phase() 和 apisix.http_balancer_phase() 这两个函数，所以我们需要在 apisix/init.lua 中的代码寻找这两个函数的定义。\n我们可以看到 apisix.http_access_phase() 主要做了以下工作：\n创建上下文并且根据 ngx 中的信息写入到其中。 根据上下文信息来匹配 Route 。 匹配成功后，执行 Route 中定义的对应阶段中需要执行的插件。 进行上游信息处理。 在第四步的进行上游信息处理中，具体工作交由函数 handle_upstream() 中来完成，可以看到其中有两个非常重要的调用：\nset_upstream() 根据 Route 找到对应的 Upstream ，这里 set_upstream() 是来自模块 apisix/upstream.lua 中的函数。 load_balancer.pick_server() 会根据 Upstream 中的配置选出具体的上游节点，这里的 load_balancer 模块实际就是 apisix/balancer.lua 。 在 handle_upstream() 的最后阶段，选中的上游节点会写入到上下文 api_ctx.picked_server 之中，整个 apisix.http_access_phase() 过程也就基本结束了。\n而 apisix.http_balancer_phase() 需要做的事情比较简单，因为它直接对应 balancer_by_lua_block 阶段，所以它的主要工作就是调用 load_balancer.run() ，也就是 apisix/balancer.lua 中的 run() 。\n下面是 apisix/balancer.lua 的代码片段，主要有 create_server_picker() ， pick_server() 和 run() 三个函数的具体内容。\nlocal function create_server_picker(upstream, checker) -- 根据 upstream 中定义的 type 选择具体的负载均衡算法 local picker = pickers[upstream.type] if not picker then pickers[upstream.type] = require(\u0026#34;apisix.balancer.\u0026#34; .. upstream.type) picker = pickers[upstream.type] end if picker then local nodes = upstream.nodes local addr_to_domain = {} for _, node in ipairs(nodes) do if node.domain then local addr = node.host .. \u0026#34;:\u0026#34; .. node.port addr_to_domain[addr] = node.domain end end local up_nodes = fetch_health_nodes(upstream, checker) -- _priority_index 和 upstream 中定义的 priority 有关 if #up_nodes._priority_index \u0026gt; 1 then core.log.info(\u0026#34;upstream nodes: \u0026#34;, core.json.delay_encode(up_nodes)) -- 这里的 priority_balancer 就是 apisix/balancer/priority.lua local server_picker = priority_balancer.new(up_nodes, upstream, picker) server_picker.addr_to_domain = addr_to_domain return server_picker end core.log.info(\u0026#34;upstream nodes: \u0026#34;, core.json.delay_encode(up_nodes[up_nodes._priority_index[1]])) local server_picker = picker.new(up_nodes[up_nodes._priority_index[1]], upstream) server_picker.addr_to_domain = addr_to_domain return server_picker end return nil, \u0026#34;invalid balancer type: \u0026#34; .. upstream.type, 0 end -- pick_server will be called: -- 1. in the access phase so that we can set headers according to the picked server -- 2. each time we need to retry upstream local function pick_server(route, ctx) core.log.info(\u0026#34;route: \u0026#34;, core.json.delay_encode(route, true)) core.log.info(\u0026#34;ctx: \u0026#34;, core.json.delay_encode(ctx, true)) local up_conf = ctx.upstream_conf for _, node in ipairs(up_conf.nodes) do if core.utils.parse_ipv6(node.host) and str_byte(node.host, 1) ~= str_byte(\u0026#34;[\u0026#34;) then node.host = \u0026#39;[\u0026#39; .. node.host .. \u0026#39;]\u0026#39; end end local nodes_count = #up_conf.nodes if nodes_count == 1 then local node = up_conf.nodes[1] ctx.balancer_ip = node.host ctx.balancer_port = node.port node.upstream_host = parse_server_for_upstream_host(node, ctx.upstream_scheme) return node end local version = ctx.upstream_version local key = ctx.upstream_key local checker = ctx.up_checker ctx.balancer_try_count = (ctx.balancer_try_count or 0) + 1 if ctx.balancer_try_count \u0026gt; 1 then if ctx.server_picker and ctx.server_picker.after_balance then ctx.server_picker.after_balance(ctx, true) end if checker then local state, code = get_last_failure() local host = up_conf.checks and up_conf.checks.active and up_conf.checks.active.host local port = up_conf.checks and up_conf.checks.active and up_conf.checks.active.port if state == \u0026#34;failed\u0026#34; then if code == 504 then checker:report_timeout(ctx.balancer_ip, port or ctx.balancer_port, host) else checker:report_tcp_failure(ctx.balancer_ip, port or ctx.balancer_port, host) end else checker:report_http_status(ctx.balancer_ip, port or ctx.balancer_port, host, code) end end end if checker then version = version .. \u0026#34;#\u0026#34; .. checker.status_ver end -- the same picker will be used in the whole request, especially during the retry -- 这里使用了 LRU 缓存，通过 server_picker 的复用达到节省资源的目的 local server_picker = ctx.server_picker if not server_picker then server_picker = lrucache_server_picker(key, version, create_server_picker, up_conf, checker) end if not server_picker then return nil, \u0026#34;failed to fetch server picker\u0026#34; end -- 每个实现具体算法的 balancer 都具有 get() 方法 local server, err = …","date":1691097405,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"fbf7c9ed34ecf8ae4891aa2e5dd5fc63","permalink":"https://yuweizzz.github.io/post/how_apisix_implements_load_balancing/","publishdate":"2023-08-03T21:16:45Z","relpermalink":"/post/how_apisix_implements_load_balancing/","section":"post","summary":"这篇笔记用来分析 Apisix 是如何实现负载均衡的。\n","tags":["Lua","Openresty","Apisix"],"title":"Apisix 的负载均衡实现","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记主要介绍 filebeat 配置文件各参数含义和一些配置实例。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ filebeat 用于收集和处理日志，是 Elastic Beats 开源组件中的关键一员，它通过 YAML 文件来控制具体的日志文件收集行为，这里会将 YAML 配置文件作为探讨的主题。\n在 Elastic 官方文件中， Beats 家族被定位为轻量级数据传输器，这里的轻量级主要是和 logstash 做比较，但它们的核心功能都是将源数据进行格式化处理后输出到目的端，所以它们的配置文件就相当于对管道进行配置，主要关心数据进入时和数据输出时的动作。\n配置 input 在 filebeat 中， input 来源非常丰富，可以是标准输入，具体文件，甚至是容器，网络流，对象存储等。我们这里以最常见的 log 为例，也就是将具体文件作为输入来进行基本配置定义。\nfilebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log exclude_files: [\u0026#39;\\.gz$\u0026#39;] log 类型的列表项中 path 和 exclude_files 可以用来定义需要被 filebeat 收集的文件，默认会收集 path 中定义的所有文件路径中的文件， exclude_files 用来去掉通配路径中不需要的文件。\nCommon options filebeat 将逻辑上的单条日志当作一个 event 来处理，并使用 Elastic Common Schema 来描述 event ，其中具体的日志内容的存储字段将是 event.message ，还会带有 event.@timestamp ， event.tags ， event.labels 这些元数据字段，而 inputs Common options 中的某些配置可以直接影响 event 中的字段内容。\nfilebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log exclude_files: [\u0026#39;\\.gz$\u0026#39;] tags: [\u0026#34;json\u0026#34;] fields: app_id: query_engine_12 fields_under_root: true tags 中的内容会直接出现在 event.tags 字段中，可以用于后续处理的重要标识。而 fields 和 fields_under_root 是关联出现的，如果 fields_under_root 为 false ，那么 fields 中的所有字段将直接出现在 event.fields 字段中，如果 fields_under_root 为 true ，那么 fields 中的每个子字段会成为 event 的根字段，并且遇到同名字段将以 fields 中定义的内容覆盖。\nline 配置 可以通过 exclude_lines 和 include_lines 来具体定义需要收集的内容，它们可以单独或同时出现，但是在同时出现时 include_lines 的优先级永远高于 exclude_lines 。\nfilebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log exclude_files: [\u0026#39;\\.gz$\u0026#39;] tags: [\u0026#34;json\u0026#34;] fields: app_id: query_engine_12 fields_under_root: true exclude_lines: [\u0026#34;^DBG\u0026#34;] include_lines: [\u0026#34;^ERR\u0026#34;, \u0026#34;^WARN\u0026#34;] 关于两者共存的逻辑，具体相关的代码部分是这样的：\nfunc (h *Harvester) shouldExportLine(line string) bool { if len(h.config.IncludeLines) \u0026gt; 0 { if !harvester.MatchAny(h.config.IncludeLines, line) { // drop line return false } } if len(h.config.ExcludeLines) \u0026gt; 0 { if harvester.MatchAny(h.config.ExcludeLines, line) { return false } } return true } multiline 配置 filebeat 还支持将多行日志聚合的功能，需要通过 multiline 相关配置来定义具体的行为。\nfilebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log exclude_files: [\u0026#39;\\.gz$\u0026#39;] tags: [\u0026#34;json\u0026#34;] fields: app_id: query_engine_12 fields_under_root: true multiline.pattern: \u0026#39;^\\[\u0026#39; multiline.negate: true multiline.match: after multiline.pattern 用来定义正则表达式，用来定义需要匹配的行； multiline.negate 用来定义使用 multiline.pattern 进行匹配或者反选， multiline.negate 为 true 则为反选， multiline.negate 为 false 则为匹配； multiline.match 用来定义匹配值向前或者向后组合，具体值有 before 和 after 。\n它的具体工作模式是这样的：首先使用 multiline.pattern 进行逐行比较，根据 multiline.negate 判定该行是聚合行中的子行或者是聚合行的开始行，再根据 multiline.match 进行组合。\n示例中的表达式匹配了使用开头为 [ 的行，由于它的 multiline.negate 为 true ，所以不以 [ 开头的行被认为是聚合行中的子行，并且根据 multiline.match 的 after ，每次匹配到 [ 之后，后续的子行会与它聚合为一行，直到下一符合 [ 开头的行为止。\njson 配置 json 配置可以使 filebeat 读取日志文件时将对应的日志内容解析为 json 格式。\nfilebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log exclude_files: [\u0026#39;\\.gz$\u0026#39;] tags: [\u0026#34;json\u0026#34;] fields: app_id: query_engine_12 fields_under_root: true exclude_lines: [\u0026#34;^DBG\u0026#34;] include_lines: [\u0026#34;^ERR\u0026#34;, \u0026#34;^WARN\u0026#34;] json: keys_under_root: true overwrite_keys: true add_error_key: true 它适用于日志文件本身就是单行 json 格式的场景，因为它默认的解析来源是 event.message 字段，不过这个来源可以通过 message_key 进行修改。其中参数 keys_under_root 和 overwrite_keys 的定义类似于 fields_under_root 的行为，如果成功解析日志，字段都会被添加到 event 中，且同名字段时也是采取覆盖操作。而 add_error_key 则会在解析日志失败时添加 event.error 字段并在其中加入报错信息。解析成功后，不会再出现 event.message 字段。\n具体行为配置 filebeat 可以通过某些配置项来定义对日志文件的具体操作细节：\nscan_frequency ：扫描通配路径中文件的时间频率，它会监控是否有新文件和已有文件变化，默认值为 10s 。 tail_files ：是否从文件末尾开始读取文件，默认值为 false ，会从头开始读取文件。 symlinks ：是否读取符号链接文件，默认值为 false ，不读取符号链接文件。 ignore_older ：忽略经过某段时间未被修改的文件，实际上是通过文件的修改时间来区分的，默认值为 0 ，即禁用这个功能，使用的话则应该使用 1h ， 5m ， 10s 这种类型的字符串。 在这部分配置中，还有两类重要配置 clean_* 和 close_* 定义了 filebeat 具体的工作行为，它们涉及到了一部分 filebeat 的工作原理。\n我们可以简单认为 filebeat 会周期性地扫描我们定义的文件，每个文件都会启动 Harvester 来进行处理，同时 Harvester 会将一些中间信息记录到 registry 中，它可以认为是 filebeat 进行作业时的中间目录，在 Linux 系统中的默认路径是 /var/lib/filebeat/registry 。如果只是处理某个文件的 Harvester 关闭，当这个文件有新变动，会启动新的 Harvester 并根据 registry 中的信息对文件进行处理，比较关键的值就是文件读取的偏移量，而如果 registry 中的关于某个文件的数据已经清除，那么 Harvester 会把它当成全新文件来处理。\nclean_* 定义了 registry 清理行为的相关内容，而 close_* 定义了 Harvester 关闭行为的相关内容。\nclose_* 有如下几个具体配置项：\nclose_eof ：当文件读取完毕时，是否关闭 Harvester ，默认为 false 。 close_inactive ：当文件读取完毕时， Harvester 并不会马上关闭，而是等待一段时间，如果超过这段时间仍然没有新的数据，则关闭 Harvester ，默认为 5m 。 close_timeout ： Harvester 超时时长，当 Harvester 工作时间超出这个值则关闭 Harvester ，默认为 0 ，也就是禁用超时功能。 close_renamed ：当文件被重命名时，是否关闭 Harvester ，默认为 false 。 close_removed ：当文件被删除时，是否关闭 Harvester ，默认为 true 。 clean_* 有如下几个具体配置项：\nclean_inactive ：当文件没有产生新的 …","date":1682342760,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"619b684331f539356900ab4370656471","permalink":"https://yuweizzz.github.io/post/detail_about_filebeat_config_file/","publishdate":"2023-04-24T13:26:00Z","relpermalink":"/post/detail_about_filebeat_config_file/","section":"post","summary":"这篇笔记主要介绍 filebeat 配置文件各参数含义和一些配置实例。\n","tags":["filebeat"],"title":"filebeat 配置参考","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些常用的 ffmpeg 命令。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 正文 ffmpeg 可以用来处理音视频，以下是一些常用的处理命令：\n# 按照给定时间长度从头截取视频或音频，不做转码 $ ffmpeg -i input.mp3 -t 03:30 -c:v copy -c:a copy output.mp3 # 从某个时间开始，按照给定时间长度截取视频或音频，不做转码 $ ffmpeg -ss 01:00 -i input.mp3 -t 03:30 -c:v copy -c:a copy output.mp3 # 以上例子会从文件开始一分钟后截取三分半钟的片段，相当于新文件会是原有文件的 01:00 到 04:30 这段内容 # 提取音频，去掉视频 $ ffmpeg -i input.mp4 -c:a copy -vn output.mp4 # 提取视频，去掉音频 $ ffmpeg -i input.mp4 -c:v copy -an output.mp4 # 获取文件信息 $ ffprobe -v quiet -print_format json -show_format -show_streams input.mp4 # 制作音频部分的淡入和淡出效果 $ ffmpeg -i input.mp4 -af \u0026#34;afade=t=in:st=0:d=5,afade=t=out:st=210:d=5\u0026#34; output.mp4 # afade=t=in 代表 audio fade in 效果， st 代表开始的时间， d 代表持续时长 # 以上例子会在视频开始前 5 秒声音渐大， 210 秒到 215 秒声音渐小 # 制作视频部分的淡入和淡出效果 $ ffmpeg -i input.mp4 -vf \u0026#34;fade=in:0:3,fade=out:210:5\u0026#34; output.mp4 # 类似于音频的例子，但它会应用于视频部分 # 使用图片来生成一图流视频 $ ffmpeg -r 15 -f image2 -loop 1 -i input.jpg -i input.mp3 -s 1920x1080 -pix_fmt yuv420 \\ -t 210 -vcodec libx264 output.mp4 # -r 代表帧率，帧率越高画面越流畅，一图流视频的帧率无需过高 # -t 代表持续时间，在使用连续图片的情况下可以不使用 -loop 和 -t 参数，但单张图片需要使用 -loop 1 代表无限循环单张图片 # -f 代表使用 image2 来处理输入， -pix_fmt 代表图片的输入格式 # -s 代表分辨率， -vcodec 代表编码格式 # 音视频推流 $ ffmpeg -i input.mp4 -re -stream_loop -1 -c copy -f flv \u0026#34;rtmp://rtmp_endpoint\u0026#34; # -re 代表 Read input at native frame rate 即以输入的原有帧率来读取 # -stream_loop -1 代表推流次数为 -1 ，也就是无限循环当前输入 ","date":1681595160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"6148fa331a6e95804a2496ae30ae08d7","permalink":"https://yuweizzz.github.io/post/useful_command_about_ffmpeg/","publishdate":"2023-04-15T21:46:00Z","relpermalink":"/post/useful_command_about_ffmpeg/","section":"post","summary":"这篇笔记用来记录一些常用的 ffmpeg 命令。\n","tags":["ffmpeg"],"title":"ffmpeg 命令笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录硬件测试工具 fio 和 stream 的使用。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 硬盘性能测试工具 fio fio 是比较常用的硬盘性能测试工具，它可以模拟不同读写场景并得到对应的 IOPS 和硬盘带宽，延迟等性能参数。\nio engine 使用 fio 需要指定 io engine ，实际上就是指定读写存储设备需要使用到的系统调用库，测试硬盘性能一般会使用 sync 或者 libaio ，其中 sync 使用的是 Linux 中基本的 IO 系统调用库，而 libaio 是 Linux 原生异步 IO 库。\nio pattern fio 可以指定具体的读写模式来模拟现实场景，比较常用的有下面几个：\nread 顺序读 write 顺序写 randread 随机读 randwrite 随机写 readwrite 顺序读写 randrw 随机读写 一般来说，机械硬盘的强项是顺序读写，而随机读写是机械硬盘的弱项；固态硬盘的强项是随机读写，顺序读写虽然表现也不错但是并不占优势，因为固态硬盘的价格比机械硬盘更高。\n像是常规的硬盘测试可以是裸盘测试或是基于文件系统测试，通常会结合顺序读写和随机读写来考量硬盘的综合性能。\n可以将如下测试过程作为参考，这是基于裸盘进行测试的：\n顺序写满 -\u0026gt; 256k 顺序写 -\u0026gt; 256k 顺序读 -\u0026gt; 4k 顺序写 -\u0026gt; 4k 顺序读 -\u0026gt; 随机写满 -\u0026gt; 4k 随机写 -\u0026gt; 4k 随机读\n顺序读写一般会以比较大的块大小去操作，通常可以测试出比较高的带宽数据，但是随机读写一般会使用 4k 或者 16k 这些比较小且贴合操作系统的内存页大小去操作，这里更重要的数据应该是 IOPS ，同时也是衡量它们性能的重点。\nfio option 下面是一些重要的 fio 命令参数：\n-name 可以命名当前进行的测试任务。 -filename 用来指定测试对象，如果是类似于 /dev/sdb 这样的设备则说明测试对象是裸盘，如果是类似于 /home/file.img 这样的文件路径则说明测试对象是基于文件系统的。如果是裸盘测试很有可能把原有的文件系统写坏，使用时要注意数据安全。 -ioengine 用来指定进行测试的 IO 函数库，也就是前述的 sync 或者 libaio 。 -rw 用来指定具体的读写模式，也就是前述的 io pattern 。 -direct 决定使用 buffered IO 或者是 non-buffered IO ，其中 buffered IO 是大多数操作系统的默认 IO 方式，数据会先被内核从硬盘中读取到缓冲中，再由用户层将内核缓冲作为第一层来读取，使用 -direct=1 可以屏蔽内核缓冲的使用，使用 non-buffered IO 来直接读取硬盘，大部分情况下应该使用 non-buffered IO 进行测试。 -iodepth 主要作用于异步 IO 库，也就是 libaio ，它可以决定并发性地发起多少个 IO 请求，这个选项在使用 buffered IO 和同步 IO 库时基本不起作用。 -bs 用来指定读写的 IO 块大小。 -size 用来指定读写内容的大小，在读写给定大小内容完成后就会结束任务，如果不指定 -size 还会根据 -runtime 决定任务的运行时间。 -numjobs 和 -thread 是控制测试任务的并发方式，如果使用 -thread 则使用单进程多线程的方式来运行，如果不使用 -thread 则使用多进程的方式来运行，而 -numjobs 则用来控制线程数量，在使用并发的情况下，可以使用 -group_reporting 来综合所有执行结果以输出测试报告。 参考命令如下：\n# 顺序读 $ fio -name=read -filename=/dev/sdb -rw=read -ioengine=libaio -direct=1 -iodepth=32 -bs=256k -size=8G \\ -numjobs=4 -thread -group_reporting # 顺序写 $ fio -name=write -filename=/dev/sdb -rw=write -ioengine=libaio -direct=1 -iodepth=32 -bs=256k -size=8G \\ -numjobs=4 -thread -group_reporting # 随机写 $ fio -name=randwrite -filename=/dev/sdb -rw=randwrite -ioengine=libaio -direct=1 -iodepth=32 -bs=4k -size=8G \\ -numjobs=4 -thread -group_reporting # 随机读 $ fio -name=randread -filename=/dev/sdb -rw=randread -ioengine=libaio -direct=1 -iodepth=32 -bs=4k -size=8G \\ -numjobs=4 -thread -group_reporting 内存性能测试工具 stream stream 是内存带宽性能的测试工具，主要通过内存运算操作来计算出机器的内存带宽性能。\n它的原理是通过在内存中划定三个数组，通过函数对不同数组内数据进行计算来得出内存的带宽性能，根据 stream 的设计，我们可以自行指定这三个数组的大小，并且这个大小应该超过 CPU 的三级缓存大小，这样测试时才不会受到 CPU 缓存的影响，推荐是 4 倍的三级缓存大小。\n实际使用可以参考下面的过程：\n# 获取机器的 CPU 信息 $ lscpu | grep -E \u0026#39;Socket|L3\u0026#39; Socket(s): 1 L3 cache: 16384K # Socket(s) 可以得到机器有多少个物理 CPU ，一般服务器是单个，两个或者四个 CPU # L3 cache 可以得到单个物理 CPU 的三级缓存大小，整台机器的三级缓存大小应该是 Socket(s) * L3 cache Size # 下载源代码 $ wget http://www.cs.virginia.edu/stream/FTP/Code/stream.c # 编译需要确定 STREAM_ARRAY_SIZE 编译参数 $ gcc -O3 -fopenmp -DSTREAM_ARRAY_SIZE=8500000 -DNTIMES=10 stream.c -o stream # -O3 指定编译优化级别为最高 # -fopenmp 启用多核支持，这样 stream 会默认以核心数量来启动线程 # -DNTIMES 用来改变 stream 的运行次数， stream 默认会多次执行计算并取出最好的运行结果，默认值为 10 # -DSTREAM_ARRAY_SIZE 就是我们需要指定的数组大小，默认值为 10000000 # 如果 STREAM_ARRAY_SIZE 默认值已经大于通过机器的三级缓存总和计算的结果，则可以直接保持这个默认值 # 源代码部分参考： /* -------------------------------------------------- */ #ifndef STREAM_TYPE #define STREAM_TYPE double #endif static STREAM_TYPE a[STREAM_ARRAY_SIZE+OFFSET], b[STREAM_ARRAY_SIZE+OFFSET], c[STREAM_ARRAY_SIZE+OFFSET]; /* -------------------------------------------------- */ # 可以看到 STREAM_ARRAY_SIZE 默认使用 double 类型 # 假设 Socket(s)=1 和 L3 cache Size=16384K ，则 STREAM_ARRAY_SIZE 的计算公式为： # 16384(KB) * 1024 * 1 * 4 / 8(B) = 8388608 -\u0026gt; 8500000 # 假设 Socket(s)=1 和 L3 cache Size=40M ，则 STREAM_ARRAY_SIZE 的计算公式为： # 40(MB) * 1024 * 1024 * 1 * 4 / 8(B) = 20971520 -\u0026gt; 21000000 # 执行 stream $ ./stream # 以下是在一台云服务器上的运行结果： ------------------------------------------------------------- STREAM version $Revision: 5.10 $ ------------------------------------------------------------- This system uses 8 bytes per array element. ------------------------------------------------------------- Array size = 8500000 (elements), Offset = 0 (elements) Memory per array = 64.8 MiB (= 0.1 GiB). Total memory required = 194.5 MiB (= 0.2 GiB). Each kernel will be executed 10 times. The *best* time for each kernel (excluding the first iteration) will be used to compute the reported bandwidth. ------------------------------------------------------------- Number of Threads requested = 4 Number of Threads counted = 4 ------------------------------------------------------------- Your clock granularity/precision appears to be 1 microseconds. Each test below will take on the order …","date":1673364165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"4ca3227ca9f0b9d046af779e1d9ffe08","permalink":"https://yuweizzz.github.io/post/knowledge_about_hardware_benchmark_tool/","publishdate":"2023-01-10T15:22:45Z","relpermalink":"/post/knowledge_about_hardware_benchmark_tool/","section":"post","summary":"这篇笔记用来记录硬件测试工具 fio 和 stream 的使用。\n","tags":["fio","stream"],"title":"硬件测试工具使用笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Kubernetes 的相关知识，主要是 Kubernetes 实践过程中的相关内容。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 安装 kubernetes 这里主要记录一些 kubernetes 初始化遇到的问题和解决方法。\n首先是 kubelet ， kubeadm 和 kubectl 三个基本的软件，此外你还需要额外安装容器运行时，可以选择 docker ， containerd 或者 crio 等。\n根据官方文档和实际运行情况来看，由于国内网络的问题，为了避免拉取镜像失败，最好把容器运行时配置中的 pause 镜像修改掉，以 crio 为例子：\n$ cat /etc/crio/crio.conf ...... [crio.image] pause_image = \u0026#34;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6\u0026#34; ...... $ systemctl enable crio $ systemctl reload crio 修改容器运行时配置并重启服务后，还需要启用内核模块和修改一些内核参数：\n# 修改内核参数 $ echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward # 不开启 ip forward 初始化会报错 # 持久化内核参数 $ echo net.ipv4.ip_forward = 1 \u0026gt; /etc/sysctl.d/kubernetes.conf # 启用内核模块 $ modprobe br_netfilter # 不启用 br_netfilter 初始化会报错 然后就可以使用 kubeadm 来初始化控制平面：\n# 直接调用 init 命令来进行初始化 $ kubeadm init \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --pod-network-cidr 10.244.0.0/16 \\ --service-cidr 10.96.0.0/16 \\ --cri-socket unix:///var/run/crio/crio.sock \\ --v 5 # 生成默认配置文件并进行修改，然后根据配置文件进行初始化 $ kubeadm config print init-defaults --component-configs KubeProxyConfiguration,KubeletConfiguration \u0026gt; config.yaml $ kubeadm init --config config.yaml # config.yaml 中需要修改的地方有： # InitConfiguration.localAPIEndpoint.advertiseAddress 需要修改为主机 IP 地址 # InitConfiguration.nodeRegistration.name 按照需要修改为对应主机名 # InitConfiguration.nodeRegistration.criSocket 需要修改为正确的容器运行时监听地址，相当于 --cri-socket # ClusterConfiguration.imageRepository 需要修改为国内的镜像地址，相当于 --image-repository # ClusterConfiguration.networking.serviceSubnet 需要修改为 service 网络范围，相当于 --service-cidr # ClusterConfiguration.networking.podSubnet 需要修改为 pod 网络范围，相当于 --pod-network-cidr # KubeProxyConfiguration.mode 可以按照需要修改为 ipvs ，否则默认使用 iptables 初始化完成后，可以使用 kubectl 来查看集群状态：\n# 复制证书信息到当前用户工作目录下 $ cp /etc/kubernetes/admin.conf $HOME/.kube/config $ kubectl get node $ kubectl get pods -A # 直接声明 KUBECONFIG 变量指定证书信息 $ export KUBECONFIG=/etc/kubernetes/admin.conf $ kubectl get node $ kubectl get pods -A 这时节点仍处于 NotReady 状态，因为网络插件还没有安装，可以选择需要的网络插件进行安装，这里以 flannel 为例子：\n$ curl -L -O https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml # 修改 kube-flannel.yml 中 ConfigMap 的 Network 为需要的 podSubnet $ kubectl apply -f kube-flannel.yml 安装完成后整个节点就会处于 Ready 状态，主节点安装完成。接下来如果要为这个集群添加工作节点，那么需要在工作节点上安装 kubelet ， kubeadm 和容器运行时，为了保证工作节点的正常运行，也应该开启对应的内核模块并修改内核参数，然后执行对应的命令即可：\n# 主节点生成 token $ kubeadm token create --print-join-command # 生成的 token 可以在 secret 中找到，对应的 type 为 bootstrap.kubernetes.io/token $ kubectl get secret -n kube-system # 工作节点通过主节点提供的 token 加入集群 $ kubeadm join \u0026lt;api-server-endpoint\u0026gt; --token \u0026lt;discovery-token\u0026gt; --discovery-token-ca-cert-hash \u0026lt;discovery-token-ca-cert-hash\u0026gt; 允许控制平面节点调度 kubernetes 的控制平面节点默认是不允许调度的，它在一些低版本中也称为 Master 节点或主节点，可以通过 kubectl describe nodes \u0026lt;name\u0026gt; 看到主节点信息中带有污点 node-role.kubernetes.io/control-plane:NoSchedule ，可以通过去掉这部信息来允许主节点进行 Pod 调度。\n# 假设主节点名称为 k8s-master # 去除主节点禁止调度的污点 $ kubectl taint node k8s-master node-role.kubernetes.io/control-plane- # 还原主节点禁止调度的污点 $ kubectl taint node k8s-master node-role.kubernetes.io/control-plane=\u0026#34;\u0026#34;:NoSchedule 修改 Pod 的 Host 有时候需要把一些特殊域名指定为固定的 IP 地址，可以通过 hostAliases 来实现，它相当于向 /etc/hosts 中添加了对应的信息。\napiVersion: v1 kind: Pod metadata: name: hostaliases-pod spec: restartPolicy: Never hostAliases: - ip: \u0026#34;127.0.0.1\u0026#34; hostnames: - \u0026#34;foo.local\u0026#34; - \u0026#34;bar.local\u0026#34; - ip: \u0026#34;10.1.2.3\u0026#34; hostnames: - \u0026#34;foo.remote\u0026#34; - \u0026#34;bar.remote\u0026#34; containers: - name: cat-hosts image: busybox:1.28 command: - cat args: - \u0026#34;/etc/hosts\u0026#34; 上述文件是官方文档给出的实例，如果是 pod 则应该将 hostAliases 添加到 .spec.hostAliases 这个字段中，而在 deployment 中则应该添加到 .spec.template.spec.hostAliases 这个字段中。\nbr_netfilter 的重要作用 在 Pod 中通过 service 的名称来访问对应的服务时， br_netfilter 起到极为重要的作用，建议把这个模块写入到模块开机启动配置 /etc/modules-load.d/modules.conf 当中。\n由于一开始没有启用这个模块，导致所有域名访问方式都超时失效，通过使用 kubectl run dnsutils --image=mydlqclub/dnsutils:1.3 --command -- sleep 3600 启动 Pod 来进行 dig 排查，结果返回了奇怪的报错信息： reply from unexpected source: 10.244.0.22#53, expected 10.96.0.10#53 ，在查看官方 issue 之后，直接在对应节点上执行 modprobe br_netfilter 马上就解决了问题。\n其实这里也侧面反映出了 service 的服务原理， Pod 所配置的 /etc/resolv.conf 是 kube-dns service 的 IP 地址 10.96.0.10 ，所以 DNS 请求直接向这个地址发起请求，数据包正常从 Pod 内经过 kube-proxy 后发往 kube-dns 的任一 Pod 中，但是在数据包返回时，由于没有启用 br_netfilter 模块， nat 没有正常修改返回地址，导致最终返回到 Pod 中时，远端 IP 地址是 kube-dns 其中某一个 Pod IP ，被 Pod 认为不是正确的返回包直接丢弃，产生超时报错。\n更新镜像和回退 通过某个 deployment 作为实例：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx …","date":1662823365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"f80738ca7de70989940af97cbb598564","permalink":"https://yuweizzz.github.io/post/practice_notes_about_kubernetes/","publishdate":"2022-09-10T15:22:45Z","relpermalink":"/post/practice_notes_about_kubernetes/","section":"post","summary":"这篇笔记用来记录一些 Kubernetes 的相关知识，主要是 Kubernetes 实践过程中的相关内容。\n","tags":["kubernetes"],"title":"Kubernetes 实践笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Lua 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ lua 是一门高效的，易于嵌入其他应用程序的轻量级脚本语言。 lua 的高效快速主要是由于 lua 本身的底层设计以及非常高效的 Lua 解释器。这里主要会记录一些 lua 入门学习时一些遇到的问题。\nLua String 处理 lua 的 string 类型数据是不可改变的，大部分情况下要依靠 string 库来处理。\nstring 拼接 strings_a = \u0026#34;a\u0026#34; strings_b = \u0026#34;b\u0026#34; strings_c = \u0026#34;c\u0026#34; print(strings_a .. strings_b .. strings_c) -- output: -- abc string 替换和截取 一般使用 string.gsub 来替换字符串内容，而 string.sub 是用来截取字符串。\nstrings = \u0026#34;abcde\u0026#34; print(string.gsub(strings, \u0026#34;a\u0026#34;, \u0026#34;z\u0026#34;)) -- output: -- zbcde 1 -- 返回值分别是 gsub 执行后的字符串和具体发生替换的次数，可以根据第二个参数判断是否已经执行替换 strings = \u0026#34;aaaae\u0026#34; print(string.gsub(strings, \u0026#34;a\u0026#34;, \u0026#34;z\u0026#34;, 2)) -- output: -- zzaae 2 -- 可以通过传进第四个参数限制替换次数 strings = \u0026#34;abcde\u0026#34; print(string.sub(strings, 3)) -- output: -- cde -- 从给定位置开始截取到最后一个字符 strings = \u0026#34;abcde\u0026#34; print(string.sub(strings, 3, 4)) -- output: -- cd -- 截取起止位置都确定的字符串 string find string.find 可以用来进行字符串搜索，需要注意的用法是带有特殊字符时，应该指定为 plain 模式。\n-- 普通字符串搜索 strings = \u0026#34;abcde\u0026#34; print(string.find(strings, \u0026#34;a\u0026#34;)) -- output: -- 1 1 -- 成功则返回匹配字符的起止位置索引，失败则返回 nil -- 带有特殊字符的字符串搜索 strings = \u0026#34;abcde.*\u0026#34; print(string.find(strings, \u0026#34;.*\u0026#34;)) -- output: -- 1 7 -- 默认以正则模式进行搜索 print(string.find(strings, \u0026#34;.*\u0026#34;, 1, true)) -- output: -- 6 7 -- 指定为 plain 模式，不再以正则模式进行搜索 string match 匹配取值有 match 和 gmatch 两种，前者用于直接匹配结果，后者可以返回迭代器函数，用于循环场景。\nstrings = \u0026#34; 1 + 1 =2\u0026#34; print(string.match(strings, \u0026#34;^%s*(.-)%s*=.*\u0026#34;)) -- output: -- 1 + 1 print(string.match(strings, \u0026#34;^%s*.-%s*=(.*)\u0026#34;)) -- output: -- 2 -- 匹配成功则返回整个匹配的字符串，由于使用了括号进行子模式匹配，所以只返回了匹配部分，其中 .- 代表任意字符的非贪婪匹配 iterator = string.gmatch(strings, \u0026#34;%d\u0026#34;) for each in iterator do print(each) end -- output: -- 1 -- 1 -- 2 -- 匹配成功则可以在每次循环中得到对应的匹配部分 Lua Table lua table 是一种异于其他编程语言的数据类型，它是动态增长的字典类容器，同时具有数组的特性，如果仔细研究它的内部结构，可以发现它是有两者混合构成的。\nt = {a=\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, c=\u0026#34;C\u0026#34;, D} print(t[0]) -- output: nil print(t[\u0026#34;a\u0026#34;]) -- output: A print(t[1]) -- output: B print(t[\u0026#34;c\u0026#34;]) -- output: C print(t[2]) -- output: nil print(t[3]) -- output: nil -- D 这个值是测试时疏漏了引号产生的错误，它会认为是一个值为 nil 的变量，相当于空洞 通过这个例子可以简单看出它们的混合特点，键值对和数组是互相独立的，需要使用明确的键取对应的值，数组值则使用数字索引，并且索引是从 1 开始而不是 0 ，要注意这里只是为了弄清楚它们的特性才使用了混合结构，实际中基本不会这样使用。\nLua 模块和 Luarocks 常规的 lua 脚本需要依靠两个重要变量 package.path 和 package.cpath 来获取一些运行模块信息，它们主要区别在于模块是由 lua 语言实现还是由 c 语言实现。\n$ cat p.lua print(package.path) print(package.cpath) $ lua p.lua ./?.lua;/usr/share/lua/5.1/?.lua;/usr/share/lua/5.1/?/init.lua;/usr/lib64/lua/5.1/?.lua;/usr/lib64/lua/5.1/?/init.lua ./?.so;/usr/lib64/lua/5.1/?.so;/usr/lib64/lua/5.1/loadall.so # 可以通过指定环境变量 LUA_PATH 和 LUA_CPATH 来改变这两个值，其中 ;; 的第二个 ; 会被替换为原有的默认值 $ LUA_PATH=\u0026#34;/tmp/?.lua;;\u0026#34; LUA_CPATH=\u0026#34;/tmp/?.so;;\u0026#34; lua p.lua /tmp/?.lua;./?.lua;/usr/share/lua/5.1/?.lua;/usr/share/lua/5.1/?/init.lua;/usr/lib64/lua/5.1/?.lua;/usr/lib64/lua/5.1/?/init.lua; /tmp/?.so;./?.so;/usr/lib64/lua/5.1/?.so;/usr/lib64/lua/5.1/loadall.so; 我们如果想使用别人写好的 lua 脚本，通常需要自己去对应的发布地址下载并自行安装，但是 lua 也有类似于模块管理器一样的软件 luarocks ，使用它我们可以更方便管理我们需要用到的模块，也可以把我们自己写好的脚本提供给他人下载使用。\n# 使用 luarocks 下载安装 luasql # --server 可以指定 luarocks 服务器 $ luarocks install luasql-mysql --server https://luarocks.cn MYSQL_INCDIR=/usr/include/mysql/ MYSQL_LIBDIR=/usr/lib64/mysql # 编译过程可能需要指定一些依赖路径，可以通过环境变量来指定，具体变量信息可以在对应模块的 rockspec 查找 # luasql-mysql rockspec package = \u0026#34;LuaSQL-MySQL\u0026#34; version = \u0026#34;2.3.5-1\u0026#34; source = { url = \u0026#34;git://github.com/keplerproject/luasql.git\u0026#34;, branch = \u0026#34;v2.3.5\u0026#34;, } description = { summary = \u0026#34;Database connectivity for Lua (MySQL driver)\u0026#34;, detailed = [[ LuaSQL is a simple interface from Lua to a DBMS. It enables a Lua program to connect to databases, execute arbitrary SQL statements and retrieve results in a row-by-row cursor fashion. ]], license = \u0026#34;MIT/X11\u0026#34;, homepage = \u0026#34;http://www.keplerproject.org/luasql/\u0026#34; } dependencies = { \u0026#34;lua \u0026gt;= 5.1\u0026#34; } external_dependencies = { MYSQL = { header = \u0026#34;mysql.h\u0026#34; } } build = { type = \u0026#34;builtin\u0026#34;, modules = { [\u0026#34;luasql.mysql\u0026#34;] = { sources = { \u0026#34;src/luasql.c\u0026#34;, \u0026#34;src/ls_mysql.c\u0026#34; }, libraries = { \u0026#34;mysqlclient\u0026#34; }, incdirs = { \u0026#34;$(MYSQL_INCDIR)\u0026#34; }, libdirs = { \u0026#34;$(MYSQL_LIBDIR)\u0026#34; } } } } # 为了维持解释器环境不被污染，可以使用 --tree 指定模块的安装路径 $ luarocks install luasql-mysql --tree $(pwd) # 如果不指定 --tree ，会根据当前用户决定安装路径 # root 用户一般会将 lua 文件安装到 /usr/local/share/lua/ 中，而动态库文件则是 /usr/local/lib/lua/ # 删除已经安装的模块，必须指定 --tree $ luarocks purge luasql-mysql --tree $(pwd) Lua Metatable lua metatable 经常用来定义模块。\nlocal _M = { _VERSION = \u0026#39;1.0\u0026#39; } local mt = { __index = _M } function _M.new(left, right) return setmetatable( { left = left, right = right }, mt ) end function _M:add() return self.left + self.right end function _M:sub() return self.left - self.right end function _M:sum(data) return …","date":1662320685,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"f7ab4e6639e10d78d648acb751960869","permalink":"https://yuweizzz.github.io/post/study_notes_about_lua/","publishdate":"2022-09-04T19:44:45Z","relpermalink":"/post/study_notes_about_lua/","section":"post","summary":"这篇笔记用来记录一些 Lua 的相关知识。\n","tags":["Lua","Luarocks"],"title":"Lua 学习笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录如何使用 Logstash S3 插件。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 为了减轻 Elasticsearch 集群存储空间的负担，需要将较旧的日志额外使用对象存储保存，以防不时之需。\n以下 Logstash 配置可以在写入 Elasticsearch 集群的同时，将日志保存一份到对象存储中：\n# 消费 Kafka 消息 input { kafka { bootstrap_servers =\u0026gt; [\u0026#34;localhost:9092\u0026#34;] topics_pattern =\u0026gt; \u0026#34;logs-*\u0026#34; group_id =\u0026gt; \u0026#34;logs-consumer-group\u0026#34; # 按照对应的 Kafka topic 分区来设置最佳，一般使用 3 个分区 consumer_threads =\u0026gt; 3 codec =\u0026gt; json # 从分区的最开始进行消费 auto_offset_reset =\u0026gt; \u0026#34;earliest\u0026#34; partition_assignment_strategy =\u0026gt; \u0026#34;round_robin\u0026#34; # 将 Kafka 来源信息写入到 metadata 中，后续可以使用 %{[@metadata][kafka]} 来取需要的字段 decorate_events =\u0026gt; true } } # 同时写入到 Elasticsearch 集群和对象存储 output { elasticsearch { index =\u0026gt; \u0026#34;%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;elastic_password\u0026#34; } s3 { # 替换为对应的密钥信息 \u0026#34;access_key_id\u0026#34; =\u0026gt; \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; \u0026#34;secret_access_key\u0026#34; =\u0026gt; \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; # 以腾讯云为例，假设对象存储访问地址为 https://my-cos.cos.ap-guangzhou.myqcloud.com # 拆分对应的桶名称 bucket 和访问后缀 endpoint \u0026#34;bucket\u0026#34; =\u0026gt; \u0026#34;my-cos\u0026#34; # 如果是 AWS S3 可以省去 endpoint \u0026#34;endpoint\u0026#34; =\u0026gt; \u0026#34;https://cos.ap-guangzhou.myqcloud.com\u0026#34; # 对应的 region \u0026#34;region\u0026#34; =\u0026gt; \u0026#34;ap-guangzhou\u0026#34; # 存储目标路径 \u0026#34;prefix\u0026#34; =\u0026gt; \u0026#34;%{[@metadata][kafka][topic]}/%{+YYYY-MM-dd}\u0026#34; \u0026#34;codec\u0026#34; =\u0026gt; \u0026#34;json_lines\u0026#34; \u0026#34;encoding\u0026#34; =\u0026gt; \u0026#34;gzip\u0026#34; \u0026#34;validate_credentials_on_root_bucket\u0026#34; =\u0026gt; \u0026#34;false\u0026#34; # 上传对象的存储类型，默认为标准存储 STANDARD \u0026#34;storage_class\u0026#34; =\u0026gt; \u0026#34;STANDARD\u0026#34; # 当文件大小达到 500M 时， logstash 将消费完成的临时文件上传到对象存储 \u0026#34;rotation_strategy\u0026#34; =\u0026gt; \u0026#34;size\u0026#34; \u0026#34;size_file\u0026#34; =\u0026gt; \u0026#34;524288000\u0026#34; \u0026#34;additional_settings\u0026#34; =\u0026gt; { \u0026#34;force_path_style\u0026#34; =\u0026gt; \u0026#34;true\u0026#34; \u0026#34;follow_redirects\u0026#34; =\u0026gt; \u0026#34;false\u0026#34; } } } 比较常用的 S3 对象存储类型有：\nS3 Standard ：标准存储，也是通用型的存储类型，对应 S3 插件 storage_class 中的 STANDARD 。 S3 Standard-IA ：低频存储，用于访问量较低的存储类型，对应 S3 插件 storage_class 中的 STANDARD_IA 。 S3 One Zone-IA ：单 Zone 的低频存储，它的 Availability Zones 比 S3 Standard-IA 少，对应 S3 插件 storage_class 中的 ONEZONE_IA 。 S3 Reduced Redundancy Storage ：低冗余性，非关键性的高频访问存储类型，对应 S3 插件 storage_class 中的 REDUCED_REDUNDANCY 。 当使用 S3 插件作为 Logstash 的 input 时，需要将对象存储中的存储类型恢复为 STANDARD 才能正常读取，因为云厂商提供的对象存储服务通常会在一定的时间后将它们转化为归档存储。\n以下 Logstash 配置可以将对象存储中日志文件还原到 Elasticsearch 集群：\n# 消费对象存储中的存储文件，输出到 Elasticsearch 集群 input { s3 { # 替换为对应的密钥信息 \u0026#34;access_key_id\u0026#34; =\u0026gt; \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; \u0026#34;secret_access_key\u0026#34; =\u0026gt; \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; # 以腾讯云为例，假设对象存储访问地址为 https://my-cos.cos.ap-guangzhou.myqcloud.com # 拆分对应的桶名称 bucket 和访问后缀 endpoint \u0026#34;bucket\u0026#34; =\u0026gt; \u0026#34;my-cos\u0026#34; # 如果是 AWS S3 可以省去 endpoint \u0026#34;endpoint\u0026#34; =\u0026gt; \u0026#34;https://cos.ap-guangzhou.myqcloud.com\u0026#34; # 对应的 region \u0026#34;region\u0026#34; =\u0026gt; \u0026#34;ap-guangzhou\u0026#34; \u0026#34;codec\u0026#34; =\u0026gt; \u0026#34;json\u0026#34; # 消费完成后关闭输入，不等待新的文件产生 \u0026#34;watch_for_new_files\u0026#34; =\u0026gt; false # 可以只消费某些特定前缀的存储文件 \u0026#34;prefix\u0026#34; =\u0026gt; \u0026#34;logs-\u0026#34; } } output { elasticsearch { index =\u0026gt; \u0026#34;logs-from-cos-%{+YYYY.MM.dd}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;elastic_password\u0026#34; } } ","date":1660575600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"921863342b010f7590a000e54185bf31","permalink":"https://yuweizzz.github.io/post/using_s3_plugin_in_logstash/","publishdate":"2022-08-15T15:00:00Z","relpermalink":"/post/using_s3_plugin_in_logstash/","section":"post","summary":"这篇笔记用来记录如何使用 Logstash S3 插件。\n","tags":["Logstash"],"title":"Logstash S3 插件使用","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录将玩客云主机作为家用服务器的搭建经历。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 购买主机和系统刷入 玩客云主机是迅雷出品的智能硬件，它会使用客户闲置的计算资源，同时给予可提现的玩客币作为回报，类似于比特币的挖矿行为，但是现在玩客币交易受限，所以被囤积挖矿的二手玩客云主机可以用较低的价格入手，在拼多多上只要 50 块钱左右就可以买到。\n主机的硬件配置为 4 核 1.5G ARM 架构 CPU ， 1GB RAM 和 8GB eMMC ROM ，千兆网卡和两个 USB 2.0 接口以及 HDMI 接口，作为轻量级内网服务器完全是足够的。\n刷机资源在网上也是非常之多，可以在恩山论坛或者 B 站都可以找到。\n刷机过程大概需要如下步骤：\n拆开主机外壳，在短接电路板上的触点的情况下，使用 USB 线连接电脑刷入底包。 完成后使用制作完成的启动 U 盘，调用脚本将系统数据写入到 eMMC ROM 中。 这里涉及了一些硬件嵌入式开发的知识，但是单从 Linux 系统的角度上看， eMMC ROM 就是作为系统盘的块设备，执行 U 盘中的安装脚本只是跳过了常规的系统安装步骤，直接将完整的系统数据写入 eMMC ROM 中，后续主机就可以从 eMMC ROM 中的系统启动。\n最终使用的镜像为 Armbian 20.12 Buster with Linux 5.9.0-rc7-aml-s812 。\n修改网络环境 现有的环境下家庭宽带申请公网 ip 地址比较困难，最终决定直接使用 IPv6 来实现公网访问。\n组网方案 原有的网络设定是光猫直接使用路由模式，路由器接入光猫千兆口作为子设备，上网设备直接接入路由器，这是比较普遍的家庭宽带组网方案。\n新的组网方案使用了光猫桥接和路由器拨号的组合模式。\n光猫桥接部分需要设置的东西比较少，只需要将光猫的因特网接口从路由模式更改为桥接模式即可，桥接后光猫主要承担光电转换的工作。\n路由器拨号部分需要修改路由器的 wan 口设置，使用原有光猫宽带账号进行 pppoe 拨号上网，同时启用 IPv6 wan ，并且设置 wan 口向运营商上游索取 IPv6 地址的方式，这样 wan 口就可以拿到运营商分配的 IPv6 地址，它通常是 64 位掩码的。\n在拿到运营商分配的 IPv6 地址之后，wan 口设置打开 prefix delegation 和 Rapid-commit ， lan 口设置关闭 DHCPv6 功能，开启 IPv6 radvd 服务。\n完成了上述设置后，内网的支持 IPv6 的所有设备就都拥有了理论上全球可通信的 IPv6 地址。\nIPv6 理论知识 在 IPv6 中主要有 SLAAC 和 DHCPv6 两种地址获取方式，其中 DHCPv6 可以继续细分为 stateless 和 stateful 方式，以实际测试的情况来看，路由器使用任一方式都可以正常拿到 IPv6 地址。\nSLAAC 的一些相关名词的详细信息如下：\nSLAAC ：全称为 Stateless Address AutoConfiguration ，也就是无状态自动地址配置，它是 IPv6 的一大特色，可以为接入 IPv6 网络的设备自动配置 IPv6 地址。 NDP ：全称为 Neighbor Discovery Protocol ，也就是邻居发现协议， SLAAC 就是由这个协议提供的功能实现。 RA ：全称为 Router Solicitation ，也就是路由协定请求，是由 NDP 实现的报文。 RS ：全称为 Router Advertisement ，也就是路由通告，同样是由 NDP 实现的报文，作为 RA 报文的响应。 一般的 SLAAC 过程是这样的：假设我们的 IPv6 网络已经架设完成，那么新设备接入网络时会根据 NDP 协议定义向网络内的路由器发起 RS 协商请求，等待来自路由器的 RA 响应，最重要的响应信息其实是 IPv6 的子网前缀，新设备再按照这个信息生成自身的 IPv6 地址，成功接入网络，完成地址配置。\nDHCPv6 则是类似于传统 DHCP 服务的 IPv6 实现，同时拥有一些 IPv6 独有的功能，比如前缀代理 prefix delegation 就是 DHCPv6 的特色功能，简称为 DHCPv6-PD 。而 Rapid-commit 是 DHCPv6 的功能选项，用来支持 DHCP 地址快速分配。\nDHCPv6-PD 可以认为是划分下属子网并进行前缀下发的服务，在前述的拨号过程中，由于已经开启了 wan 口的 PD 功能，它就会作为客户端，向运营商获取子网前缀，然后在这个子网前缀允许的范围内再次划分子网，向下属的 lan 口进行派发。\nDHCPv6-PD 的意义在于路由自动协商，在路由器开启 PD 之后，运营商是 DHCPv6-PD 服务端，路由器是运营商服务端的直接客户端，路由器下联设备是运营商服务端的间接客户端，当路由器拿到运营商分配的子网前缀，它继续下发到设备的子网前缀信息会自动添加到运营商服务端的静态路由中，这也是子网设备能够全球访问的原因，如果路由器不开启 PD ，则 wan 口可以正常获取到 IPv6 但不进行子网前缀下发，内网设备只能通过保留回环进行 IPv6 通信，这时 lan 口的 DHCPv6 服务就显得无关紧要，因为它分配的网络只在本地局域网可用，不会通告给运营商，公网环境没有到达这个子网的路由。\nIPv6 radvd 服务用于辅助 SLAAC 地址获取，但是需要注意不开启这个服务设备仍然可以正常获取 IPv6 ，它的存在意义在于它会主动向网络链路中的设备广播 RA 报文，这样在 pppoe 重新拨号后， IPv6 地址的子网前缀改变，所有内网设备也会收到 RA 报文并重新配置地址，否则只能等待设备主动发起 RS 或者 DHCP 才能获取到新的 IPv6 地址。\n前面还讲到 DHCPv6 细分为 stateless 和 stateful 方式，实际上是这两种方法主要是网络参数的获取差异，其中 stateful 方式的 IPv6 地址， DNS 地址等网络参数都从 DHCP 服务器获取，而 stateless 方式的 IPv6 地址是从 RA 通告报文中获取子网前缀并自行生成， DNS 地址等网络参数仍然从 DHCP 服务器获取，决定使用何种获取方式完全由 RA 报文的标识决定，在使用硬件路由的情况下是很难直接改变的。\n实际表现 我的个人 Win10 电脑在网络环境设置完成后，在默认设置开启了 DHCP 的情况下，拿到了三个 IPv6 地址：\n第一个地址是通过 SLAAC 获取的地址， SLAAC 一般会使用网卡 mac 地址或者网络接口的随机 ID 来组合子网前缀生成 IPv6 地址， Win10 使用了随机 ID 和子网前缀来生成。 第二个地址是通过 DHCP 获取的地址， DNS 服务器地址也是在这一步获取的，由于我所有的内网设备只有 Win10 开启了 DHCP ，所以这个地址是 IPv6 局域网子网前缀下的第一个 ip 。 第三个地址是随机生成的临时地址，这是 IPv6 为了隐私安全设定的标准，会基于 SLAAC 的地址额外生成临时地址，用于对外主动通信，它的使用优先级是最高的。 我的玩客云 Linux 设备在网络环境设置完成后，拿到了一个 IPv6 地址：\n唯一的地址是通过 SLAAC 获取的，使用了网卡 mac 地址和子网前缀来生成。 在 Linux 中， IPv6 地址的生成方式是依赖于内核参数的：\nnet.ipv6.conf.eth0.autoconf ：参数为 0 代表禁用 SLAAC ，参数为 1 代表启用 SLAAC 。 net.ipv6.conf.eth0.accept_ra ：参数为 0 代表不接收 RA 报文，参数为 1 代表允许接收 RA 报文，参数为 2 代表允许接收和转发 RA 报文，这个参数对软路由能否启用 IPv6 协议栈非常关键。 net.ipv6.conf.eth0.addr_gen_mode ：参数为 0 代表使用网卡 mac 地址作为 SLAAC 的地址后缀，参数为 1 代表使用网络接口的随机 ID 作为 SLAAC 的地址后缀。 net.ipv6.conf.eth0.use_tempaddr ：参数为 0 代表不生成临时地址，参数为 1 代表使用并生成临时地址。 在我使用的 Armbian 中， accept_ra 参数为 1 ， autoconf 参数为 1 ， addr_gen_mode 参数为 0 ， use_tempaddr 参数为 0 ，所以最终只获取到一个由 SLAAC 生成的 IPv6 地址。\n现在的 DNS 设置仍然是由 ipv4 的 DHCP 服务配置的，因为 SLAAC 并不会配置 DNS 信息，如果想要在 Linux 中使用 DHCPv6 自动配置 DNS 信息，应该使用命令 dhclient -6 \u0026lt;interface\u0026gt; 主动拉起 DHCPv6 客户端，这样 DNS 设置就会由 DHCPv6 客户端接管配置。在 Armbian 中，和 DNS 配置相关的文件是 /etc/resolv.conf 和 /etc/resolvconf/run/resolv.conf ，它们之间应该通过软链接相关联。\n配置硬盘休眠 由于主机有 USB 2.0 接口，并且家里有一块闲置的西数 500GB SATA 硬盘，所以就将这块硬盘作为主机的外置存储。因为在实际使用过程中并不会经常地读写硬盘，所以我希望硬盘在平时可以处于休眠状态，并且在真正读写工作完成后继续休眠。\n在一开始时优先想到使用 hdparm 工具来设置硬盘的待机模式：\n# 使用 hdparm 设置硬盘的待机模式 $ hdparm -y /dev/sda $ hdparm -C /dev/sda # -y 可以使硬盘强制进入待机模式，不应该在系统盘上使用 # -C 可以查看目前硬盘状态，硬盘一般会处于 active 或者 standby # -Y 可以使硬盘强制进入睡眠状态，尽量不要使用 # 待机状态的硬盘可以被硬盘写入事件唤醒，但是睡眠状态的硬盘未经实测，很可能无法直接被硬盘写入事件唤醒 但是实际使用过程中却发生了错误，经过资料查阅之后得到大概的结论如下：\nhdparm 工具是通过 kernel 的 libata 子系统和 IDE 子系统来和硬盘交互，达到读取或者设置硬盘参数的目的。 玩客云主机使用 USB2.0 接口连接硬盘，系统实际上通过 UAS (USB Attached SCSI) 协议来和硬盘进行交互。 # 查看运行时的系统模块， …","date":1660402800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"3ae6e818781f75131dda45c48b7ed416","permalink":"https://yuweizzz.github.io/post/build_a_lightweight_server/","publishdate":"2022-08-13T15:00:00Z","relpermalink":"/post/build_a_lightweight_server/","section":"post","summary":"这篇笔记用来记录将玩客云主机作为家用服务器的搭建经历。\n","tags":["Linux","IPv6","Rsync"],"title":"搭建轻量级服务器","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Kubernetes 的相关知识，主要是 Kubernetes 基本原理的相关内容。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ Pod 的 DNS 策略 根据官网，目前 Kubernetes 支持以下特定 Pod 的 DNS 策略，设定字段为 .spec.dnsPolicy ：\nDefault ： Pod 从运行所在的节点继承 /etc/resolv.conf ，但是这种策略并不是 Pod 默认的 DNS 策略。 ClusterFirst ：使用集群提供的 DNS 服务，通常使用 CoreDNS 作为集群 DNS ， ClusterFirst 才是 Pod 默认的 DNS 策略。 ClusterFirstWithHostNet ：对于以 hostNetwork 方式运行的 Pod ，应该将它的 DNS 策略显式设置为 ClusterFirstWithHostNet 。 None ：允许 Pod 忽略集群中的 DNS 设置并使用 .spec.dnsConfig 作为 DNS 设置。 使用 Default 策略时， Pod 中的 /etc/resolv.conf 将和宿主机中的 /etc/resolv.conf 完全一致，这时就无法通过 service name 域名的方式去访问内部服务。\n使用 ClusterFirst 策略时，可以突破 Default 策略的限制，在 Pod 中直接通过 service name 域名去访问集群中 service 类型的资源，这时 Pod 中的 /etc/resolv.conf 一般会是如下的内容：\nsearch default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.96.0.10 options ndots:5 可以看到 search 把搜索域限定在集群中，所以我们就可以直接访问内部服务了，不跨命名空间的 service 访问将在 default.svc.cluster.local 域中搜索，因为这部分内容取自 default 命名空间中的 Pod ，不同命名空间中的 Pod 将会自动跟随当前所在空间；跨命名空间的 service 访问将在 svc.cluster.local 域中搜索，但没有显式给出命名空间，所以要由使用者自动指定 service name 加 namespace name 的域名。\n而 options 中的 ndots 是用来判断是否需要使用 search 查找域的凭据，如果给定域名中的 . 小于 ndots 定义的数值，则会先在所有 search 定义的域中查询，在这些查询都失败后再直接使用给定域名进行查询。当访问内部服务时，不论是否跨命名空间，给定域名是 service-name 或 service-name.namespace-name 这种形式，它们都会被 ndots 命中并执行 search 查找。\n使用 ClusterFirstWithHostNet 策略则是在 .spec.hostNetwork 值为 true 时所需要使用的关键字，不使用这个策略，则 .spec.hostNetwork 值为 true 的 Pod 无法访问集群内部服务。\n使用 None 策略可以参考官方给出的配置参考，定义自己需要的配置：\napiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsPolicy: \u0026#34;None\u0026#34; dnsConfig: nameservers: - 192.0.2.1 searches: - ns1.svc.cluster-domain.example - my.dns.search.suffix options: - name: ndots value: \u0026#34;2\u0026#34; - name: edns0 Service 类型 Kubernetes 的 sevice 资源有四种类型，它决定了集群服务是如何暴露的，设定字段为 .spec.type ：\nClusterIP ClusterIP 是默认和最常见的服务类型，成功创建资源后集群会自动分配服务的 IP 地址，这个 IP 地址和 Pod 网段是隔离的，它的网络通信依赖于 kube-proxy 组件。\napiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: - name: nginx image: nginx:stable ports: - containerPort: 80 name: http-web-svc --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app.kubernetes.io/name: proxy ports: - name: name-of-service-port protocol: TCP port: 80 targetPort: http-web-svc 通常会将 Pod 或者 Deployment 资源加上 label ，然后使用 Service selector 来选中这些资源，这样的 Service 就可以将请求均衡发送到 Pod 集合中。\nNodePort NodePort 是 ClusterIP 的扩展类型。除了集群内部的服务 IP 地址，还会在集群中所有节点的对应端口进行监听，并代理到各自节点中的 Pod 中，实现外部访问集群内服务。\napiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app.kubernetes.io/name: MyApp ports: - port: 80 targetPort: 80 nodePort: 30007 在 YAML 文件中， port 是 Pod 暴露服务的端口， targetPort 是 Service 暴露服务的端口，两者可以保持一致，由 targetPort 跟随 port 即可， nodePort 可以显式指定，也可以不进行指定并由集群自动分配，通常采取第二种方式，来避免手动指定的端口和已有的服务产生冲突。自动分配的范围由 apiServer 中的 --service-node-port-range 参数指定，默认值是 30000-32767 。\nLoadBalancer LoadBalancer 是基于云厂商提供服务实现的服务类型，它基于 NodePort 进行扩展，和云厂商的负载均衡器绑定，是更高级的外部访问集群内服务实现方式。\n可以认为这里 NodePort 是隐式实现的，并且云厂商还应该提供负载均衡器，它会对集群中所有节点代理，实现负载均衡。这样外部访问不再直接访问集群节点而是通过负载均衡器进行代理转发。\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 type: LoadBalancer status: loadBalancer: ingress: - ip: 192.0.2.127 在创建 LoadBalancer Service 时，通常只需要指定 LoadBalancer 作为 Service 类型即可，关于 port 的设置和 nodePort 类型相似。创建资源完成后会将负载均衡器信息报告在 status 中。\nExternalName ExternalName 是不直接关联 Pod 的服务类型，它实际是通过创建 CNAME 记录，来实现 Service 访问映射到其他域名。\napiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com 从官方文档中的实例可以非常明确这个服务类型的作用，它创建了一个内部服务，在提供给其他服务的同时，它本身并未在集群中运行，而这个服务只是在内部 DNS 创建 CNAME 记录，当其他服务访问时，实际上会通过 CNAME 访问到集群外部。也就是在访问 my-service.prod.svc.cluster.local 时，实际相当于访问 my.database.example.com 。\n除了映射外部域名，它也经常用于将其他命名空间的服务映射为本地命名空间的服务，方便内部服务的域名访问规范。\nKubernetes 身份认证与鉴权 在 Kubernetes 中， Pod 和其他的工作负载资源是我们最经常接触的资源对象，通常来说它们是直接和集群外部交互的，但是当它们需要进行集群内部交互，比如最常见的与 ApiServer 进行交互的时候，就会涉及到身份认证与鉴权，在 Kubernetes 中是一般使用基于 RBAC 的鉴权方式来实现。\n在这个认证体系中，比较重要的是 ServiceAccount 资源对象，它在每个新的 namespace 创建时都会随之默认生成，实际上也就是这个命名空间中的资源向 ApiServer 请求时默认使用的身份，当然除了这个默认对象之外也可以自行创建。\n# 来自 ingress nginx controller manifest 中的 ServiceAccount apiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx …","date":1660231365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"e07f3578fa16b762fd84ebdcc94f7f8a","permalink":"https://yuweizzz.github.io/post/knowledge_about_kubernetes/","publishdate":"2022-08-11T15:22:45Z","relpermalink":"/post/knowledge_about_kubernetes/","section":"post","summary":"这篇笔记用来记录一些 Kubernetes 的相关知识，主要是 Kubernetes 基本原理的相关内容。\n","tags":["kubernetes"],"title":"Kubernetes 知识笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来 Nginx 在实际应用场景的一些使用技巧。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ Nginx 获取内置变量 Nginx 经常需要使用内置变量和请求相关的变量来构成配置文件。\nNginx 可以轻松获取 HTTP 请求头，使用 $http_xxxx 来获取，比如 Host 可以使用 $http_host 来获取， User-Agent 可以使用 $http_user_agent 来获取。\n还有一些比较重要的内置变量：\n$remote_addr ：请求的来源 IP 地址，这里是指直接和当前 Nginx 交互的对端地址，所以它有可能只是源请求的一个代理地址。 $server_name ： Nginx 虚拟主机中的 server_name 。 $request ：原始的请求 URL 。 $scheme ：请求的协议，通常是 http 和 https 。 $request_method ：请求的方法，也就是 HTTP 的请求方法 GET ， POST 等。 $request_uri ：原始的请求路径和完整参数，是不可修改的。 $uri ：来自 $request_uri 中的路径部分，在经过重写行为后可能会与原来不同。 $args ：来自 $request_uri 中的参数部分。 $request_uri 可以拆解为 $uri$is_args$args ，其中 $is_args 就是 uri 请求中的 ? ，在实际请求没有参数的时候， $is_args 和 $args 都为空。\n$host ：以下出自官方文档的解释，可以看到取值优先级来源于以下三个值：请求 URL 中的主机部分；请求头中 Host 的部分，同 $http_host ； Nginx 虚拟主机中匹配命中的 server_name 。 $host：in this order of precedence: host name from the request line, or host name from the “Host” request header field, or the server name matching a request\n和请求时间相关的变量，更多用于排查问题和性能分析：\n$request_time ：本地响应请求所使用的时间，从客户端读取第一个字节开始计时，单位精确到毫秒，这个值计算的时间已经包括了 $upstream_response_time 。 $upstream_response_time ：使用代理的情况下，上游响应代理请求所使用的时间，单位精确到毫秒，这个值可以直观反应具体程序对请求的处理所需时间。 $time_local ：输出当前时间值，通常用于日志记录。 使用 map 映射变量 map 是 Nginx 模块 ngx_http_map_module 提供的关键字，可以基于某个变量的现有值制定一定规则，从而设置新的变量。\n# 来自 Nginx 官方文档的示例 map $http_user_agent $mobile { default 0; \u0026#34;~Opera Mini\u0026#34; 1; } # 获取 HTTP 请求头 User-Agent 的值，如果使用正则匹配到 Opera Mini 则设置变量 mobile 为 1 ，其他情况默认为 0 设置 json 格式日志 Nginx 原生的日志格式是列出了关键信息的单行文本，我们可以创建模拟 json 格式的日志写入规则。\n# 模拟 json 的日志格式 http { log_format main escape=json \u0026#39;{\u0026#39; \u0026#39;\u0026#34;datetime\u0026#34;: \u0026#34;$time_local\u0026#34;,\u0026#39; \u0026#39;\u0026#34;remote_addr\u0026#34;: \u0026#34;$remote_addr\u0026#34;,\u0026#39; \u0026#39;\u0026#34;http_host\u0026#34;: \u0026#34;$http_host\u0026#34;,\u0026#39; \u0026#39;\u0026#34;upstream_addr\u0026#34;: \u0026#34;$upstream_addr\u0026#34;,\u0026#39; \u0026#39;\u0026#34;request_method\u0026#34;: \u0026#34;$request_method\u0026#34;,\u0026#39; \u0026#39;\u0026#34;http_referer\u0026#34;: \u0026#34;$http_referer\u0026#34;,\u0026#39; \u0026#39;\u0026#34;status\u0026#34;: \u0026#34;$status\u0026#34;,\u0026#39; \u0026#39;\u0026#34;server_name\u0026#34;: \u0026#34;$server_name\u0026#34;,\u0026#39; \u0026#39;\u0026#34;request_uri\u0026#34;: \u0026#34;$request_uri\u0026#34;,\u0026#39; \u0026#39;\u0026#34;http_user_agent\u0026#34;: \u0026#34;$http_user_agent\u0026#34;,\u0026#39; \u0026#39;\u0026#34;http_x_forwarded_for\u0026#34;: \u0026#34;$http_x_forwarded_for\u0026#34;,\u0026#39; \u0026#39;\u0026#34;body_bytes_sent\u0026#34;: \u0026#34;$body_bytes_sent\u0026#34;,\u0026#39; \u0026#39;\u0026#34;upstream_response_time\u0026#34;: \u0026#34;$upstream_response_time\u0026#34;,\u0026#39; \u0026#39;\u0026#34;request_time\u0026#34;: \u0026#34;$request_time\u0026#34;\u0026#39; \u0026#39;}\u0026#39;; server { access_log logs/access.log main; } } Nginx 日志轮转 生产环境的 Nginx 日志应该定时切割和归档，否则可能会将存储空间占满，一般通过 logrotate 或者 crontab 完成这项工作。\n# Nginx 切割日志实例 $ cat /etc/logrotate.d/nginx /usr/local/openresty/nginx/logs/*.log { daily # 每日执行一次日志轮转 rotate 7 # 历史日志保留 7 份 missingok # 忽略文件不存在的报错 dateext dateformat -%Y%m%d%s # 以固定的日期格式重命名历史日志 compress delaycompress # 启用日志压缩，并在下次轮转时，压缩上一份历史日志 notifempty # 空文件则不执行轮转 sharedscripts # 使用 *.log 说明目录可能存在多种日志，这个关键字用来声明所有日志都需要执行 postrotate 脚本 postrotate # 轮转工作完成后需要执行脚本 [ -e /usr/local/openresty/nginx/logs/nginx.pid ] \u0026amp;\u0026amp; kill -USR1 `cat /usr/local/openresty/nginx/logs/nginx.pid` endscript } logrotate 的精细度最快只能每天执行一次，如果我们需要频繁地切割日志，可以将 crontab 和 logrotate -v /etc/logrotate.d/nginx 配合使用。\npostrotate 的关键点在于 kill -USR1 nginx.pid 这个命令，它可以使 Nginx 重新打开日志文件。\n代理 WebSocket WebSocket 是基于 HTTP 协议的实时双向通讯协议，它属于应用层的协议。\n在浏览器中， WebSocket 会以 ws:// 的形式出现，基于 ssl 加密则会以 wss:// 的形式出现，在 Nginx 中，所有的 WebSocket 请求只是带有特定请求头的普通 HTTP 请求。\nWebSocket 的握手请求带有重要的两个请求头 Upgrade: websocket 和 Connection: Upgrade ，握手成功将会返回 101 Switching Protocols ，后续双方就可以使用 WebSocket 的方法互相通信。\n通常 WebSocket 代理是这样做的：\nhttp { map $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } upstream webscoket { server 127.0.0.1:9000; } server { location /webscoket { proxy_pass http://webscoket; proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } } 支持跨域访问 CORS 跨域访问是浏览器对 Web 资源访问的安全限制，通过特定的请求头响应来确认是否允许访问非同源的资源。\n# 简单的跨域请求 GET /resource HTTP/1.1 # 请求资源路径： /resource Origin: http://web.domain.com # 所在的请求页面： web.domain.com Host: api.domain.com # 完整请求： api.domain.com/resource 所有的跨域请求都需要带上 Origin 作为来源标识，此外可能还额外带有一些额外的请求头，通常会以 OPTIONS 方法进行预检测服务端是否响应跨域请求。\n# Preflight Request from MDN Docs # Request OPTIONS /doc HTTP/1.1 Host: bar.other User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-us,en;q=0.5 Accept-Encoding: gzip,deflate Connection: keep-alive Origin: https://foo.example Access-Control-Request-Method: POST Access-Control-Request-Headers: X-PINGOTHER, Content-Type # Response HTTP/1.1 204 No Content Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2 Access-Control-Allow-Origin: https://foo.example Access-Control-Allow-Methods: POST, GET, OPTIONS …","date":1657983600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"59eb4243ad95565c2c95de6db0d8c655","permalink":"https://yuweizzz.github.io/post/practical_tips_in_nginx/","publishdate":"2022-07-16T15:00:00Z","relpermalink":"/post/practical_tips_in_nginx/","section":"post","summary":"这篇笔记用来 Nginx 在实际应用场景的一些使用技巧。\n","tags":["Nginx","OCSP","HSTS","CORS","acme"],"title":"Nginx 实用技巧","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Containerd 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 containerd 是由 docker 公司贡献，符合 OCI 标准的容器进行时。\ncontainerd 和 docker 的关系 首先我们需要了解 OCI 是什么，它的全称为 Open Container Initiative ，也就是开放容器提案，目前主要有 Runtime Spec 和 Image Spec ，Distribution Spec 几个定制标准，标准化使得各种第三方组件不再限定于某种容器运行时，而是可以自由运行在任何符合 OCI 标准的容器进行时，可以说 OCI 促进了容器技术的生态发展。\n通常来说，容器进行时指的是管理容器生命周期的守护进程，比如 docker 就是最经典的容器进行时，我们只需要调用 docker 命令就可以启动或者停止容器。但实际上 docker 要做的工作不止这些，它还兼顾了镜像管理，镜像构建，容器网络和存储管理等工作，属于高层级的容器进行时。\ncontainerd 也属于高层级的容器进行时，但相比于 docker 它更加精简，最关键地保留了管控容器生命周期的核心功能，在较新版本的 docker 中，管控容器生命周期的功能已经完全由 containerd 承担， docker 守护进程和 containerd 守护进程之间使用 grpc 进行通信。\ncontainerd 守护进程管理着运行中的容器，与容器具体的交互功能则是由 runc 负责的， runc 是低底层的容器运行时接口，只要是符合 OCI 标准的容器镜像，就可以直接调用 runc 运行这个镜像。\n但是 containerd 并不是直接调用 runc ，而是采用了垫片的方式，由 containerd-shim 去调用 runc ，而 containerd 只需要和 shim 交互，无需直接和更底层的 runc 交互，垫片机制在容器技术中很常见， Kubernetes 使用 dokcer 作为容器运行时也是通过 shim 来实现的。\n# containerd 和 docker 的关系 docker daemon | grpc v containerd daemon | shim v runc | v container 直接通过 containerd cli 运行容器 由于 containerd 具备完善的容器生命周期能力，我们可以不再使用 docker cli ，直接使用 containerd 的 cli 工具 ctr 直接运行容器。\n# 安装 containerd $ yum install containerd $ systemctl enable containerd $ systemctl start containerd # 拉取镜像 $ ctr i pull docker.io/library/busybox:latest $ ctr i pull docker.io/library/nginx:alpine # 生成容器 $ ctr c create docker.io/library/busybox:latest mybusybox $ ctr c ls CONTAINER IMAGE RUNTIME mybusybox docker.io/library/busybox:latest io.containerd.runtime.v1.linux # 启动容器 $ ctr t start -d mybusybox $ ctr t ls TASK PID STATUS mybusybox 2973 RUNNING 根据 OCI Runtime Spec ，正常的容器会经过以下几种运行状态：\n# OCI Runtime Spec init -\u0026gt; creating | ^ v delete | created | start v stopped \u0026lt;- running \u0026lt;-\u0026gt; paused kill pause/resume 在 containerd 中，除了 container ，还引入了 task 的概念， start ， kill 和 delete 等动作是使用在 task 上的，对应 OCI Runtime Spec 定义的容器状态。而 container 只有 create 和 delete 两个动作，从表象上来看 task 在 containerd 中是实际运行的容器进程，而 container 是容器信息的声明。\n# task 支持多种动作 # pause 和 resume 就是冻结和恢复 $ ctr t ls TASK PID STATUS mybusybox 2973 RUNNING $ ctr t pause mybusybox $ ctr t ls TASK PID STATUS mybusybox 2973 PAUSED $ ctr t resume mybusybox $ ctr t ls TASK PID STATUS mybusybox 2973 RUNNING # kill 就是发送进程 signal ，一般用来发送终结信号 $ ctr t kill -s 9 mybusybox # 2: SIGINT # 9: SIGKILL # 15: SIGTERM $ ctr t ls TASK PID STATUS mybusybox 2973 STOPPED # 只有处于 STOPPED 状态才能被删除 $ ctr t delete mybusybox $ ctr t ls TASK PID STATUS # exec 则是在容器中执行命令 $ ctr t exec -t --exec-id busybox-sh mybusybox sh # -t 选项要求命令执行时分配终端 # --exec-id 选项用于命名这个 exec 进程 # 执行上述命令会生成基于 mybusybox task 的 sh 进程 增强 containerd 运行的容器 声明持久化挂载点 虽然 containerd 不具备 docker volumes 的功能，但是可以基于 mount 命名空间将宿主机上的某些文件目录映射到容器中。\n# 需要在 container creater 阶段声明 mount 信息 $ ctr c create docker.io/library/busybox:latest mybusybox --mount type=bind,src=/home/busybox,dst=/mnt,options=rbind:ro $ ls /home/busybox/ fileA fileB fileC $ ctr start -d mybusybox # 进入容器并查询挂载目录 $ ctr t exec -t --exec-id busybox-sh mybusybox sh / # ls -l /mnt/ total 0 -rw-r--r-- 1 root root 0 Jun 6 13:23 fileA -rw-r--r-- 1 root root 0 Jun 6 13:23 fileB -rw-r--r-- 1 root root 0 Jun 6 13:23 fileC 使用 cni 扩展网络能力 containerd 可以基于 net 命名空间，配合 cni 工具构建网络模型。\ncni 工具由 cnitool 和各个网络插件组成，一般使用 cnitool 读取定义了各种参数和插件的配置文件来生成网络接口，再将它们附加到 net 命名空间中。\n# 直接与宿主机共享网络栈 $ ctr c create --net-host docker.io/library/busybox:latest mybusybox # 这样设置使得容器完全使用宿主机的网络，包括本地回环等所有网络接口 # --net-host 用于声明容器共享主机网络 # 使用 cni 扩展工具构建网络模型 # 使用 yum 安装 cni $ yum install containernetworking-plugins # cni 严重依赖环境变量提供运行时信息 # 配置文件路径声明，这一步通常可以省去，因为默认值就是 /etc/cni/net.d $ export NETCONFPATH=/etc/cni/net.d # 插件所在路径声明，如果 cnitool 执行失败，通常是没有指定这个路径导致的 $ ls /usr/libexec/cni/ bandwidth firewall host-local macvlan sample tuning bridge flannel ipvlan portmap sbr vlan dhcp host-device loopback ptp static $ export CNI_PATH=/usr/libexec/cni/ # 使用 iproute2 创建 net 命名空间 $ ip netns add container-net # 通过 conf 文件创建网络接口 $ cat /etc/cni/net.d/cni.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cni\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;isDefaultGateway\u0026#34;: true, \u0026#34;forceAddress\u0026#34;: false, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.88.0.0/16\u0026#34; } } # 创建这个网络接口并附加到命名空间中 $ /usr/libexec/cnitool add cni /var/run/netns/container-net # 查看已经生成的 cni0 网桥 $ ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever …","date":1654631085,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"0f21b7eae912a4a9416de5fbcce7d259","permalink":"https://yuweizzz.github.io/post/knowledge_about_containerd/","publishdate":"2022-06-07T19:44:45Z","relpermalink":"/post/knowledge_about_containerd/","section":"post","summary":"这篇笔记用来记录 Containerd 的相关知识。\n","tags":["Container","Containerd"],"title":"符合容器化标准的容器运行时：Containerd","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Namespace 与 Cgroup 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 容器的实现依赖于 Namespace 与 Cgroup ，它们是内核直接提供的功能，如果想要深入理解容器的运行机制，这是必须了解的知识点。\nNamespace 命名空间 Namespace 是内核用来隔离资源的一项技术，不同的进程所拥有的资源可以互相隔离，互不干扰。\n容器本质上也是系统的进程，但是它拥有独立的命名空间，可以和系统原有的进程隔离。\n在 Linux 中已经定义的命名空间有 8 种：\nUser ： 用户和用户组 PID ： 进程 ID Mount ： 挂载点 Network ： 网络设备和网络协议栈 UTS ： 主机名和域名 IPC ： 进程间通信 Time ： 系统时钟 Cgroup ： cgroup 控制群组 在这些不同的命名空间种类中， Time 和 Cgroup 在高版本的内核中才得到实现，现有生产环境的流行内核版本大部分只实现了前六种类型。\n# CentOS 7.9 # Kernel Version 3.10.0 # 以 root 用户运行 $ whoami root $ lsns NS TYPE NPROCS PID USER COMMAND 4026531836 pid 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 111 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 18 root kdevtmpfs 4026531956 net 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532430 mnt 2 1010 root /usr/sbin/NetworkManager --no-daemon 4026532439 mnt 1 1027 chrony /usr/sbin/chronyd # 以低权级用户查看命名空间 $ whoami mine $ lsns NS TYPE NPROCS PID USER COMMAND 4026531836 pid 2 8945 mine bash 4026531837 user 2 8945 mine bash 4026531838 uts 2 8945 mine bash 4026531839 ipc 2 8945 mine bash 4026531840 mnt 2 8945 mine bash 4026531956 net 2 8945 mine bash # 仔细对比使用 root 和使用低权级用户的命名空间，可以发现它们仍然在同一 namespace 中，但是低权级用户仅能查询到有限的信息 系统启动后默认会生成 init namespace ，系统进程基本都运行在 init namespace 中，后续的命名空间都和它是有关联的，大部分情况下是继承关系，某些特殊进程可能使用了一到两种额外的命名空间类型。\nLinux 提供了用户层命令 unshare 和 nsenter 来操作进程的命名空间归属。\n# CentOS 7.9 # Kernel Version 3.10.0 # 当前系统可能对 user 命名空间有限制，一般有两种情况： # 如果内核禁用了 user 命名空间，可能需要重新安装系统或者编译内核 # 如果编译选项正常，则可能是内核对命名空间数量做了限制 $ whoami root $ cat /boot/config-3.10.0-1160.el7.x86_64 | grep -i user_ns CONFIG_USER_NS=y $ cat /proc/sys/user/max_user_namespaces 0 # 开放内核对命名空间的限制， unshare 才能正常创建 user 命名空间 $ echo 10 \u0026gt; /proc/sys/user/max_user_namespaces # 模拟容器，创建一个隔离的进程 [root@localhost ~]# unshare --pid --user -r --uts --net --mount --ipc --fork --mount-proc --propagation private /bin/sh $ unshare \\ \u0026gt; --mount --propagation private \\ \u0026gt; --uts \\ \u0026gt; --ipc \\ \u0026gt; --net \\ \u0026gt; --pid --fork --mount-proc \\ \u0026gt; --user --map-root-user \\ uts 和 ipc 基本上都是直接从原命名空间克隆一些必要的数据，后续和父空间互不干扰，比如 uts namespace 会克隆父空间的 /proc/sys/kernel/hostname 作为自己的 hostname 。\nnet 除了和父空间隔离外，会生成新的网络栈并默认生成独立的 lo 网络设备。\n对于 user ， unshare 提供了 -r/--map-root-user 来实现 root 用户映射，低权级用户可以通过使用这个选项成为新命名空间中的 root 用户，获取最高权限。\n对于 pid ，unshare 提供了 --fork 和 --mount-proc 扩展选项，值得注意的是这里无论带什么选项， unshare 这个进程不会加入新的 pid 命名空间：\n--mount-proc 可以使得隔离进程和父空间的所有进程信息隔离开来，子空间的 pid 将重新从 1 开始衍生，但父空间仍然可以监控到这部分信息。如果不带 --mount-proc 选项，则父空间的 pid 信息会被子空间继承。 --fork 则可以生成子进程来运行指定程序，如果使用上面的例子，你将会得到 unshare 进程和作为子进程的 /bin/sh ，其中 unshare 进程属于父 pid 命名空间，子进程 /bin/sh 新生成的子 pid 命名空间。如果不带 --fork 将无法生成新的 pid 命名空间，生成的 /bin/sh 仍然在原有的命名空间，并且它的生命周期极短，命令运行完成后进程就不再分配内存了。 mount 命名空间是最复杂的一种，它使用 shared subtrees 运行机制，允许在不同 mount 命名空间之间自动，受控地传播 mount 事件和 unmount 事件。简单来说，我们通过设置某个命名空间的 propagation 属性，决定这个命名空间中挂载动作和卸载动作是如何传播到其他命名空间。\npropagation 有 private|shared|slave|unchanged 这几种，最常用的是 private ，它默认从父空间继承挂载节点，后续不同空间的挂载动作和卸载动作互不影响，在 docker 的官方文档中，有这么一段话： Volumes use rprivate bind propagation, and bind propagation is not configurable for volumes. ，说明了卷是基于 mount 命名空间来实现挂载特性的。\n# 在运行容器的情况下查看现有的命名空间 # 使用 containerd 作为容器运行时 $ ctr i pull docker.io/library/busybox:latest $ ctr c create docker.io/library/busybox:latest busybox $ ctr t start -d busybox $ ctr t ls TASK PID STATUS busybox 61830 RUNNING $ lsns NS TYPE NPROCS PID USER COMMAND 4026531836 pid 114 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 115 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 114 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 114 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 112 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 18 root kdevtmpfs 4026531956 net 114 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532430 mnt 1 972 chrony /usr/sbin/chronyd 4026532446 mnt 1 61830 root sh 4026532447 uts 1 61830 root sh 4026532448 ipc 1 61830 root sh 4026532449 pid 1 61830 root …","date":1652474685,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"914b26e45c9bf9dcb785a0222655eda6","permalink":"https://yuweizzz.github.io/post/knowledge_about_namespace_and_cgroup/","publishdate":"2022-05-13T20:44:45Z","relpermalink":"/post/knowledge_about_namespace_and_cgroup/","section":"post","summary":"这篇笔记用来记录 Namespace 与 Cgroup 的相关知识。\n","tags":["Container","Namespace","Cgroup"],"title":"实现容器技术的基础：Namespace 与 Cgroup","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Golang 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 创建 Go 项目 Go 的运行需要依赖于环境变量，以下是比较重要的环境变量：\nGOPATH ： Go 项目的存放路径，也就是整体的工作空间。 GO111MODULE ： Go 的依赖管理系统开关。 GOROOT ： Go 的安装路径，包括了命令行工具，标准库和文档等。 GOPROXY ： Go 依赖包下载的代理地址，在国内的网络环境中非常重要，使用 go env -w GOPROXY=https://goproxy.cn,direct 直接更换即可。 在我开始使用 Go 语言的时候，主线版本是 1.16 ，所以此时 Go Module 已经非常成熟了，但网上仍存在大量关于 GOPATH 的相关资料，它同样和依赖管理密切相关，在这里首先需要明确的是 Go Module 是用来替代 GOPATH 的依赖管理模式。\n使用 GOPATH 模式一般会有三个目录： bin ， pkg ， src ，其中所有项目的源代码都会在 src 目录中，这个目录下会以多级目录的形式维护各级依赖，而 pkg 用来存放编译过程的中间文件， bin 用来存放最终生成的可执行文件。\n一般来说，我们创建新项目是最终目标是创建 main 包，并且经常需要在 main 包中导入依赖，但是这些依赖在 GOPATH 模式下，存放位置就只有 $GOPATH/src 和 GOROOT 中的标准库这两个位置，新项目仍需要处在 GOPATH 中，否则会找不到项目的依赖模块，就算可以找到对应的模块，也有着版本冲突的危险。\n在使用 Go Module 的情况下，我们可以使用 go mod 来创建新项目和声明依赖，当前新项目的子目录就可以视为自身的子模块，在使用时就可以直接导入而不会在 GOPATH 中搜索并报错，这样就脱离了 GOPATH 的限制。如果是来自外部的模块，则需要使用 go get package 来导入，并且这些外部依赖的源代码会下载到 $GOPATH/pkg/mod 中，并在当前模块中执行严格的版本控制，这也是为什么使用 Go Module 进行依赖管理而 GOPATH 变量依旧重要的原因。\n# go mod 常用命令 # 创建项目 $ go mod init package # 获取外部依赖 $ go get github.com/BurntSushi/toml # 去除无用依赖 $ go mod tidy # 修改依赖信息，例如修改依赖版本或者依赖包重命名 $ go mod edit -replace github.com/BurntSushi/toml=github.com/BurntSushi/toml@v1.1.0 # 等号前是修改前的信息，等号后是目标信息 使用 Go Module 的 Go 项目可以使用这两种结构：\n在整个项目的根预留 main.go 作为总入口，使用子目录来区分各个功能模块。 # 整体的项目结构如下 $ tree . ├── submoduleA │ └── a.go ├── submoduleB │ └── b.go ├── go.mod └── main.go # main.go 会是这样的： ... package main import module/submoduleA import module/submoduleB func main(){...} ... # submodule 的编写会是这样： ... package submoduleA func A(){...} ... 将整个项目视为模块，额外创建目录用于制作 main.go 总入口。 # 整体的项目结构如下 $ tree . ├── submoduleA │ └── a.go ├── submoduleB │ └── b.go ├── c.go ├── go.mod └── cmd └── main.go # main.go 和 submodule 的编写和前面基本一致 # 但 c.go 则直接属于 module ，它的编写会是这样： ... package module func C(){...} ... 某个目录存在 package main 的文件意味着当前目录下的 go 文件都是运行程序，这个目录允许存在多个 package main 文件，但不能存在其他 package 的 go 文件，并且这样的目录在任意模块中也不会被作为子模块导入。\nGolang 的数据类型 Golang 的数据类型和大多数编程语言相似，但有一些细微的区别。\n这里先明确值类型和引用类型的区别：\n值类型：变量直接存储数据。 引用类型：变量直接存储指针，再由指针指向实际存储的数据。 值类型数据 布尔类型 bool 是比较简单的数据类型，在 Golang 中以 true 和 false 作为直接关键字，但不能像 Python 直接将空值或者零值做为布尔值来推断，因为 Go 不支持隐式转换类型。\n整型 int 细分为有符号类型 int8 ， int16 ， int32 ， int64 和各自对应的无符号类型 uint8 ， uint16 ， uint32 ， uint64 一共 8 种类型。这里对整形数据的定义和 C 语言是基本一致的，实际代码编写过程中通常直接使用 int 类型和 uint 类型，而 int 的具体位数一般在编译时确定，在现代操作系统中通常会是 32bits 或者 64bits ，但即使是在 64bits 操作系统中 int 的编译结果通常会是 32bits 。在需要强关联某个具体 bits 类型时可以通过显式声明来实现，这种情况常见于内核开发，普通应用比较少。\n浮点型 float 则有 float32 和 float64 两种，具体用法类似于 int 。\n还有两种特殊类型 byte 和 rune ，其中 byte 是 int8 的别名，rune 是 int32 的别名，这两种类型用来表示数据不是数值类型，应该直接以二进制格式对待。\n字符类型 string 是更为特殊的数据类型，它的本质是只读的 byte slice ，所以 string 是可以遍历的，但只能得到 byte 类型的数据，由于字符编码格式的不同，直接使用 len 测试 string 长度会出问题，因为 UTF-8 编码的中文字符占用三个 byte 的存储空间，而 UTF-8 编码和 ascii 编码的英文字符都只需要占用一个 byte 的存储空间，两个中文字符的计算结果会是 6 而不是 2 。所以字符类型应该使用对应的库去处理，减少类型转换，如果确实需要使用类型转换，纯中文字符，纯英文字符和混合中英字符可以使用 rune 类型，纯英文字符还可以额外使用 byte 类型。\narray 在 Go 中是值类型，声明时以 [n]T 的形式出现，它必须在声明时指定长度，可以通过类似 Array := [...]int{12, 78, 50} 的形式自动推断长度，并且后续这个长度是不可变的。\nstruct 也是值类型，是由其他类型组合形成的复合数据类型。\n引用类型数据 slice ， map 和 channel 是引用类型数据。\nslice 称为切片，和 array 类似，一般从 array 截取得到 slice 。但和 array 不同， slice 是长度可变的，声明时以 make([]T, n) 的形式出现，声明后仍然可以申请内存空间。\nmap 则是映射，类似于字典，声明时以 make(map[T]T) 的形式出现。\n// array string_array := [3]string{\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;} // slice var string_slice = []string{\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;} string_slice := []string{\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;} string_slice := make([]string, 10) // map map_expamle := map[string]int{\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 4} 函数传参 在 Golang 中，所有函数的传参都是传值，在函数中的形参是实参的副本。\n如果是值类型作为参数，这种结论非常容易理解，那就是形参在函数中的任何修改并不会影响实参，因为它们在内存中的实际位置并不相同。\n如果是引用类型作为参数，通常会造成比较大的迷惑，我们可以逐一分析：\npackage main import \u0026#34;fmt\u0026#34; func changeArrayByPointer(a *[3]int) { a[0] = 10 } func changeArray(a [3]int) { a[0] = 10 fmt.Println(\u0026#34;in func changeArray():\u0026#34;, a) } func changeSlice(s []int) { s[0] = 10 } func changeMap(m map[string]int) { m[\u0026#34;a\u0026#34;] = 10 } func main() { a := [3]int{1, 2, 3} b := [3]int{1, 2, 3} s := []int{1, 2, 3} m := map[string]int{\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2} changeArrayByPointer(\u0026amp;a) fmt.Println(\u0026#34;a:\u0026#34;, a) changeArray(b) fmt.Println(\u0026#34;b:\u0026#34;, b) changeSlice(s) fmt.Println(\u0026#34;s:\u0026#34;, s) changeMap(m) fmt.Println(\u0026#34;m:\u0026#34;, m) } // output: // a: [10 2 3] // in func changeArray(): [10 2 3] // b: [1 2 3] // s: [10 2 3] // m: map[a:10 b:2] 这里是一个非常典型的例子，首先我们可以看到，对于值类型 array ，当它们作为函数参数时，并无法改变实参，但是通过使用指针变量作为函数参数时，确实成功改变了实参，而对于引用类型 map 和 slice ，由于它们隐式使用了指针变量作为函数参数，所以最终实参也被成功改变。\n这里比较颠覆传统的思路是 struct 类型，要注意它实际是值类型，如果想要通过函数改变结构体的内容，直接将 struct 作为参数是不行的，应该这样做：\npackage main import \u0026#34;fmt\u0026#34; type A struct { a string b string } func …","date":1652125485,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"286d38ff8675e8ef1da8fc935fa57028","permalink":"https://yuweizzz.github.io/post/study_notes_about_go/","publishdate":"2022-05-09T19:44:45Z","relpermalink":"/post/study_notes_about_go/","section":"post","summary":"这篇笔记用来记录一些 Golang 的相关知识。\n","tags":["Go"],"title":"Golang 学习笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 TLS 协议的工作原理及其相关计算机密码学的知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 计算机密码学算法 TLS 协议的是结合了多种计算机密码学算法实现的，它用来实现传输过程中的数据安全。\n在计算机密码学中，经常使用的有对称加密算法，非对称加密算法和散列算法。\n对称加密算法用来加密数据，它的特点是数据加密和解密需要使用相同的密钥，所以密钥泄露时，加密数据就会被轻松破解，实际应用的对称加密算法有 DES 和 AES 。\n非对称加密算法则更特殊，使用时需要预先生成密钥对，在通信时保留私钥，把公钥分发给对方，将信息使用私钥进行加密，这样对方就可以使用公钥对加密信息进行解密。这种加密方式的特点在于使用公钥加密后，只能通过对应的私钥进行解密，保证了只有通信双方能获取到加密信息。实际应用的非对称加密算法有 RSA 和 DSA 。\n散列算法也就是哈希 hash 算法，主要用来验证数据的完整性。它的特点是对不定长的数据总是可以计算出定长的散列值，并且这个过程是单向不可逆的。即使是很小的数据改动，散列值总是不相同的。我们利用这部分散列信息来验证的数据的完整性，实际应用的散列算法有 MD5 和 SHA 。\n如何实现数据安全 在前面密码学的理论支持下，我们需要做到以下三点来确保数据安全：\n数据不被泄露。 数据是真实的，未被篡改破坏的。 数据是来源可靠的。 实现第一点需要对数据进行加密，基于前面的理论，我们应该优先使用的是非对称加密算法，但实际中是结合了两种加密算法来实现。\n实现第二点需要对数据进行校验，在实际应用中，我们通过散列算法计算出散列值，这个值通常称为信息摘要，再通过非对称加密算法对信息摘要进行加密，得到的加密信息称为数字签名。在验证数据真实完整性时更多是使用加密后的数字签名，而不是单纯的信息摘要。\n由于我们并无法直接确认通信目标就是可信的，为了实现第三点，在这里引出了证书的概念。\n虽然我们无法保证通信方是百分百可信的，但是我们可以选择性地信任某些权威机构，并委托这些机构去认证通信来源，如果这些机构认可通信来源，就会给它颁发证书。当我们与对方通信时，如果它出示的证书来自我们信任的权威机构，那么我们就可以信任对方并与之通信。\n权威机构 Certificate Authority 自身也需要证书，并且我们需要主动信任 CA 证书，由这个 CA 签发的系列证书才能被正常信任，各大认证机构的 CA 证书通常已经内置于操作系统，随意信任来历不明的 CA 证书是非常危险的。\n在三点要求满足的情况下，我们可以认为已经实现了数据安全。\n生成 CA 证书和 TLS 证书 前面我们已经描述了证书在通信过程中的重要地位，其中 CA 机构自身使用的证书和签发出去的 TLS 证书是略有区别的，但它们都遵循 X.509 标准。\nCA 证书包括了该机构的组织信息，持有的公钥和信息的数字签名等，我们需要 CA 证书的重要原因是验证时需要使用 CA 证书中的公钥去验证 TLS 证书的数字签名。\nTLS 证书包括了该站点的组织信息，域名信息，站点所使用的公钥和信息的数字签名等， TLS 证书的公钥会用于和使用该证书的站点通信。\n签发证书的通常过程是这样的：\n域名拥有者将域名信息，申请者所属组织等必要信息生成签发证书的请求，给到具体的 CA 机构。 CA 对签发请求的信息生成信息摘要，并使用 CA 的私钥生成数字签名，再附上 CA 自身的信息，生成证书文件。 在实际应用中，我们可以直接自行签发 TLS 证书，也可以通过自建 CA 来签发需要的 TLS 证书。\n# 自签名 TLS 证书 # 生成私钥 $ openssl genrsa -out key.pri 2048 # 以 www.ibm.com 为例，生成证书签发请求 $ openssl req -new -sha256 -key key.pri \\ -subj \u0026#34;/C=US/ST=IL/L=Chicago/O=IBM Corporation/OU=IBM Software Group/CN=www.ibm.com\u0026#34; -out request.csr # 生成自签证书，由于它不会被自动信任， signkey 可以重新生成或者直接沿用自身的私钥 $ openssl x509 -req -sha256 -days 365 -in request.csr -signkey key.pri -out ibm.crt # 上述例子生成的证书就是 RSA 证书，下面的例子是生成 ECC 证书 # 查看当前 openssl 版本支持的椭圆曲线 $ openssl ecparam -list_curves # 一般使用 prime256v1 来生成私钥 $ openssl ecparam -genkey -name prime256v1 -out key.pri $ openssl req -new -sha256 -key key.pri \\ -subj \u0026#34;/C=US/ST=IL/L=Chicago/O=IBM Corporation/OU=IBM Software Group/CN=www.ibm.com\u0026#34; -out request.csr $ openssl x509 -req -sha256 -days 365 -in request.csr -signkey key.pri -out ibm.crt 自签名意味着自身就是签发的 CA 机构，在访问使用自签名 TLS 证书的站点时，通常会显示证书不受信任的提示，手动把这个自签名证书加入到操作系统的信任链中后这个提示就不会再出现了。但是将单一站点的 TLS 证书加入到信任链中是非常少见的操作，一般只会直接信任 CA 证书。\n所以如果需要多个自签 TLS 证书，则自建 CA 会更方便，它与自签名 TLS 的区别在于不随意使用私钥去签名，而是固定一对密钥，把它持续用于后续 TLS 证书的签发，并且将固定公钥生成 CA 证书，那么只需要信任 CA 证书就可以自动信任它所签发的 TLS 证书。\n# 自建 CA # 生成私钥 $ openssl genrsa -out key.pri 2048 # 生成证书签发请求 $ openssl req -new -sha256 -key key.pri \\ -subj \u0026#34;/C=US/ST=IL/L=Chicago/O=IBM Corporation/OU=IBM Software Group/CN=IBM Root Certificate\u0026#34; -out request.csr # 生成 CA 证书 $ openssl x509 -req -sha256 -days 365 -in request.csr -signkey key.pri -out CA.crt # 使用自建 CA 签发 TLS 证书 # 站点生成证书签发请求 $ openssl genrsa -out server_key.pri 2048 # 这里可以通过配置多个 CN ，也可以将单个 CN 指定类似 *.software.ibm.com 的泛解析来满足多域名的情况 $ openssl req -new -sha256 -key server_key.pri \\ -subj \u0026#34;/C=US/ST=IL/L=Chicago/O=IBM Corporation/OU=IBM Software Group/CN=software.ibm.com\u0026#34; -out request.csr # CA 处理签发请求，生成证书 $ openssl x509 -req -sha256 -days 365 -in request.csr -CA CA.crt -CAkey key.pri -CAcreateserial -out software.crt 在签发 CA 证书时最好通过 -extfile \u0026lt;(printf \u0026#34;basicConstraints=CA:TRUE\u0026#34;) 表明自身是用作 CA 证书用途。\n在 golang 1.15 之后的版本，进行 tls 握手时发生报错 \u0026#34;x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\u0026#34; 是因为当前使用的证书依赖于 CN 作为域名绑定，需要使用 x509 拓展字段 Subject Alternative Name 才能进行正常验证，也就是报错信息所述的 SAN 。\n# 生成 SAN 证书 # 前序步骤和使用 CN 的证书相似 # 生成私钥 $ openssl genrsa -out server_key.pri 2048 # 生成证书签发请求 $ openssl req -new -sha256 -key server_key.pri \\ -subj \u0026#34;/C=US/ST=IL/L=Chicago/O=IBM Corporation/OU=IBM Software Group/CN=software.ibm.com\u0026#34; -out request.csr # 使用 CA 处理签发请求，生成 SAN 证书 $ openssl x509 -req -sha256 -days 365 -in request.csr -CA CA.crt -CAkey key.pri -CAcreateserial \\ -extfile \u0026lt;(printf \u0026#34;subjectAltName=DNS:www.software.ibm.com,DNS:software.ibm.com,IP:1.1.1.1\u0026#34;) -out software.crt # 如果是高版本的 openssl 可以尝试这样做 $ openssl x509 -req -sha256 -days 365 -in request.csr -CA CA.crt -CAkey key.pri -CAcreateserial \\ -addext \u0026#34;subjectAltName=subjectAltName=DNS:www.software.ibm.com,DNS:software.ibm.com,IP:1.1.1.1\u0026#34; -out software.crt # 可以看到实际的签发方式和使用 CN 的证书类似，但是使用了 x509 ext 字段，这样可以扩展证书的泛用性，匹配多个域名和 IP 地址 # 签发之后就可以在证书的 X509v3 extensions 看到这部分信息 通过信任上述例子中的 CA.crt ，后续 …","date":1648300965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5df9cea489f6de6d1871f466fe90c1e4","permalink":"https://yuweizzz.github.io/post/knowledge_about_tls/","publishdate":"2022-03-26T13:22:45Z","relpermalink":"/post/knowledge_about_tls/","section":"post","summary":"这篇笔记用来记录 TLS 协议的工作原理及其相关计算机密码学的知识。\n","tags":["OpenSSL","TLS","OCSP"],"title":"TLS 协议工作原理","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 PowerDNS 的性能优化。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ PowerDNS 的特点 PowerDNS 和 BIND 一样，都是 DNS 服务软件。\nPowerDNS 比较大的特色在于它把权威解析和递归解析的能力拆解开来分为两个服务，负责权威解析的是 pdns authoritative server ，负责递归解析的是 pdns recursor server 。我认为这种拆解是有利的，它将不同的解析流量区分，排查问题时会更方便，对后续的性能瓶颈分析也是有利的。\nPowerDNS 支持各种各样的后端，可以是文件系统，也可以是通用关系型数据库，虽然 BIND 可以通过 DLZ 来实现关系型数据库作为记录的存储后端，但从支持的后端多样性来看，明显是 PowerDNS 更丰富。\nPowerDNS 内置 Web Server 实现了 API 支持和监控系统，可以进行实时数据监控和动态更新记录，而 BIND 虽然也有相关的统计信息输出，但是这部分功能明显逊色于 PowerDNS 。\nPowerDNS 的搭建和配置都相对简单，这里只记录一些调优过程的思考。\nAuthoritative Server 的性能调优 我们需要先了解 Authoritative Server 中重要的缓存种类：\nPacket Cache ：数据包缓存，可以无需做任何额外处理，直接响应查询请求的数据缓存。 Query Cache ：执行后端查询后，后端查询到的数据库记录缓存。 Negative Cache ：在 Query Cache 中，请求信息无法在后端查询到记录的数据缓存。 其实可以直接地认为是两种缓存， Packet Cache 是对请求回答数据的缓存， Query Cache 和 Negative Cache 都是对数据库记录的缓存，通常我们希望直接返回 Packet Cache ，这会是最快最节省资源的响应办法。如果确实无法直接命中，则应该优先在 Query Cache 部分寻找命中，可以节省对数据库的查询行为。在这一部分， Negative Cache 和常规 Query Cache 的命中都是同样的，只是这个请求是否能得到回答数据的区别而已。\n关于缓存部分，性能优化的调节点是在于缓存时间和缓存条数，我们可以把默认的 Packet Cache 的缓存时间略微提高一些，它在配置文件中以 cache-ttl 出现，默认时间为 20s ，这个值可以调高至 60s 。\nQuery Cache 的缓存时间在配置文件中以 query-cache-ttl 出现，默认时间为 20s ，这个值也可以和 cache-ttl 一样调高到 60s 。\nNegative Cache 的缓存时间在配置文件中以 negquery-cache-ttl 出现，默认时间为 60s ，由于它和 Query Cache 类似，可以保持和 query-cache-ttl 一致的 60s 。\n要注意调节缓存时间能起到性能优化的前提是使用关系型数据库作为后端，如果是文件系统或者是基于内存的后端存储，这些缓存时间需要额外考量，甚至可以直接禁用缓存，因为它们的响应速度足够快，缓存反而会成为性能的拖累。\n另一个性能优化的调节点在于对工作线程的调节，在 Authoritative Server 中有 receiver thread 和 distributor thread 的概念。\nreceiver thread 是用于接收请求的线程，它的线程数可以自由调节，但要达到优化性能，这个数量应该适中，最好是和 CPU 数量成倍数关系。它在配置中以 receiver-threads 出现，默认值是 1 。\ndistributor thread 是 receiver thread 接收请求后，用于处理这些请求的线程，主要担任查询工作。它在配置中以 distributor-threads 出现，默认值是 3 。\n需要注意的是这里 distributor-threads 是每个 receiver thread 所关联的线程数量，也就是说一个 receiver thread 可以对应一个或多个 distributor thread ，这个值应该由使用的后端类型决定，如果只有单个关系型数据库作为后端，那么 distributor-threads 为 1 应该是最优的做法，但如果使用了多个后端数据库，设置较大的 distributor-threads 可以得到更好的性能。\n然后是配置中的 reuseport 这个特性开关，它是通过启用内核的 SO_REUSEPORT 选项来使得多个套接字可以在同个端口监听，如果内核版本过低不支持 SO_REUSEPORT ，那么不管这个选项如何设置，它都是默认关闭的。\n设置多个 receiver thread 和开启 reuseport 的两者组合应该是最佳性能的工作方式，这样内核会将请求均衡到各个 receiver thread 中，获取比较好的性能表现。\n最终的优化配置大概如下：\n# 假设是 4 核机器，单个 MySQL 作为 backend $ cat pdns.conf cache-ttl=60 query-cache-ttl=60 negquery-cache-ttl=60 distributor-threads=1 receiver-threads=4 reuseport=yes Recursor Server 的性能调优 Recursor Server 同样有着多个缓存种类：\nNameserver Speeds Cache ：对所有远端权威服务器的平均延迟时间的缓存。 Negative Cache ：对无响应数据请求的缓存。 Recursor Cache ：对递归过程一些公共记录信息的缓存。 Packet Cache ：数据包缓存，可以无需做任何额外处理，直接响应查询请求的数据缓存。 在递归服务器中，各类缓存的 ttl 已经被默认设置为较高值，所以这部分并没有对它们做额外调节，更多的优化细节在于工作线程这一方面。\nRecursor Server 的 threads 和 Authoritative Server 的 receiver threads 类似，是处理具体请求的线程，但它不负责具体的后端查询工作。\n但在 Recursor Server 中还是有 distributor thread 的概念，它负责将请求分发到 thread 中，按照官方的说法，使用 distributor thread 可以提高缓存的命中率。但以实际测试情况来看，在原有 Packet Cache 命中率就很高的情况下，开启 distributor thread 会导致实际工作的 thread 负载不均衡，而缓存命中率只是略有提高。\n所以实际使用中，较好的做法是禁用 distributor thread 和开启 reuseport 特性，由内核去把请求分配到 thread 中，并且使用 cpu-map 来把 thread 和具体的 CPU 绑定，有助于缓存的就近访问，提高响应的速度。\n最终的优化配置大概如下：\n# 假设是 4 核机器，单个 MySQL 作为 backend $ cat pdns.conf threads=4 pdns-distributes-queries=no reuseport=yes cpu-map=0=0 1=1 2=2 3=3 Recursor Server 的 lua 扩展 powerdns 可以通过 lua 扩展脚本在查询的基础上实现更复杂的功能。\n# 添加 lua 脚本扩展 $ cat pdns.conf lua-dns-script /path/to/lua/script # 动态重载 lua 脚本 rec_control reload-lua-script powerdns 提供了多个查询钩子，可以在对应的查询阶段进行请求拦截并重写对应的回答动作，有以下几个钩子：\nipfilter ：在查询数据包开始解析之前。 gettag ：在查询数据包缓存之前。 prerpz ：在应用响应策略之前。 preresolve ：在查询逻辑工作开始之前。 nodata, nxdomain ：在返回无数据结果和无域名结果之后。 postresolve ：在查询逻辑工作结束之后。 preoutquery ：在向权威服务器查询之前。 policyEventFilter ：在响应策略命中之后。 由于存在着多阶段的钩子函数，所以实现扩展功能只需要重写对应的函数即可。\n以下是一些参考用例，更多详细用法可以参考官方文档。\n-- 以无响应域名结果的钩子为例，来自官方实例 nxdomainsuffix = newDN(\u0026#34;com\u0026#34;) function nxdomain(dq) pdnslog(\u0026#34;nxdomain called for: \u0026#34;..dq.qname:toString()) if dq.qname:isPartOf(nxdomainsuffix) then dq.rcode = 0 -- 修改为正常应答 dq:addAnswer(pdns.CNAME, \u0026#34;www.powerdns.org\u0026#34;) dq:addAnswer(pdns.A, \u0026#34;1.2.3.4\u0026#34;, 60, \u0026#34;www.powerdns.org\u0026#34;) return true -- return true 说明这个钩子函数生效，如果 return false 则这个钩子函数不生效 end return false end -- 以查询逻辑工作开始之前的钩子为例，由官方实例改写 blockset = newDS() blockset:add{\u0026#34;powerdns.org\u0026#34;, \u0026#34;powerdns.com\u0026#34;} -- 以列表形式添加多个域名 dropset = newDS() dropset:add(\u0026#34;pdns.org\u0026#34;) -- 添加单个域名 function preresolve(dq) -- 重写响应结果 if blockset:check(dq.qname) then dq.variable = true -- disable packet cache in any case if dq.qtype == pdns.A then dq:addAnswer(pdns.A, \u0026#34;1.2.3.4\u0026#34;) return true end end -- 黑名单机制 if dropset:check(dq.qname) then dq.appliedPolicy.policyKind = pdns.policykinds.Drop -- 改 …","date":1647080565,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"787e081e3e2d1a903be604e61d70c392","permalink":"https://yuweizzz.github.io/post/powerdns_performance_guide/","publishdate":"2022-03-12T10:22:45Z","relpermalink":"/post/powerdns_performance_guide/","section":"post","summary":"这篇笔记用来记录 PowerDNS 的性能优化。\n","tags":["Linux","DNS","PowerDNS","Lua"],"title":"PowerDNS 调优","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 LDAP 的相关知识以及如何在 Python 项目中接入 LDAP 。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ LDAP 的基本知识 LDAP 全称为 Light Directory Access Portocol ，它是基于 X.500 标准的轻量级目录访问协议，经常应用于企业的权限管控和登录认证，比如 AD 域控就是微软基于 LDAP 协议所开发的权限管控应用。\n实际上 LDAP 主要是通过维护一个类似于文件目录结构的数据库，根据存储的记录条目提供给外部作为权限管控的数据来源。\n由于涉及到 LDAP 有比较多的名词，在这里先给出常用的 LDAP 名词：\nDN ： Distinguished Name ，区分名称，它用来表示每条记录 entry 的唯一位置。 DC ： Domain Component ，域名的组成部分，多个 DC 共同组成完整的域名。 OU ： Organization Unit ，组织单元，组织是允许多级存在的。 CN ： Common Name ，公共名称，也就是记录的名称。 entry 存储了实际的数据值，LDAP 使用 Objectclass 作为 entry 的基本存储对象，它和编程语言中的对象类似，在 Objectclass 内可以定义多个 Attributes 并且设定具体的值。\n一条 entry 至少需要拥有一个 Objectclass ，所以获取一条 entry 时，就可以拿到它所定义的 Attributes ，再根据这些值去验证对应的权限。\nentry 是通过 DN 来定位的，由于 LDAP 的存储结构是基于层级的，类似于文件目录，我们可以通过类比文件目录来加深对 DC ， OU 和 DN 的理解。\n假设某个 LDAP 定义了三个域名： A.com ， B.com ， C.com ，并且下属均各有两个组织 dev 和 prd ，而我们在 A.com 的 prd 存放了 fileA ，那么整个 LDAP 存储信息会像下面这样：\ncom ├── A │ ├── prd │ │ └── fileA │ └── web ├── B │ ├── prd │ └── web └── C ├── prd └── web 这时我们就可以使用 cn=fileA,ou=prd,dc=A,dc=com 这条 DN 来指向 fileA 这条 entry 。\n可以看到， 在 DN 中 DC 就是对域名的分割，和常规的域名一致，顶级域处于第一位，域名级别越低，它在 DN 中出现的位置越靠前。\n而 OU 和 CN 都是可以多级嵌套的，例子中没有体现，但实际中可以有 cn=fileA,ou=prd,ou=web,dc=A,dc=com 或者 cn=fileA,cn=files,ou=prd,dc=A,dc=com 这样的 DN 。多级 CN 和多级 OU 的顺序也类似于 DC ，都是级别越低，位置越靠前，我们可以自由地根据实际去定义 OU 或者 CN 。\n一般来说， CN 出现在 DN 的首位，这里可以使用 uid 或者 sAMAccountName 去替代 CN 在 DN 中的位置，因为它们都是 entry 的 Attributes ，可以需要根据具体使用的 LDAP 服务器决定。\n接入 LDAP 的代码思路 为了代码更具有通用性，方便接入到各式各样的系统中，认证过程通常可以这样设计：\n在 LDAP 中，在比较高等级的域设置一个只读权限的账号，只需要它可以获取下级记录的各项 Attributes 即可。 使用只读账号连接 LDAP 服务器，做好查询准备。 根据具体的 LDAP 设定和业务需求，确定需要使用的 Attributes 。 当用户连接时，把用户的某部分信息作为 Attributes 的搜索条件，使用只读账号在对应域中找到这条记录后拿出对应的 DN 。 使用这条 DN 和用户提供的密码测试连接是否正常，完成验证。 具体实现代码 使用以下代码前需要先安装依赖：\npip install ldap3 实际代码内容：\nfrom ldap3 import Server, Connection class LDAP(object): def __init__(self): # 具体的 LDAP 服务器地址 self.uri = \u0026#34;ldap://192.168.0.1:389\u0026#34; # 只读账号的 DN self.base_dn = \u0026#34;CN=Admin,ON=Users,DC=example,DC=com\u0026#34; # 只读账号的密码 self.password = \u0026#34;xxxxxx\u0026#34; # 具体的搜索域 self.search_dn = \u0026#34;OU=Accounts,OU=Users,DC=example,DC=com\u0026#34; # 初始化连接 self.server = Server(self.uri) self.admin_connection_bind() def admin_connection_bind(self): self.admin_connection = Connection( self.server, user = self.base_dn, password = self.password, auto_bind = True ) def target_connection_bind(self, dn, password): auth = Connection( self.server, user = dn, password = password ) if auth.bind(): auth.unbind() return True return False def search_Attributes(self, name): hit = self.admin_connection.search( search_base = self.search_dn, # 假设这里的 Attributes 为 sAMAccountName search_filter = f\u0026#34;(sAMAccountName={name})\u0026#34; ) if hit: return self.admin_connection.entries[0].entry_dn return None def auth(self, user, password): dn = self.search_Attributes(user) if dn: return self.target_connection_bind(dn, password) return False ldap_conn = LDAP() print(ldap_conn.auth(\u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;)) 上述代码还有改进空间，不过已经可以覆盖大部分场景，只需要选定的 Attributes 实际值唯一即可。\n","date":1646151765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"a1b87cdbe1a24833de8c14ac49b74819","permalink":"https://yuweizzz.github.io/post/using_ldap_in_python_project/","publishdate":"2022-03-01T16:22:45Z","relpermalink":"/post/using_ldap_in_python_project/","section":"post","summary":"这篇笔记用来记录 LDAP 的相关知识以及如何在 Python 项目中接入 LDAP 。\n","tags":["Python","LDAP"],"title":"Python 项目接入 LDAP","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Django 常用的项目组件。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 创建 Django 项目 在虚拟环境中创建 Django 项目。\n# 激活虚拟环境 $ source project/bin/activate # 安装 Django 3.2 (project) $ pip install django # 初始化 Django 项目 # django-admin startproject name [directory] (project) $ django-admin startproject new_project # 在项目中添加 app (project) $ django-admin startapp new_app # 测试项目 (project) $ python manage.py runserver 不带 directory 的初始化会在当前工作目录创建新的项目目录，然后你可以在其中找到 manage.py 和保存项目配置文件的目录。\n使用 MySQL 做后端存储 Django 默认的后端存储是 sqlite3 ，可以用更强大的 MySQL 或者 PostgreSQL 代替。\n# 安装依赖 (project) $ pip install mysqlclient # 安装后需要将以下内容写入到 settings.py 中 (project) $ cat settings.py ... DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;database_name\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;root\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;mysecret\u0026#39;, \u0026#39;ATOMIC_REQUESTS\u0026#39;: True, } } ... # Django 不会自动生成 database ，需要手动创建数据库 mysql \u0026gt; CREATE DATABASE database_name -\u0026gt; DEFAULT CHARACTER SET utf8 # 尽量把字符集设置为 utf8 ，这样可以将中文字符存入数据 -\u0026gt; DEFAULT COLLATE utf8_general_ci; # 在 app 中定义 model 后，需要执行迁移命令生成对应的表 (project) $ python manage.py makemigrations (project) $ python manage.py migrate 如果因为已有数据库的字符集不是 utf8 而导致的中文字符乱码，可以这样做：\n# 查看表的详细信息 mysql \u0026gt; SHOW CREATE TABLE table_name; # 修改 database 的字符集，但这对已有字段不起效果 mysql \u0026gt; ALTER DATABASE db_name DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; # 修改所有字段的字符集，可以即时生效 mysql \u0026gt; ALTER TABLE table_name CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; 使用 Redis 做缓存引擎 低版本的 Django 原生支持 Memcached 作为缓存引擎，我们可以额外安装扩展来支持 Redis ，而 Django 4.0 已经原生支持 Redis 作为缓存引擎。\n可选的 Redis 扩展有很多，比较推荐使用 django-redis 和 python-redis-lock 的组合应用。\n# 只使用 django-redis (project) $ pip install django-redis # 安装后需要将以下内容写入到 settings.py 中 (project) $ cat settings.py ... CACHES = { \u0026#34;default\u0026#34;: { \u0026#34;BACKEND\u0026#34;: \u0026#34;django_redis.cache.RedisCache\u0026#34;, \u0026#34;LOCATION\u0026#34;: \u0026#34;redis://127.0.0.1:6379/1\u0026#34;, \u0026#34;OPTIONS\u0026#34;: { \u0026#34;CLIENT_CLASS\u0026#34;: \u0026#34;django_redis.client.DefaultClient\u0026#34;, \u0026#34;PASSWORD\u0026#34;: \u0026#34;mysecret\u0026#34; } } } ... # 使用 django-redis 和 python-redis-lock 的组合 (project) $ pip install \u0026#34;python-redis-lock[django]\u0026#34; # 这是 python-redis-lock 推荐使用的方法，所有扩展都会自动安装 # 安装后需要将以下内容写入到 settings.py 中 (project) $ cat settings.py ... CACHES = { \u0026#39;default\u0026#39;: { \u0026#39;BACKEND\u0026#39;: \u0026#39;redis_lock.django_cache.RedisCache\u0026#39;, \u0026#39;LOCATION\u0026#39;: \u0026#39;redis://127.0.0.1:6379/1\u0026#39;, \u0026#39;OPTIONS\u0026#39;: { \u0026#39;CLIENT_CLASS\u0026#39;: \u0026#39;django_redis.client.DefaultClient\u0026#39; } } } ... # 原生的 django-redis 没有 lock 函数，使用 python-redis-lock 可以较好应对并发的问题 (project) $ cat cache_test.py from django.core.cache import cache def function(): val = cache.get(key) if not val: with cache.lock(key): val = cache.get(key) if not val: # DO EXPENSIVE WORK val = ... cache.set(key, value) return val 使用 Celery 做任务队列 Celery 是分布式任务队列，可以非常方便地实现任务调度。\nCelery 通过消息机制进行通信，生产者将消息发布到 Broker 中， Worker 会消费 Broker 中的消息，执行具体的任务内容， Celery 可以有多个 Worker 和 Broker ，实现高可用和横向扩展。\n在已有的 Djnago 项目中使用 Celery 只需要简单添加修改几个文件即可。\n# 在 Django 项目中配置 Celery # 假设已有项目名为 project ，那么应该在 project 配置文件的目录中新增这个文件 # project/project/celery.py import os from celery import Celery # Set the default Django settings module for the \u0026#39;celery\u0026#39; program. os.environ.setdefault(\u0026#39;DJANGO_SETTINGS_MODULE\u0026#39;, \u0026#39;project.settings\u0026#39;) # Enable running a worker with superuser privileges, don\u0026#39;t use it in production. os.environ[\u0026#39;C_FORCE_ROOT\u0026#39;] = \u0026#39;True\u0026#39; app = Celery(\u0026#39;project\u0026#39;) # Using a string here means the worker doesn\u0026#39;t have to serialize # the configuration object to child processes. # - namespace=\u0026#39;CELERY\u0026#39; means all celery-related configuration keys # should have a `CELERY_` prefix. app.config_from_object(\u0026#39;django.conf:settings\u0026#39;, namespace=\u0026#39;CELERY\u0026#39;) # Set timezone app.conf.timezone = \u0026#39;UTC\u0026#39; # Load task modules from all registered Django apps. app.autodiscover_tasks() # 在 Django 项目启动时，同时加载完成配置的 Celery 实例 # project/project/__init__.py # This will make sure the app is always imported when # Django starts so that shared_task will use this app. from .celery import app as celery_app __all__ = (\u0026#39;celery_app\u0026#39;,) # 使用指定 namespace 的 config_from_object 从 Django settings 加载 Celery 配置 # 这里使用 Redis 作为 Broker 和任务执行结果的保存后端，实际可以根据情况使用不同的后端 # part of project/project/settings.py CELERY_BROKER_URL = \u0026#39;redis://127.0.0.1:6379/2\u0026#39; CELERY_RESULT_BACKEND = \u0026#39;redis://127.0.0.1:6379/3\u0026#39; CELERY_TASK_SERIALIZER = \u0026#39;pickle\u0026#39; CELERY_RESULT_SERIALIZER = \u0026#39;pickle\u0026#39; CELERY_ACCEPT_CONTENT = [\u0026#39;json\u0026#39;, \u0026#39;pickle\u0026#39;] # 在具体 Django app 中添加任务实体，它会由 autodiscover_tasks 自动发现 # project/appA/tasks.py from celery import shared_task @shared_task def add(x, y): return x + y 可以看到大概的配置步骤如下：\n创建 Celery 实例。 在设置 Django 项目的环境变量完成后，从它们之中获取 Celery 实例需要的配置。 执行自动任 …","date":1642288965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"266a1f48dacf9e53879e09507d40ce89","permalink":"https://yuweizzz.github.io/post/knowledge_about_django/","publishdate":"2022-01-15T23:22:45Z","relpermalink":"/post/knowledge_about_django/","section":"post","summary":"这篇笔记用来记录一些 Django 常用的项目组件。\n","tags":["Python","Django"],"title":"Django 知识笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 python 编程的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 创建 python 虚拟环境 在 python 项目中经常会因为依赖的版本不同而存在不兼容的问题，所以我们应该使用虚拟环境将不同项目的依赖隔离，否则混用的依赖可能会产生各种各样的运行问题。\n在 python2 中经常使用 virtualenv 作为虚拟环境管理库，在 python3 中可以直接使用标准库的自带虚拟环境管理库 venv 。\n# python2 的 virtualenv 需要独立安装 $ pip install virtualenv # 使用 python2 的 virtualenv 创建虚拟环境 $ virtualenv project # 激活后可以获得独立的虚拟环境 $ source project/bin/activate (project) $ python --version Python 2.7.5 (project) $ pip list Package Version ---------- ------- pip 20.3.3 setuptools 44.1.1 wheel 0.36.2 # 退出虚拟环境 $ deactivate # python3 的 venv 不需要额外安装 # 实际使用和 virtualenv 没有特别大的区别 $ python3 -m venv project $ source project/bin/activate (project) $ python --version Python 3.6.8 (project) $ pip list pip (9.0.3) setuptools (39.2.0) $ deactivate 依赖管理 创建虚拟环境后就可以直接安装需要的依赖，我的建议是除了虚拟环境管理库，其他依赖尽量不要在全局环境安装。\n# 更换镜像源 $ pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ # 安装全局依赖 $ pip install virtualenv # 在虚拟环境中安装需要的包 (project) $ pip install \u0026lt;package\u0026gt; # 将项目中的依赖导出 (project) $ pip freeze \u0026gt; requirements.txt # 导入已定义的依赖列表 (project) $ pip install -r requirements.txt 字符串格式化 在 python 中对字符串格式化的操作比较频繁，这里做个备忘，记录一下相关的方法。\n# format 在 python2 和 python3 中均可用 name = \u0026#34;python\u0026#34; greet = \u0026#34;hi, this is {}\u0026#34;.format(name) # f-string 表达式在 python 3.6 以上版本可用 name = \u0026#34;python\u0026#34; greet = f\u0026#34;hi, this is {name}\u0026#34; # 可以对原有数据做格式化，主要通过 : 来定义输出格式 value = 16 # 十进制数字转为十六进制字符串 value_format = \u0026#34;{:x}\u0026#34;.format(value) value_format = f\u0026#34;{value:x}\u0026#34; 这里介绍的方法返回值都是字符串，比较适合用在格式化输出或者日志记录中。\n字典的常用方法 字典在各类编程语言中非常常见，这里记录一下 python 中字典的常用方法。\nCount = {\u0026#34;A\u0026#34;: 10, \u0026#34;B\u0026#34;: 20} # 删除一个原有的键值对 Count.pop(\u0026#34;A\u0026#34;) # 增加一个新的键值对 Count.update({\u0026#34;C\u0026#34;: 30}) # 获取指定键的值，如果没有这个键则返回预设值 Count.get(\u0026#34;A\u0026#34;, 0) # 遍历所有键值对 for each in Count.items(): # items 返回值是元组 print(each) for key, value in Count.items(): # 直接在循环中拆分元组 print(key, value) 集合 在 python 中集合使用的频率可能会比较低，但它在某些场合非常好用。\n集合最大的特点在于集合内的所有元素都是唯一的。\narray = [1, 1, 2, 2, 3, 3] print(set(array)) # print 将会输出 {1, 2, 3} 实际中，集合应该是通过 hash 来实现这个特性的。\n由于列表是可变的对象，如果你想将任意嵌套列表转化为集合就会得到 unhashable type 的报错，因为它无法计算出确定的 hash 值。相反地，常规的数字，字符串是 hashable 的，并且 python 的元组也是 hashable 的。\n我们可以利用集合元素的唯一性，来实现两部分数据的差异对比：\n首先提取两部分数据的关键值，它应该像数据库记录的主键，作为这部分数据的最大特征值。 将它们分别转化为集合，并取得补集，或称为对称差集。 对这个对称差集做遍历，将它们分类归属。 # 假设有以下两个数据集 Old = {\u0026#34;A\u0026#34;: 10, \u0026#34;B\u0026#34;: 20, \u0026#34;C\u0026#34;: 30} New = {\u0026#34;B\u0026#34;: 100, \u0026#34;C\u0026#34;: 30, \u0026#34;D\u0026#34;: 40} # 拿到数据集的所有重要特征值组合为元组 Old_tuples = [(\u0026#34;A\u0026#34;, 10), (\u0026#34;B\u0026#34;, 20), (\u0026#34;C\u0026#34;, 30)] New_tuples = [(\u0026#34;B\u0026#34;, 100), (\u0026#34;C\u0026#34;, 30), (\u0026#34;D\u0026#34;, 40)] # 集合化后取补集 Old_set = set(Old_tuples) New_set = set(New_tuples) Sysmmetric_Difference = Old_set ^ New_set # Sysmmetric_Difference: {(\u0026#39;D\u0026#39;, 40), (\u0026#39;A\u0026#39;, 10), (\u0026#39;B\u0026#39;, 100), (\u0026#39;B\u0026#39;, 20)} # 遍历数据 for i in Sysmmetric_Difference: if i in Old_tuples: print(\u0026#34;Old Data: \u0026#34;, i) elif i in New_tuples: print(\u0026#34;New Data: \u0026#34;, i) # 实际输出： # New Data: (\u0026#39;D\u0026#39;, 40) # Old Data: (\u0026#39;B\u0026#39;, 20) # Old Data: (\u0026#39;A\u0026#39;, 10) # New Data: (\u0026#39;B\u0026#39;, 100) # 这里还不能完全确定部分数据，可以多做一次特征筛选 Indexs = [ key for key, _ in Sysmmetric_Difference ] # Counter 可以统计列表的元素出现次数 from collections import Counter Both = [ key for key, value in dict(Counter(Indexs)).items() if value \u0026gt; 1 ] for key, value in Sysmmetric_Difference: if key in Both and (key, value) in Old_set: print(\u0026#34;Before Changed Data: \u0026#34;, (key, value)) elif key in Both and (key, value) in New_set: print(\u0026#34;After Changed Data: \u0026#34;, (key, value)) elif (key, value) in Old_set and (key, value) not in New_set: print(\u0026#34;Old Data: \u0026#34;, (key, value)) elif (key, value) in New_set and (key, value) not in Old_set: print(\u0026#34;New Data: \u0026#34;, (key, value)) # 实际输出： # New Data: (\u0026#39;D\u0026#39;, 40) # Before Changed Data: (\u0026#39;B\u0026#39;, 20) # Old Data: (\u0026#39;A\u0026#39;, 10) # After Changed Data: (\u0026#39;B\u0026#39;, 100) 以上只是一个理想化的计算过程，实际中应该要结合目标场景来使用。\n迭代器和生成器 这是对使用 map 函数引发异常的探究。\ndef add(x): return x + 1 List = [1, 2, 3, 4, 5] Map = map(add, List) print(list(Map)) print(list(Map)) # 在使用 python3 的情况下： # 第一次输出为 [2, 3, 4, 5, 6] # 第二次输出为 [] 在 python 中，很多数据类型都是可迭代的，比如列表就是最经典的可迭代对象，可以使用 isinstance(Object, Iterable) 来判断对象是否是可迭代的， Iterable 需要从 collections 中导入。\n实际中，一个对象如果是可迭代的，那么它需要实现 __iter__ 方法或者 __getitem__ 方法。\n其中 __getitem__ 方法常见于通过索引取值的数据类型，而 __iter__ 是可迭代对象优先使用的方法，一般它会返回迭代器对象 iterator object 。\n迭代器对象的作用是遍历可迭代对象。它自身需要实现 __iter__ 和 __next__ 两种方法。\n其中 __iter__ 还是用于返回迭代器对象，通常就是这个 iterator object 本身， __next__ 则是迭代器对象用来获取下一元素的方法。\n在 python2 中， map 函数返回是一个列表，但在 python3 中， map 函数返回是一个 map 对象。虽然列表和 map 对象都是可迭代对象，但是实际中 map 对象是迭代器对象，所以在上面的测试用例中， map 对象经历两次迭代，第二次迭代时 __next__ 已经无法继续获取下一个元素，取值为空。\n除了可迭代对象和迭代器，还会使用到生成器 generator object ，实际中它就是迭代器对象的子类。\n我们一般通过关键字 yield 定义生成器函数。\ndef gen(): yield \u0026#34;step1\u0026#34; yield \u0026#34;step2\u0026#34; yield \u0026#34;step3\u0026#34; print([each for each in gen()]) # …","date":1641334965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"fecbce86b396dc853bc1b068d90b04e9","permalink":"https://yuweizzz.github.io/post/knowledge_about_python/","publishdate":"2022-01-04T22:22:45Z","relpermalink":"/post/knowledge_about_python/","section":"post","summary":"这篇笔记用来记录一些 python 编程的相关知识。\n","tags":["Python","ElasticSearch"],"title":"Python 知识笔记","type":"post"},{"authors":null,"categories":null,"content":"2021 年总结和新年展望。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 正文 就我个人感觉来说，每次思考时，虽然心里想到了很多，但是口头表达却不及其一，我越发认同提笔记录是很好的办法。\n在今年的年末回首前，先记录一件小事，我在周二早上起床后就感觉喉咙不舒服，但当时情况并不是十分严重，因为我本身就有咽炎，这一天也就这么过去了。\n等到周三早上情况就加重了，而且在中午休息过后一度有发热的迹象，我对自己的身体比较清楚，如果有十分疲惫的感觉，就有很有可能导致发热，这回我不敢忽视了，一下班就马上买了药，去买药的时候还担心已经发热被测温仪器检查出来，当天晚上睡觉感觉浑身发冷，这是比较明显的发热前兆。\n周四早上起床这种发热的感觉和咽喉发炎的情况有所好转，但人还是不太清醒，坚持到了公司，等到下午情况渐渐好转，虽然咽喉炎症依然明显，但是疲惫的感觉渐渐退去。\n一整天也吃不下东西，人也是精神恍惚，回到家吃了药，人反而变得有精神了，临睡前居然感觉到饥饿。 昨天晚上我躺在床上，想着明天病好了，一定要多搞点好吃的。\n幸运的是今天真的好了一大半，虽然说不上完全恢复，但是至少人可以正常活动了，所以今天就血拼了一把，买了不少好吃的，当作是庆祝下。\n这次的事情对我来说，对我最大的触动是周四晚上躺在床上，畅想明天的美食，那种摒弃所有弊病，那种带着对明天美好的强烈期待，实在是令人备受鼓舞。\n在今年的工作和生活中，我感觉陷入了一种困境，当萦绕在心头的疑问越来越多，而它们无法得到解答时，会渐渐消磨对生活的热情，每一天的你对明天不再抱有期待，快乐也跟着变少了，就如同病重躺在床上时，人是非常痛苦的，如果是抱着痊愈的希望，想着明天的能吃到的好吃食物，那这苦中还留有着快乐，但如果没有对于明天的希望，心中的苦闷将会比身体上的更加令人难受。\n我认为它可以作为今年的主基调，进入年末总结的正题：\n在生活上，疫情时不时爆发，虽然我已经明白它基本上会长期存在，但是坏消息总是能带来无形的压力，还有来自互联网的多元信息，包括政府政策，网民言论，突发事件，企业变动，八卦事件等等，它们可能不会直接关系到我个人，但是它们经常带来大量的负面信息，这对生活热情也是极大的打击。所以我经常通过一些比较低回报的事情消磨时间，比如看网络小说，熬夜打游戏，我个人认为这些事情不至于做到完全杜绝，但是它们应该被有计划地控制，但是没有较好做到自我控制是今年比较失败的地方。\n在工作上，大形势是新的技术不断推出，但是旧的技术又不得不用，而它们都需要时间成本去学习，我觉得来自时间上的压力和方向上的困惑是两个重要原因，不像在读书的时候，每次都有着明确的方向和目标，我只需要顶住时间上的压力就可以了。但在工作中，方向和目标都非常容易迷失，由于这种迷失甚至导致了专注力的下降，很容易出现懈怠和厌恶，导致最终一无所成，这是现在最令我恐惧的地方。\n还有就是日益增长的孤独感，像这次生病的事情，我个人不想向任何人直接透露，其实这是不健康的心理状态，因为人的生活需要交流，不管是人生经验或者是技术经验，都应该是交流催生更大的进步。这是今年甚至说已经存在了较长时间的，人际交流方面的欠缺。\n前面的可能涉及了比较多的消极部分，但其实今年也是有收获的，首先是我已经认识到了过往的一些缺陷，这是一个好的开始，那么下一年需要做到事情就是改进，虽然我知道这可能是困难重重的，但我认为这应该可以做到更好。\n在工作和个人学习上，虽然常有懈怠，但是我一直是保持着学习的习惯，自我学习并没有停止过，总结今年，我的综合能力确实是提升了，也许不是在某个具体技能上，但它将会是更强技术能力的基石。\n关于孤独这方面，我认为它是人生当中不可避免的，虽然它在我的人生占据了更大的比例，但是目前我觉得这样也还行，甚至更好地利用它，即使这是一条更困难的路。我认为性格是这方面很重要的影响因素，而它不是目前强烈需要改变的地方，也不是一时能解决的，我更愿意顺其自然，慢慢改变。\n而且今年还收获了一个新的乐趣，集邮过程也许不全是快乐，有时会因为某个套票而着急，有时会懒得动手去做一些必要的洗片工作，但是我认为它是可以成为锻炼毅力的工具，全心地投入并坚持一项事业是非常难得的。\n好的也说完了，坏的也说完了，我感觉明年来说，也许人生的大方向是迷离不定的，但是踏实过好每一天，让自己每天都快乐也是很不错的，在下一年，我觉得至少要做到改进一些事情：\n生活上，注意保护身体，毕竟生病太难受了，希望可以改善不吃早餐的毛病和尽量早睡。\n工作学习上，我感觉做好规划是比较有效的方法，虽然现在还保持着学习，但是没有良好的规划和整理的话，学习效果会大打折扣。今年整年都是无组织无计划，导致欠下了不少技术债。希望下一年可以更好地改进。\n此外，对于外部信息的接收，我感觉自身的见识和认知还不能够抵御它们的冲击，这个目前还是无解，也许多看看书，少接收一些消极的外部信息是好办法，下一年可以朝着这个方向多做尝试。\n其实我不常做总结或者回顾，但希望这次的破例能给下一年带来更好的自己。\n写于 2021 年 12 月 24 日，平安夜。\n","date":1640382285,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"816dbd3445fb8fa839736fbb04d8758c","permalink":"https://yuweizzz.github.io/post/review_of_2021/","publishdate":"2021-12-24T21:44:45Z","relpermalink":"/post/review_of_2021/","section":"post","summary":"2021 年总结和新年展望。\n","tags":["Life"],"title":"回顾 2021 年","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些服务器磁盘管理的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 服务器磁盘管理 如今的家用计算机可能很多已经使用了 NVMe 协议的固态硬盘，但是一般情况下你仍然可以看到用于接入 SATA 硬盘的接口，它们是通过直接接入到主板南桥中的 SATA 控制器来扩展额外硬盘的。\n服务器上的情况会略微不同，因为一般的 SATA 控制器只能接入有限数量的硬盘，通常是两个到四个这一量级。服务器动辄十几个以上的硬盘数量，只靠主板集成的 SATA 控制器是远远不够的，这时就需要用到 HBA 卡。\n关于磁盘的总线控制芯片，市场上很多使用的是 Broadcom 的芯片和对应的扩展卡，如果是 Broadcom 生产的扩展卡，一般可以通过 lspci 查看到 LSI ， AVAGO 相关的字样，这是由于这家公司经过了多次并购的原因。\n我们所说的 HBA 卡就是 Host Bus Adapter ，外观和使用上与网卡这一类的 PCI 扩展设备相似，接入这种卡的硬盘设备就不再经过南桥，而是通过它提供的 PCIe 通道和 CPU 进行数据交互，一般具有 12 Gb/s 和 24 Gb/s 的吞吐能力，比如使用 SAS3008 芯片的 LSI SAS 9300-8i 扩展卡就是 12 Gb/s ， HBA 卡使用的集成芯片一般是 SAS/SATA Storage I/O Controllers ，也就是只支持 SAS 和 SATA 接口的硬盘设备。但是近年来随着 NVMe 协议硬盘的使用越来越频繁，带有 Tri-Mode I/O Controller 的新一代 HBA 卡已经可以直接支持 PCIe/NVMe 协议的硬盘。\n另一类带有 RAID 控制能力的扩展卡是 RAID Controller Card ，也就是常说的 RAID 卡，相较于 HBA 卡，它使用的芯片是 RAID-on-Chip ICs ，除了常规的 IO 控制之外还具有磁盘阵列的控制功能。可以认为是一种更智能的 HBA 卡，吞吐能力和 HBA 一样，具有 12 Gb/s 和 24 Gb/s 两种级别，也具有 Tri-Mode 类型的芯片。之前比较多见的 RAID 卡类型是使用 SAS3108 芯片的 MegaRAID SAS 9361-8i 。\n除了原厂生成的扩展卡，很多服务器厂商使用 Broadcom 的芯片制作自己的扩展卡，比如 DELL 和 inspur 以及 Lenovo 等公司都有这样的操作，虽然最终制成的 RAID 卡名称各不相同，但一般情况下都可以使用 megacli 和 storcli 这两个工具通过命令行进行管理硬件 RAID 。\n不过 SAS/SATA Storage I/O Controllers 这一类芯片可以通过特定的 firmware 来支持 RAID 功能，比如 SAS3008 IR 版本就可以支持 RAID0 和 RAID1 ，具有 IR 能力的较旧芯片 LSI SAS-3 controllers 系列可以使用 sas3ircu 工具通过命令行管理， LSI SAS-2 controllers 系列对应的是 sas2ircu 。\n除了两种扩展卡外还有一种支持更大磁盘数量的扩展设备是 SAS expanders ，但这种扩展设备一般用于盘柜，它可以聚合大量的磁盘，并承担中转设备的责任，还需要配合服务器上的 HBA 卡来使用，因为 SAS expanders 不具有直接 IO 的能力。\n一开始我对 SAS expanders 这种设备是有疑惑的，但有一种情况可以帮助理解这种设备的作用，前面所说的 LSI SAS 9300-8i 和 MegaRAID SAS 9361-8i 都具有相同的 8i 命名结尾，这是由于这两种卡上都有两个 SFF8643 的接口，每个 SFF8643 可以分出四个 SFF-8482 接口直连硬盘，所以 8i 的含义就是可以直连 8 个硬盘。我们可以看到更强大的 16i 和 24i 的扩展卡，不支持 Tri-Mode 一般由支持 4i 的 SFF8643 来组合扩展卡接口，支持 Tri-Mode 的一般由支持 8i 的 SFF8654 来组合扩展卡接口，这样可以支持更多数量的硬盘。\n很多 2u 的服务器都采用 12 Bay 的硬盘背板 backplane ，有一些服务器可能还扩展到 16 Bay ，但是这样的机器通常可以使用一张 8i 的扩展卡管理所有磁盘，是由于服务器的硬盘背板 backplane 集成了 SAS expanders 芯片，由它去中转 8i 能力之外的硬盘，这种情况应该可以帮助理解 SAS expanders 设备的存在意义。\n磁盘阵列等级 前面所说的 RAID 就是磁盘阵列技术，全称为 Redundant Array of Independent Disks ，它是将多个物理磁盘组成一个逻辑磁盘的技术，可以提高磁盘的逻辑容量，带来更强的性能和数据冗余能力。\n根据磁盘的数量可以创建不同的 RAID 等级，分别具有不同的性能表现和冗余能力提升，比较常用的有下面几个等级：\nRAID0 ：至少需要两个或两个以上的物理磁盘，逻辑容量为所有磁盘容量的总和，读写性能会增强，但是没有数据冗余能力，一个磁盘损坏则整个逻辑磁盘都会丢失数据。在写入数据时， RAID 控制器会将数据条带化，分别存储在不同的子磁盘中，读取时也需要同时获取不同子磁盘中的存储条带才能最终得到完整文件。 RAID1 ：至少需要两个或两个以上的物理磁盘，逻辑容量为单个磁盘的容量，读写性能会增强，数据冗余能力会增强，容错能力取决于磁盘数，只要最终仍有一个磁盘正常，那么数据就是安全的。在写入数据时，所有子磁盘都是镜像写入的，各子磁盘的存储内容一致，在读取数据时效率会更高。 RAID5 ：至少需要三个或三个以上的物理磁盘，逻辑容量为磁盘容量总和减去一个磁盘容量（n-1），最大的容错能力为一个磁盘。在写入数据时， RAID 控制器会将数据条带化并计算奇偶校验码，分别存储在不同的子磁盘中，读取效率和 RAID0 近似，同时可以通过校验码容忍单个磁盘的故障。 RAID10 ：复合型 RAID 等级，先组合出多个 RAID1 逻辑盘，再将所有逻辑盘组合为 RAID0 逻辑盘，弥补了 RAID0 的数据冗余能力，但是继承了 RAID1 容量利用率低的问题。 在 Linux 中有软 RAID 的概念，但软 RAID 是由操作系统模拟 RAID 行为，性能比不上硬件 RAID ，生产环境中一般只用 RAID 卡或者主板集成的 RAID 控制器来实现 RAID 功能。\nRAID 组策略 RAID 卡一般会配置缓存模块和相应的电源模块，一些老式的 RAID 卡甚至还会使用外置电池，外置电池损坏是常有的事情，会有缓存丢失的风险，但现代的 RAID 卡一般使用内置的电源模块，提高了耐用性。\n新建的 RAID 组可以设置不同的策略来达到性能目标：\n写策略 Write Policy ：\nWrite Through (WT) ：直写， RAID 组的写入 IO 请求会直接写入物理磁盘。 RAID0 推荐使用这种策略，可以实现最好的写入带宽。 Write Back (WB) ：回写， RAID 组的写入 IO 请求会写入到 RAID 卡的缓存。这种策略会有掉电丢失数据的风险，所以有些 RAID 卡在缓存电源模块异常时会拒绝使用这种策略。 读策略 Read Policy ：\nNo Read Ahead (NORA) ：不预读，RAID 组的读取 IO 请求会以 block 级别去处理，对随机读取友好。 Read Ahead (RA) ：预读， RAID 组的读取 IO 请求会读取 block 所在的整个条带并进行缓存，对顺序读取友好。 Adaptive Read Ahead (ADRA) : 自适应预读，由 RAID 卡根据读取 IO 请求决定使用 RA 或者 NORA 。 IO 策略 IO Policy ：\nCached IO ：启用缓存，这里的缓存是 RAID 卡的读写缓存，类似于操作系统的缓存。 Direct IO ：不启用缓存。 磁盘缓存策略 Disk Cache Policy ：\nUnchange ：保持磁盘原有的策略设置。 Cached ：启用缓存，这里的缓存是磁盘自身的读写缓存。 Direct ：不启用缓存。 很多情况下是根据自身的业务特性来设置读写策略的，读策略更多考量顺序读取和随机读取的比例，写策略在固态硬盘做子磁盘时基本都会选择直写，使用机械硬盘的 RAID5 和 RAID1 会选择回写， RAID0 可以选择直写。\n至于 IO 策略一般都是禁用 RAID 缓存和磁盘缓存，禁用 RAID 缓存是由于操作系统已经拥有缓存机制，禁用磁盘缓存是由于它不像 RAID 卡缓存一样拥有自己的掉电保护机制。\nRAID 卡命令行工具使用 这里介绍用于 HBA 卡的 sas3ircu 。\n# 查看所有控制器 $ sas3ircu list # 会输出所有可识别的扩展卡信息和对应 Index # 根据 Index 查看某个控制器的磁盘信息和 RAID 信息 $ sas3ircu 0 display # Enclosure 用来索引不同的 backplane # Slot 用来索引 backplane 中的槽位 # IR 模式的 HBA 卡可以设置 RAID 组， IT 模式的无法使用 RAID 功能 # 假设 Enclosure Id 为 2 ，使用两块硬盘创建 RAID1 组 $ sas3ircu 0 create RAID1 max 2:0 2:1 noprompt # 通过 volumeid 删除 RAID 组，假设 volumeid 为 100 ，实际可以使用 display 看到 volumeid $ sas3ircu 0 deletevolume 100 noprompt # 点亮指定硬盘的指示灯，在替换故障盘时非常有用，可能需要手动关闭 $ sas3ircu 0 locate 2:0 on $ sas3ircu 0 locate 2:0 off # 混合使用 RAID 组和直接使用物理盘可能会导致启动异常，可以根据需要指定启动设备 # 通过 volumeid 设置 RAID 组作为主启动项 $ sas3ircu 0 bootir 100 # 通过 volumeid 设置 RAID 组作为备选启动项 $ sas3ircu 0 altbootir 100 # 设置单盘作为主启动项 $ sas3ircu 0 bootencl 2:0 # 设置单盘作为备选启动项 $ …","date":1636321740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"e8075b0f3cd7b1eff9375642696e9df6","permalink":"https://yuweizzz.github.io/post/knowledge_about_raid_and_hba/","publishdate":"2021-11-07T21:49:00Z","relpermalink":"/post/knowledge_about_raid_and_hba/","section":"post","summary":"这篇笔记用来记录一些服务器磁盘管理的相关知识。\n","tags":["HDD","RAID","HBA"],"title":"硬盘知识拓展：总线适配器和磁盘阵列","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些硬盘的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 常见的硬盘接口和传输速率 硬盘这一类外设存储处于计算机存储设备中最外围的一层，但它是数据持久化的核心部分，可靠性和传输速率是非常重要的指标。\n外设和计算机通信需要建立传输链路，这个链路一般称为总线 bus ，使用通用的外设接口 interface 来增强总线的扩展性，使用传输协议 protocol 来协同传输信号。\nIDE 早期的硬盘接口为 IDE ，全称为电子集成驱动器 Integrated Drive Electronics ，它是直连主板的 40 针并口总线设计，拥有独立的电源接口。在一些老旧的 intel 奔腾 PC 机还可以见到这类硬盘，它使用的是宽大的并行排线，一般一个排线有两个接口，用于接入主从两块 IDE 硬盘。\nIDE 接口使用的传输协议一般称为 ATA ，它已经演变了多个版本，每个版本的演进都增强了总线的带宽，在这个过程也衍生出了一些变种的接口，在最后的并行总线版本 ATA-7 中，使用 IDE 并行接口的硬盘最大的传输速率达到 133 MB/s ，之后并行总线的设计慢慢由串行总线取代，所以我们所说的 IDE/ATA/PATA 一般都是指这类 40 针并行接口的硬盘。\n之后是流行的是 SATA 硬盘接口，至今还是主流的接口种类。\nSATA SATA ，全称为串行的 ATA 接口 Serial ATA ，它的很多设计都是在 IDE 之上延续的，所以传输协议也不可避免地使用了一些原来的设计。相比于 IDE 接口，它的优势在于排线简单，抗干扰性强，更快的传输速率和更多的扩展功能。 SATA 接口是长条形状的物理接口，中间有断口将整个长条分为长短两个部分，并且断口处有防呆设计。其中长的部分为电源接口，短的部分为数据接口。除了通用的 SATA 接口类型，还有 mSATA ， eSATA 等物理接口变体。\nSATA 接口的传输协议有 3 个版本：\nSATA 1.0 ： 1.5 Gbps ， 8b/10b 编码 ， 150 MB/s\nSATA 2.0 ： 3 Gbps ， 8b/10b 编码 ， 300 MB/s\nSATA 3.0 ： 6 Gbps ， 8b/10b 编码 ， 600 MB/s\n我们可以通过换算公式 总线传输速率 * 编码 / 8 = 总线带宽 计算它的带宽。其中编码一般为校验或者标志位的码率损耗，由比特位换算到存储字节单位还需要除以 8 ，但是最终的带宽值为理论值，实际传输还会有折损，需要再去掉 10% 才能接近实际的带宽。\n我们可以看出串行设计的优势所在，即使是最低版本的协议都要比并行版本来的快。但是 SATA 接口的硬盘本身速率都在 100 MB/s 到 200 MB/s 之间，即使是固态硬盘也大多在 300 MB/s 到 400 MB/s 之间，总线的理论上限带宽已足够现有的硬盘使用。\nSATA 的传输协议继承了原有 ATA 的一些指令，为了支持原生的 SATA 指令，使用 SATA 设备的高级功能，英特尔制定了 AHCI 技术，来实现这一目的。它可以看做是 SATA 总线扩展的传输协议。现有的 PC 或者服务器的集成 SATA 控制器大都默认使用了这种协议，因为大部分现代设备本身就支持 AHCI 标准。它对一些场景下的设备性能有优化作用，但在带宽上没有特别明显的提升。\nSAS 另一种常见的硬盘物理接口形式为 SAS ，全称为 Serial Attached SCSI ，是 SCSI 技术的一种变体，它和 SATA 硬盘类似，都使用了串行总线技术以获得更高的传输速度。 SAS 接口是长条形状的物理接口，对比 SATA 接口来说，它的中间没有断口，而是使用突起取代，它外观上和 SATA 接口非常相似，所以大部分服务器的背板兼容这两种物理接口。\nSCSI ，全称为 Small Computer System Interface ，它是一套庞大复杂的技术标准，比起 SATA 来说，它的应用面更广泛，接口类型也更多。 SAS 只是其中的一种，它的设计兼具了 SATA 的特色，并且在 SAS 的协议栈中，兼容了 SATA 的传输协议，这就是 SAS 控制器能够兼容使用 SATA 接口硬盘的原因。 SAS 比起 SATA 来说成本更高，在 PC 和服务器中，主板很少集成 SAS 控制器，虽然有少数的服务器主板会集成 SAS 控制器，但是更多地是通过外加 HBA 卡或者 RAID 卡来连接 SAS 硬盘。\nSAS 接口的传输协议也有 3 个版本：\nSAS 1.1 ： 3 Gbps ， 8b/10b 编码 ， 300 MB/s\nSAS 2.1 ： 6 Gbps ， 8b/10b 编码 ， 600 MB/s\nSAS 3.0 ： 12 Gbps ， 8b/10b 编码 ， 1200 MB/s\nSAS 的优势在于存储集群的场景，带宽比较大，并且可以兼容 SATA 这类主流的硬盘接口。在现有的机械硬盘接口中，使用最多的就是 SATA 和 SAS ，但整个存储构架使用了 SAS 的模式，就是因为它的兼容优势。\n此外还有两种比较特殊的物理接口: M.2 和 U.2 ，它们多用于固态硬盘中。\nM.2 和 U.2 固态硬盘最明显的特点是速率远高于机械硬盘，为了适应这种速度，总线设计或者传输协议就要进行相应的改进。\n在不改变原有总线设计的情况下，普通的固态硬盘还是使用了 SATA 接口，虽然它的速率提升了，但是以 SATA 总线的传输效率还是可以满足它的理论需求。\n为了获得更高的总线传输效率，一些固态硬盘弃用了 SATA 总线和接口，使用了 PCIe 总线作为新的传输总线，这是直通 CPU 的捷径，速度提升非常明显。同时也就诞生了新的硬盘接口类型和传输协议。\n原生的 PCIe 接口是给网卡或者显卡外设使用，是直接安装在主板的 PCIe 插槽中，一些固态硬盘也是使用了这样的接口设计，还有 M.2 和 U.2 两种接口设计。\nM.2 接口是金手指直接安装在主板上的，接触面小，硬盘体积也比较小，外形类似于直尺。\nU.2 接口则和 SAS 硬盘基本一致，长条形状的接口，中间无断口，硬盘体积也和普通硬盘类似，可以直接接在硬盘背板接口上。这两种物理接口都是直接连接到主板上的，不过 U.2 特殊一些，需要背板和主板的支持，多用于服务器场景，而 M.2 只需要主板带有对应插槽一般就可以了，服务器和 PC 场景都有使用到。\nSATA 接口的固态硬盘的传输速率计算和普通硬盘相同，但实际速率会因为固态硬盘的特性而得到提升，在现有使用 PCIe 总线的固态硬盘中，大部分都使用了 NVMe 传输协议，所以传输速率的计算直接和使用的 PCIe 通道数相关，和 CPU 支持的 PCIe 总线版本相关。如果使用了 PCIe 总线，而采用 SATA 作为传输协议，则无法享受 PCIe 的高带宽了，一般的 PCIe NVMe 的硬盘都是使用 x4 的 PCIe 通道，以 PCIe 3.0 的速率计算，可以达到 4 GB/s 。\n高级格式化磁盘 传统硬盘的扇区大小一般是 512B ，在 2010 年左右，硬盘厂商开始向 4K (4096B) 扇区过渡，扇区的增大可以提高数据的存储密度，增强错误检测能力，这种扇区大小大于 512B 的硬盘一般会带有 Advanced Format 标识，称为高级格式化。\n在硬盘扇区大小的过渡时期，为了实现系统对不同大小扇区的兼容，高级格式化硬盘需要进行一些额外的工作，这部分是硬盘固件负责的，当系统请求 512B 扇区的读写行为时，硬盘会将其转换为对应物理扇区的读写行为，所以有了物理扇区和逻辑扇区的区别。\n物理扇区是硬盘原生支持的扇区大小，由硬盘厂商决定，逻辑扇区是操作系统的最小存储扇区大小。虽然不同操作系统有不同大小的逻辑扇区，但最常见的大小就是 512B ，即便不是 512B ，它们一般都是 512B 的整数倍。\n4K 是最经典的高级格式化硬盘的物理扇区大小，这类 4K 模拟 512B 的硬盘标记为 512e (512 emulation)，如果 4K 硬盘不支持模拟小扇区的读写行为，标记为 4Kn (4K native)。\n# 这是虚机上运行的虚拟系统盘 $ parted -l Model: VMware, VMware Virtual S (scsi) Disk /dev/sda: 10.7GB Sector size (logical/physical): 512B/512B # 虚拟硬盘都是模拟的，模拟的设备一般都是原生的 512B 扇区大小 Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 525MB 524MB primary xfs boot 2 525MB 1599MB 1074MB primary linux-swap(v1) 3 1599MB 10.7GB 9138MB primary xfs # 这是实机上运行的一块硬盘，它就是 512e 的硬盘 $ parted -l Model: ATA ST500LT012-9WS14 (scsi) Disk /dev/sdb: 500GB Sector size (logical/physical): 512B/4096B # 硬盘的物理扇区大小为 4096B ，逻辑扇区大小为 512B ，这是 512e 的 AF 硬盘 Partition Table: msdos Disk Flags: Number Start End Size Type File system 标志 1 1049kB 53.7GB 53.7GB primary ntfs 2 53.7GB 500GB 446GB extended lba 5 53.7GB 165GB 112GB logical ntfs 6 165GB 277GB 112GB logical ntfs 7 277GB 389GB 112GB logical ntfs 8 389GB 500GB 111GB logical ntfs 可以看到，这块希捷的 500GB 机械硬盘的物理扇区大小是 4K ，但是逻辑扇区大小是 512B ，这是在 Linux 系统上比较常见的情况，而虚拟机的物理扇区和逻辑扇区都是 512B ，现代硬盘比较少出现这种情况，尤其是大容量硬盘。\n在 512e 硬盘的读写过程中，固件层做了隐式的扇区大小转换，读行为比较简单，我们需要读取的扇区只是 512B 大小，但固件是以 4K 去读取的，它要做的工作是找到目标数 …","date":1633791600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"17302611b055a326a7ed1dc7485debf1","permalink":"https://yuweizzz.github.io/post/knowledge_about_hdd/","publishdate":"2021-10-09T15:00:00Z","relpermalink":"/post/knowledge_about_hdd/","section":"post","summary":"这篇笔记用来记录一些硬盘的相关知识。\n","tags":["HDD"],"title":"硬盘知识笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些前端项目开发过程中的常用操作。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 搭建 Node.js 环境 Node.js 提供了二进制预编译的安装包，在官网直接下载到本地即可解压使用。\n这里以 LTS 14.15.4 为例:\n$ curl -o /usr/local/src/node-v14.15.4-linux-x64.tar.xz https://nodejs.org/download/release/v14.15.4/node-v14.15.4-linux-x64.tar.xz $ tar -xvJf /usr/local/src/node-v14.15.4-linux-x64.tar.xz -C /usr/local/ $ echo \u0026#39;export PATH=\u0026#34;$PATH\u0026#34;:/usr/local/node-v14.15.4-linux-x64/bin/\u0026#39; \u0026gt;\u0026gt; /etc/profile 如果只有某个用户需要使用 Node.js，可以把 PATH 变量写入到对应用户的 bash_profile 或 bashrc 中，这样可以起到一样的效果并且不会影响其他用户的环境变量。\n修改 npm 镜像地址 npm 是 Node.js 的依赖包管理工具，类似于 python 的 pip ， npm 在国内的网络环境会有下载速度慢的现象，可以将镜像地址修改为国内的镜像地址来解决这个问题。\n# 检查 npm 基本配置： $ npm config list ; cli configs metrics-registry = \u0026#34;https://registry.npmjs.org/\u0026#34; # 默认的源镜像 scope = \u0026#34;\u0026#34; user-agent = \u0026#34;npm/6.14.10 node/v14.15.4 linux x64\u0026#34; ; node bin location = /usr/local/node-v14.15.4-linux-x64/bin/node ; cwd = /usr/local/node-v14.15.4-linux-x64 ; HOME = /root ; \u0026#34;npm config ls -l\u0026#34; to show all defaults. # 修改为淘宝 npm 镜像以加快访问速度： # 只修改当前用户的配置，非全局修改会生成 $HOME/.npmrc $ npm config set registry http://registry.npm.taobao.org/ $ cat ~/.npmrc # 只影响当前用户 registry=http://registry.npm.taobao.org/ # 使用 -g 修改全局配置，全局修改会生成 $PREFIX/etc/npmrc $ npm -g config set registry http://registry.npm.taobao.org/ $ cat /usr/local/node-v14.15.4-linux-x64/etc/npmrc # 生成于 node 安装目录下的 etc 目录中 registry=http://registry.npm.taobao.org/ 初始化项目 下面简单记录了初始化前端项目的过程，可以提供后续参考。\n生成 package.json 是一个项目的开始，无论使用的依赖包管理工具是 npm 或是 Yarn 。我选用了 npm ，实际可以按照个人喜好来选择。\n$ mkdir myproject $ cd myproject $ npm init # 交互式生成 package.json This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults. See `npm help init` for definitive documentation on these fields and exactly what they do. Use `npm install \u0026lt;pkg\u0026gt;` afterwards to install a package and save it as a dependency in the package.json file. Press ^C at any time to quit. package name: (myproject) version: (1.0.0) description: my project entry point: (index.js) src/index.js test command: node -v; npm -v; # 这里是为了测试，实际使用时可以按需修改 git repository: keywords: author: me license: (ISC) About to write to /tmp/myproject/package.json: { \u0026#34;name\u0026#34;: \u0026#34;myproject\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;my project\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;src/index.js\u0026#34;, # 通常会使用 src 来存放项目代码，实际使用时可以按需修改 \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;node -v; npm -v;\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;me\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34; } Is this OK? (yes) yes 生成 package.json 后，我们可以按照需要安装一些项目模块和开发模块工具。\n# 安装项目需要的模块 $ npm install \u0026lt;dependency\u0026gt; # 使用 -D 安装项目需要的开发模块工具 $ npm install -D webpack # 打包工具 $ npm install -D webpack-cli $ npm install -D eslint # 语法检查工具 # 检查配置文件，可以看到新的依赖项已经被安装 $ cat package.json { \u0026#34;name\u0026#34;: \u0026#34;myproject\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;my project\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;node -v; npm -v;\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;me\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;webpack\u0026#34;: \u0026#34;^5.48.0\u0026#34;, \u0026#34;eslint\u0026#34;: \u0026#34;^7.32.0\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^4.7.2\u0026#34; } } # 安装完成后，可以在 node_modules 中找到对应模块目录 # 可直接执行脚本还会产生符号链接提供给项目使用，如下例子： ./node_modules/.bin/webpack-cli -\u0026gt; ../webpack-cli/bin/cli.js 虽然可以通过指明路径调用一些可用的脚本和命令，但我们一般把这些常用的命令写入到 package.json 的 scripts 中去，方便后续使用。\n# package.json 中 scripts 的基本使用： # scripts 通常使用 build ， lint ， run 等常用命名来配置对应的一系列命令 # npm run 可以执行这些配置好的命令 # 测试之前已经配置的 scripts test ： $ npm run test \u0026gt; myproject@1.0.0 test \u0026gt; node -v; npm -v v14.15.4 7.20.3 # 后续使用会配置一些新的 scripts 下面是一些常用的开发模块，只用到了比较简单的场景，更详细的配置需要参考官方文档和其他优秀项目的用法。\n使用 webpack 作为打包工具。\n# 配置 webpack ： # 手工生成 webpack.config.js ，文件位于项目根目录 $ cat webpack.config.js // start const webpack = require(\u0026#39;webpack\u0026#39;); const path = require(\u0026#39;path\u0026#39;); const config = { entry: \u0026#39;./src/index.js\u0026#39;, // 实际使用按需修改 output: { path: path.resolve(__dirname, \u0026#39;dist\u0026#39;), filename: \u0026#39;bundle.js\u0026#39; // output 为打包输出目标 } }; module.exports = config; // end # 生成 webpack.config.js 后配置 webpack 打包命令到 npm scripts $ npm set-script \u0026#34;build\u0026#34; \u0026#34;webpack --mode production --config webpack.config.js\u0026#34; # 添加完成后可以直接调用 npm run build 执行打包 使用 ESLint 作为语法检查工具。\n# 配置 eslint ： # 直接调用可执行脚本进行语法相关的配置初始化 $ ./node_modules/.bin/eslint --init ... # 整个过程按照提示进行选择即可，一般会产生依赖安装，完成后会自动生成文件 Successfully created .eslintrc.json file in /tmp/myproject ... # 配置 eslint 检查命令到 npm scripts $ npm set-script \u0026#34;lint\u0026#34; \u0026#34;eslint src/\u0026#34; # 添加后可以直接调用 npm run lint 执行语法检查 ","date":1628028000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5e1a409e30d2c037a11349eff4c1a8bf","permalink":"https://yuweizzz.github.io/post/tips_about_javascript_project/","publishdate":"2021-08-03T22:00:00Z","relpermalink":"/post/tips_about_javascript_project/","section":"post","summary":"这篇笔记用来记录一些前端项目开发过程中的常用操作。\n","tags":["npm","Node.js"],"title":"前端项目开发笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Linux 下常用命令和一些实用 Shell 技巧。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ Linux 的归档和压缩命令 归档压缩是经常需要进行的操作，在 Linux 系统中基本上通过 tar 就可以应对大部分场景，某些特殊的场景可能会涉及 ZIP ， 7z ， rar 这些不同的压缩文件格式，它们需要使用额外的工具去操作，这里只介绍 tar 命令。\n我们通常会使用 tar 对一系列文件进行归档打包，生成一个新的文件，但是新生成的打包文件我们可以自行决定是否压缩。所以归档打包和压缩不是同一概念，只是它们通常是紧密结合在一起的。\n# 归档实例： # 打包目录 $ tar -cvf output.tar /mydir # -c 代表创建打包文件 # -v 代表显示详细过程 # -f 命名生成的打包文件 # 这一般是打包文件的最小操作命令，将 /mydir 打包为 output.tar # 打包文件并进行压缩 $ tar -cvzpf output.tar.gz /mydir $ tar -cvjpf output.tar.bz2 /mydir $ tar -cvJpf output.tar.xz /mydir # -j 代表使用 bzip2 压缩 # -z 代表使用 gzip 压缩 # -J 代表使用 xz 进行压缩 # -p 代表使用原来的文件权限还原文件 # 这是带了压缩命令的打包文件，以上三种压缩格式都是比较常用的 # 命名可以任意，一般保持通用的命名规则 # 解包文件 $ tar -xvf output.tar -C /mydir # -x 代表解压文件操作 # -f 代表进行操作的文件 # -C 用来指定解压的路径 # 这一般是解压文件的通用命令，可以不指定压缩格式，tar 会自动适配 # 指定的压缩格式解压文件 $ tar -xvJf output.tar.xz -C /mydir # 可以指定对应的压缩格式 硬盘分区和格式化 硬盘需要进行分区和格式化之后才能使用，现阶段主流的硬盘分区方式是 GPT ，小容量的硬盘和系统盘还经常会使用 MS-DOS 。常用的分区工具有 parted ， fdisk ， gdisk 这几个，文件系统格式化一般使用 mkfs 命令。\n# fdisk 只能支持 msdos 分区 # gdisk 在 fdisk 的基础上扩展 gpt 的功能 # 推荐使用 parted ，它拥有更全面的功能 # 查看已有分区信息 $ parted /dev/sda print # 以下命令具有一定危险，需要注意数据安全 # 设置分区表格式 $ parted /dev/sda mklabel [ gpt | msdos ] # 新增分区 $ parted /dev/sda mkpart [ primary | extended | logical ] [ ext4 | xfs ] start end # gpt 不区分分区类型，这里会以 Name 代替， msdos 需要用到 primary ， extended 和 logical 分区 # 文件系统的标记一般可以不写 # 分区范围是必填项 # 删除某一分区， Number 代表分区编号 $ parted /dev/sda rm Number # 格式化分区 $ mkfs.ext4 [-b size] [-L label] /dev/sda1 $ mkfs.xfs [-b size] [-L label] /dev/sda1 # 两种最常用的文件系统，参数都是相近的 # -b 用来设定最小区块大小，有1K，2K，4K # -L 用来设置文件系统标签，用于挂载文件系统 # 速用脚本 $ parted /dev/sda -s mklabel gpt $ parted /dev/sda -s mkpart primary 0% 100% # 这里的 primary 为分区命名，可以自行定义 # -s 用来屏蔽 parted 的交互信息 $ mkfs.xfs /dev/sda1 # 备份和还原 xfs 文件系统目录 $ xfsdump -f ~/home.img /home $ xfsrestore -f ~/home.img /home curl 的基本使用 curl 在 Linux 代替了浏览器的工作，经常用来调试接口。\n# 经典 curl 用例 $ curl -X POST -d \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;http://....\u0026#39; # -X HTTP 请求方法，通常有 GET,POST,PUT,DELETE # -d body ，常用于 POST 方法的请求体 # -H header ，定义 HTTP 请求头 # 这里列举其他常用的参数 # -b 用来设置 cookie ，常以 -b \u0026#39;key1=value1;key2=value2\u0026#39; 的形式出现 # -k 用来请求 https 协议时跳过证书认证 # -i 用来额外输出请求的响应头，响应体正常输出 # -I 使用这个参数后只输出请求的响应头，响应体不再输出 # -x 用来设置代理服务器，用法为 -x/--proxy 127.0.0.1:8080 # -u 用来设置 HTTP Basic Authentication ，用法为 -u \u0026#39;username:password\u0026#39; # 将文件内容作为请求体 $ cat data.json { \u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;, } $ curl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; --data-binary @data.json \u0026#39;http://....\u0026#39; # 将文件内容转换为标准输入流之后再作为请求体，这样可以多做一次处理 $ cat data.json | curl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; --data-binary @- \u0026#39;http://....\u0026#39; curl 结合 bash 变量使用 curl 可以结合 bash 变量，实现更灵活的 URL 请求。\n# 结合 bash 变量的简单实例 $ keyA=AAA $ keyB=BBB $ curl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;keyA\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$keyA\u0026#34;\u0026#39;\u0026#34;,\u0026#34;keyA\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$keyB\u0026#34;\u0026#39;\u0026#34;}\u0026#39; http://.... 可以看到，这个过程使用了大量的 \u0026#39; 和 \u0026#34; ，可以将其细分为五块：\n\u0026#39;{\u0026#34;keyA\u0026#34;: \u0026#34;\u0026#39; \u0026#34;$keyA\u0026#34; \u0026#39;\u0026#34;,\u0026#34;keyA\u0026#34;: \u0026#34;\u0026#39; \u0026#34;$keyB\u0026#34; \u0026#39;\u0026#34;}\u0026#39; 其中 \u0026#39; 包围的文本不会被 bash 转义，所以 {} 会以源文本的格式保留而不被转义。而变量总是以 \u0026#34;$keyA\u0026#34; 的形式出现，保证它被 bash 正确转义并获取变量内容。所以文本块就是 \u0026#39; 包围的部分和变量转义后的组合文本，注意 \u0026#34;$keyA\u0026#34; 和任意 \u0026#39; 包围的部分之间不能有空格，这样它们就会被认为是完整的 -d 选项的内容，发起请求时就不会报错了。\n使用 sed 快速去除无用字符 sed 可以很简单快速去除空白行，首尾空白字符，注释行。\n# 删除空白行 $ sed \u0026#39;/^$/d\u0026#39; # 删除行左边的空白字符 $ sed \u0026#39;s/^[ \\t]*//g\u0026#39; # 删除行右边的空白字符 $ sed \u0026#39;s/[ \\t]*$//g\u0026#39; # 删除注释行 $ sed \u0026#39;/^#/d\u0026#39; # 可能要配合删除左侧空白字符使用 # 实现上述操作基于正则表达式： # $ 代表行尾 # ^ 代表行首 # [] 整体代表一个字符，[] 里面是字符范围 # 在上面的操作中，[ \\t] 用来匹配 \u0026#39; \u0026#39; 或 \u0026#39;\\t\u0026#39; 任意一种 # * 用来表示零个或多个字符，可以结合确定的单个字符或者字符范围使用 # 可以用来代表字符的特殊符号有： # . 用来表示任意单一字符 # ? 用来代表零个或一个字符 # + 用来代表一个或多个字符 快速生成随机强密码 # 随机生成含有特殊字符，数字，大小写字母的强密码 # head -c 可以设置密码长度 $ cat /dev/urandom | tr -dc \u0026#39;[:graph:]\u0026#39; | head -c 24; echo # 字符集由 tr -dc 决定，可以按需指定 # 只需数字 $ cat /dev/urandom | tr -dc \u0026#39;0-9\u0026#39; | head -c 24; echo # 只需大小写字母 $ cat /dev/urandom | tr -dc \u0026#39;a-zA-Z\u0026#39; | head -c 24; echo 快速转换进制 # 将数值快速转换成目标进制表示 # 十进制转十六进制 $ printf \u0026#34;%x\u0026#34; 12345 # 十进制转八进制 $ printf \u0026#34;%o\u0026#34; 12345 # 十进制转科学计数 $ printf \u0026#34;%e\u0026#34; 12345 在 shell 中使用数组 bash 4 原生支持一维数组，在某些情况下可能会使用到这种数据结构。\n#!/bin/bash # 使用之前需要先声明数组 declare -a array declare -A Array # -a 声明的数组是普通的一维数组， index 只能是 0,1,2,... # -A 声明的数组是关联数组，类似于 python 的字典， index 可以自由定义 # 推荐使用 -A ，因为关联数组的兼容性比较好，它可以模拟普通数组， # 而普通数组无法实现关联数组的特性 # 数组赋值 array=(1 2 3 4) Array=([A]=1 [B]=2 [C]=3 [D]=4) # 或者直接定义 index 和 value Array[\u0026#34;index\u0026#34;]=vaule # 遍历 index # 使用 * 和 @ 均可 for i in ${!array[*]}; do echo $i; done; for i in ${!array[@]}; do echo $i; done; # 遍历 value ，和 index 类似 for i in ${array[*]}; do echo $i; done; for i in ${array[@]}; do echo $i; done; # 获取数组的元素个数 echo ${#array[*]}; echo ${#array[@]}; # 如果指定了确切的 index ，将会获取对应 value 的长度 echo ${#array[\u0026#34;index\u0026#34;]}; bash  …","date":1627052400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"8299e445d28a75a73e5848edf6da9297","permalink":"https://yuweizzz.github.io/post/practical_tips_in_linux/","publishdate":"2021-07-23T15:00:00Z","relpermalink":"/post/practical_tips_in_linux/","section":"post","summary":"这篇笔记用来记录 Linux 下常用命令和一些实用 Shell 技巧。\n","tags":["Linux","Shell"],"title":"常用 Linux 命令和 Shell 技巧笔记","type":"post"},{"authors":null,"categories":null,"content":"在 Hugo 静态网站添加 Live2D 看板娘。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 在没有建设自己的站点之前，见到别人站点上的 Live2D 看板娘觉得很有趣，所以在搭建了自己的站点后，也想要在自己的站点实现。\nHugo 站点自定义 实现网页添加 Live2D 看板娘需要先对站点进行适配改造。\nJekyll ， Hexo 和 Hugo 这一类静态网站生成器的工作方式都是类似的，它们大都是以 Markdown 语法编写网页的主体，通过不同的模板组合来渲染静态页面，这种工作方式使得自定义非常方便，一般只需要修改或者添加模板就可以达到自定义的效果了。\n自定义过程中为了避免修改过多的模板，把需要进行的工作全部放在 JavaScript 中是个不错的选择。本次自定义也遵循这样的做法，基本上只需要额外引用 JavaScript 文件就可以了，在 Hugo 中，负责加载 JavaScript 的部分一般存放在 Partial Templates 中。\n以下片段出自 Hugo 的官方文档：\nPartial templates—like single page templates and list page templates—have a specific lookup order. However, partials are simpler in that Hugo will only check in two places: 1.layouts/partials/*\u0026lt;PARTIALNAME\u0026gt;.html 2.themes/\u0026lt;THEME\u0026gt;/layouts/partials/*\u0026lt;PARTIALNAME\u0026gt;.html This allows a theme’s end user to copy a partial’s contents into a file of the same name for further customization. 根据上述文档，我们可以复制并修改主题中的 Partial Templates 来达到自定义的效果，并且可以不改动主题原有的模板文件。\n以下是具体的操作步骤：\n# 修改 config.toml # 使用变量控制自定义部分 $ vi config.toml ..... # 新增变量 [params] custom_js = \u0026#34;js/custom_live2d.js\u0026#34; ..... # 复制原有主题文件到高查找等级的目录中 $ cp themes/\u0026lt;THEME\u0026gt;/layouts/partials/script.html layouts/partials/script.html # 修改 layouts/partials/script.html $ vi layouts/partials/script.html ..... # 直接将下列部分添加到script.html中 {{ if .Site.Params.custom_js -}} \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{ $.Site.Params.custom_js | absURL }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{- end }} ..... # 将完成之后的 Javascript 文件放到 static/js 中即可 # 后续生成站点就会以新模板工作 关于 Partial Templates 修改的部分，可以选择直接写入 JavaScript 引用标签，这里使用带变量控制的语法块是为了后续的维护方便。模板语法和 python 的 jinja2 非常相似，有过使用经验应该很容易理解。\n到这里 Hugo 需要改动的地方就完成了，接下来需要做的是将 Live2D 模型资源和对应的 JavaScript 文件正确引用。\nLive2D 资源的应用 在网络上已经有很多开箱即用的相关应用了，但是它们不契合我的喜好，于是我在已有资源的基础上修改，改进为我喜欢的效果。\n首先需要了解的是现有的模型资源分类，现有的 Live2D 模型资源有 Cubism 2 ， Cubism 4 和 Cubism 3 三个版本，其中 3 和 4 之间是互相兼容，所以基本上可以认为只有两个版本。 2 和 3 之间还是有很大区别的，在个人项目中我只使用到了 Cubism 3 。\n# Cubism 3 Model 基本结构： . ├── Name.moc3 ├── Name.model3.json # 最重要的入口文件 ├── Name.physics3.json ├── motions │ ├── Action1.motion3.json │ ├── Action2.motion3.json │ └── Action3.motion3.json └── textures ├── texture_00.png ├── texture_01.png └── texture_02.png 模型文件大都是 json 格式，其中 model3.json 是最为重要的入口文件，很多模型的行为都是由它定义的，后续也需要修改这个文件来实现一些自定义需求。\n现在我们拥有了模型，如果要在网页上实现模型的应用，还需要对应模型版本的 framework 的支持。在这一块我们只能使用 Live2D 出品公司原有的框架，或者使用由大神整理过的资源，我直接使用了 pixi-live2d-display 插件，在这个插件中， pixijs 是整个项目的重要部分，它是一个强大的前端 2D 动画渲染引擎，和 Cubism framework 很好地结合在一起，所以我们不必去了解底层的 API ，可以直接使用这个插件为我们提供的 API ，这也是我选择这个插件的原因。\n在我的个人项目中，主要使用了原生 pixijs 的 API 以及 pixi-live2d-display 中提供的 API ，通过数据绑定和原生 API 来构建主体功能，并且对模型资源进行了部分修改，使得运行时的模型支持交互动作，主要是在 model3.json 文件中修改 motion 相关的部分。有兴趣可以参考源码。\n最后一步是将整个项目通过 webpack 打包为单个 JavaScript 文件，并将打包后的 JavaScript 文件和 Live2D 模型资源安置到 Hugo 的 static 目录中就大功告成了。\n结语 总结来说，自定义站点这部分比较简单，在 Live2D 应用花费了比较多的时间，因为我是个前端新手，对 JavaScript 的应用还只停留于通过简单的 DOM 操作和基础函数调用。但是经过这个项目，让我感受到了 JavaScript 的迅速发展，它对于有后端经验的人还是比较友好的。\n在实际的使用过程，网络是重要的因素，如果经常会出现模型加载异常，可以考虑使用 CDN 资源。\n","date":1625519325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5fb52e7f86b7febd6535bb1290ef2dd0","permalink":"https://yuweizzz.github.io/post/add_live2d_on_hugo_website/","publishdate":"2021-07-05T21:08:45Z","relpermalink":"/post/add_live2d_on_hugo_website/","section":"post","summary":"在 Hugo 静态网站添加 Live2D 看板娘。\n","tags":["Hugo","Live2D"],"title":"在网页上添加 Live2D 看板娘","type":"post"},{"authors":null,"categories":null,"content":"将 MBR+BIOS 启动的硬盘转换为 GPT+UEFI 启动。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 现代设备中，采用 UEFI 启动的机器越来越多了，虽然在服务器中可以通过 CSM 来提供 Legacy BIOS 的支持，但是 UEFI 将会是未来使用的主流方式，所以有必要开始尝试使用 UEFI 启动。\n将 MBR 转换为 GPT 和 UEFI 启动模式紧密配合的分区方式是 GPT ，原有的 MBR 分区方式存在一定的局限性。而 GPT 为了保持兼容，使用了 LBA 0 扇区作为 MBR 保护扇区，这也使得 MBR 分区方式迁移到 GPT 分区方式成为可能。\n将 MBR+BIOS 启动的硬盘转换为 GPT+UEFI 启动的第一步工作就是将 MBR 转换为 GPT 。\n进行分区操作时，一定要小心谨慎，做好数据备份，对根分区的错误操作可能是无法挽救的。\n# 首先观察根分区情况，这时可以使用 fdisk 或 parted # 这个硬盘来自我的虚拟机 $ fdisk /dev/sda ... Command (m for help): p Disk /dev/sda: 10.7 GB, 10737418240 bytes, 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x0004dcb6 Device Boot Start End Blocks Id System /dev/sda1 * 2048 1026047 512000 83 Linux /dev/sda2 1026048 3123199 1048576 83 Linux /dev/sda3 3123200 20971519 8924160 83 Linux $ parted -l Model: VMware, VMware Virtual S (scsi) Disk /dev/sda: 10.7GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 525MB 524MB primary xfs boot 2 525MB 1599MB 1074MB primary linux-swap(v1) 3 1599MB 10.7GB 9138MB primary xfs 如果你的系统盘如上面展示的一样，最后一个分区用尽了所有剩余的硬盘扇区，这种情况下不建议进行分区方式的转换，因为根据 GPT 分区格式的定义， GPT 比 MBR 使用到了更多的硬盘起始扇区存放分区信息 (Primary GPT) ，而且还占用了硬盘末尾的部分扇区作为分区信息的备份 (Secondary GPT) 。\n虽然很多分区工具在操作 MS-DOS 硬盘时会保留一些起始扇区，默认从 1 MB 开始分区，以便后续兼容 GPT 格式，但一般不会对末尾扇区做保留。所以在转换 GPT 时，处于硬盘头部的分区通常可以无损转换，而处于硬盘尾部的分区会遭遇文件系统受损。\n# 尝试对这个无法无损转换的硬盘进行操作 $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing \u0026#39;q\u0026#39; if you don\u0026#39;t want to convert your MBR partitions to GPT format! *************************************************************** Warning! Secondary partition table overlaps the last partition by 33 blocks! You will need to delete this partition or resize it in another utility. # 这里已经给出分区告警：检测到无效的 GPT 和有效的 MBR ， # 分区工具将分区表自动转换为 GPT 格式，并且最后一个分区需要重新划分， # 在退出分区工具时可以按 q 来取消这次转换 # 为了文件系统安全，记得通过按下 q 来退出。 虽然在我的虚拟机硬盘上无法直接实现转换，但是如果你的系统盘没有用尽所有的空间，或者最后一个分区用作交换分区时，分区方式就可以实现完美转换。\n# 对可以无损转换的硬盘进行操作，原有分区信息大概如下 # 这是一块 msdos 分区的硬盘，未用尽所有扇区 # 这里为了实验删除了 swap 分区，实际上这一步可以省去，直接建立新分区和直接转换是没问题的 ... Number Start (sector) End (sector) Size Code Name 1 2048 4196351 2.0 GiB 8300 Linux filesystem 2 4196352 903086079 428.6 GiB 8300 Linux filesystem 3 904110080 905134079 500.0 MiB 8200 Linux swap ... # 直接通过 gdisk 进行操作 $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing \u0026#39;q\u0026#39; if you don\u0026#39;t want to convert your MBR partitions to GPT format! *************************************************************** Command (? for help): d Partition number (1-3): 3 Command (? for help): n Partition number (3-128, default 3): 3 First sector (34-976773168, default = 904110080) or {+-}size{KMGTP}: 34 Last sector (34-2047, default = 2047) or {+-}size{KMGTP}: Current type is \u0026#39;Linux filesystem\u0026#39; Hex code or GUID (L to show codes, Enter = 8300): ef02 Changed type of partition to \u0026#39;BIOS boot partition\u0026#39; Command (? for help): n Partition number (4-128, default 4): 4 First sector (904110080-976773168, default = 904110080) or {+-}size{KMGTP}: Last sector (904110080-976773168, default = 976773168) or {+-}size{KMGTP}: +500M Current type is \u0026#39;Linux filesystem\u0026#39; Hex code or GUID (L to show codes, Enter = 8300): ef00 Changed type of partition to \u0026#39;EFI System\u0026#39; Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): y OK; writing new GUID partition table (GPT) to /dev/sda. Warning: The kernel is still using the old partition table. The new table will be used at the next reboot. The operation has completed successfully. # 执行 partprobe 可以得到新的分区信息如下： ... Number Start (sector) End (sector) Size Code Name 1 2048 4196351 2.0 GiB 8300 Linux filesystem 2 4196352 903086079 428.6 GiB 8300 Linux filesystem 3 34 2047 1007.0 KiB EF02 BIOS boot partition 4 904110080 905134079 …","date":1625324400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"ac59b2f4c5d34bacae9277150a5635e8","permalink":"https://yuweizzz.github.io/post/convert_legacy_bios_to_uefi/","publishdate":"2021-07-03T15:00:00Z","relpermalink":"/post/convert_legacy_bios_to_uefi/","section":"post","summary":"将 MBR+BIOS 启动的硬盘转换为 GPT+UEFI 启动。\n","tags":["Linux","UEFI"],"title":"由 BIOS 传统启动向 UEFI 启动迁移","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Nginx 的编译安装和基本使用。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 安装 Nginx Nginx 是应用广泛的服务器软件，在 Nginx 的官方文档中可以看到， Nginx 可以应用在多种场景，包括基本 HTTP 服务器，代理服务器，邮件代理服务器。由于支持的特性非常多，所以安装的时候需要考虑的地方也相应增多了。\n我们可以选择编译安装或者提供官网提供给对应发行版的软件包进行安装，使用软件包安装基本上只要解决软件源的问题就可以了，使用编译安装虽然比较麻烦，但是更适用于需要定制的软件特性的场景。\n需要的源码包或者软件包的安装方式都可以在 Nginx 的官网找到，在这里只介绍编译安装。\nNginx 源码包结构 在下载源码包并进行解压后，可以看到几个关键的文件和目录：\nconfigure : 配置编译环境的脚本文件，执行这个脚本后会生成 Makefile 文件。 src : nginx 源代码目录。 auto : configure 脚本需要调用 auto 中的各个脚本文件完成配置工作。 Nginx 源码包使用 autoconf 生成 configure 脚本，这种源码包的使用方式一般都是通过 configure 文件配置编译选项并生成 Makefile ，再通过 Makefile 进行编译和安装。\n# 由 autoconf 生成配置脚本的源码包编译时的一般步骤 $ ./configure $ make $ make install 在 configure 执行时，我们可以选择是否编译某些功能特性，使用了 autoconf 的源码包都有这一特点。\n在 Nginx 的编译安装时，我们可以选择编译某个模块来开启某个功能特性。其中， HTTP 服务模块是默认支持的，而邮件服务模块和传输层服务模块并不会默认支持，需要在编译时额外指定。\n常用模块 Nginx 默认作为 HTTP 服务器工作，所以 HTTP 模块是 Nginx 的核心，但是这个模块可以强制禁用，方法是在 configure 执行时带上 --without-http 参数。\nHTTP 服务模块包括以下几个常用模块，它们都是默认编译的：\nngx_http_autoindex_module : 常用于下载服务器，启用这个模块可以自动建立下载目录页面。 ngx_http_fastcgi_module/ngx_http_uwsgi_module/ngx_http_scgi_module : 对各类通用网关接口协议的支持模块，启用这些模块后可以对接其他程序的接口。 ngx_http_gzip_module : 用于压缩响应结果，可以减小传输数据的体积。 ngx_http_rewrite_module : 用于 URL 重写和返回重定向。 ngx_http_proxy_module : 用于设置代理。 ngx_http_upstream_module : 用于设置负载均衡。 还有一个特殊的 HTTP 模块 ngx_http_ssl_module ，随着 https 的流行，它已经是不可或缺的模块，尤其是在生产环境中，但是它不是默认编译选项，在安装时需要额外指定。\n更多的编译信息可以参照 Nginx 官方网站。\n编译安装 下面给出最小的编译参考例子。\n# rewrite 模块使用 prce 正则表达式，依赖 pcre 库 $ yum install pcre-devel pcre # gzip 模块依赖 zlib 库 $ yum install zlib-devel zlib # ssl 模块依赖 openssl 库 $ yum install openssl-devel openssl # 添加 nginx 用户组 $ groupadd nginx # 添加 nginx 用户 $ useradd nginx -g nginx -s /sbin/nologin -M $ ./configure --prefix=/opt -user=nginx -group=nginx --with-http_ssl_module # 不指定 prefix 则默认安装在 /usr/local 的 nginx 中 # 执行编译并安装 $ make \u0026amp;\u0026amp; make install # 直接启动服务 $ /opt/nginx/sbin/nginx 使用 Nginx Nginx 通过 nginx.conf 文件来配置各项服务。\n基本配置实例 以下的例子是 Nginx 安装时提供的配置。\n$ cat nginx.conf worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } Nginx 是通过 master process 和 worker process 协同工作的，其中 master process 承担管控工作， worker process 承担具体请求处理工作。一般会设置多个 worker process 提高工作效率，数量通常是机器的硬件 CPU 核数。\n在 conf 文件中的 worker_processes 是需要启动的 worker process 的数量， worker_connections 则是单个 worker process 负责承载的连接数量。\n在 conf 文件中的 http 域用来配置具体的 HTTP 服务，它的下一级主要是通过 server 来划分不同的作用域。 http 域可以设置 HTTP 一些共有的服务配置，比如列出的 default_type 和 include ，它们主要定义 HTTP 服务的支持传输的类型相关内容， keepalive_timeout 设置长连接的超时时间。\n单个 Nginx http 域可以配置多个 server ，这些 server 甚至可以使用相同的 listen 端口，但需要至少保持不同 server_name 。 server_name 用来配置域名， loaction 用来匹配 URL ，它们共同组合定位资源。\nerror_page 用来导向 http 响应错误的返回页面，如果它放在 server 域中，则所有的 location 都会受到它的影响。\n除了 HTTP 服务， Nginx 还支持邮件服务和传输层服务，可以在 conf 中配置 mail 域和 stream 域来定义相关服务细节，但这些服务需要额外编译对应的模块。\nloaction 实现资源匹配 location 通过 URL 来匹配目标资源。\n具体的匹配方式有 prefix string 和 regular expression ，也就是前缀匹配和正则匹配，还有比较特殊的完整匹配，可以参考 location 的语法规则。\nSyntax: location [ = | ~ | ~* | ^~ ] url { ... }\n各个匹配符号的含义如下：\n= 用来代表完整匹配，只有 URL 完全一致才会被这个 location 所匹配，这是最高优先级的规则。 ^~ 表示从头开始匹配，开头的符号和普通正则的符号一致，这是前缀匹配的一种。 不带任何修饰符也是一种前缀匹配。 ~ 和 ~* 代表 location 采用正则匹配，带 * 不区分大小写，不带 * 则需要匹配大小写。 一个请求到达 Nginx 时的具体匹配过程由 server 中所有的 location 共同参与，一般的匹配规则如下：\n拿到请求 URL ，进行解码，去除重复的 / 符号等规范化处理。 对 URL 进行精准匹配，如果匹配成功，立即返回结果并结束匹配过程。 进行前缀匹配，如果成功匹配到带 ^~ 修饰符的最长匹配，立即返回结果并结束匹配过程。 在第二步没有直接结束匹配的情况下，继续寻找最长匹配的前缀匹配，并临时存储这个匹配。 由上至下逐一进行正则匹配。 如果正则匹配成功，立即返回结果并结束匹配过程。 如果正则匹配没有结果，则将存储的最长匹配作为最终结果返回，并结束匹配过程。 整个匹配过程主要复杂的地方在于前缀匹配，在第三个步骤时，带修饰符和不带任何修饰符两者没有优先之分，都会倾向寻找最长的匹配，但如果这个最长匹配是由 ^~ 修饰的，则直接返回这个匹配。不带修饰符的写法和带 ^~ 的写法的目标不能完全相同， Nginx 的配置检查将会无法通过，可以把一些更为详细的前缀匹配使用 ^~ 修饰，可以加快解析速度。\n在匹配进入到某个 location 中后，匹配资源的工作就由关键字 alias 和 root 来定义，它们用来指向资源在服务器中的位置，一直以来，网上对 alias 和 root 的说法有很多，这里给出一些我的测试过的结论以供参考：\nroot 和 index 已经被隐式定义在整个 server 域中，默认位置和前面给出的默认配置实例一致，如果不进行覆写，隐式地址为 Nginx 安装后自带的 HTML 目录， index 默认为目录下的 index.html 。 这个结果可以通过去除所有 location 和 server 的关键字定义，并在 html 下随意写入一个 index.html 文件得出。\n在不配置任何错误定向页面时， Nginx 会使用默认的 404 和 50x 的页面。 如果你在 location 中重定义 root ，则这个 root 只能在当前 location 域中生效。这种配置方式除了要匹配 location ，还需要 URL 在配置的 root 中准确命中资源才不会返回 404 错误。 使用这类配置的常见情况是 url 匹配了 location ，但是无法找到正确的资源路径，比如 location /cn { root country/cn; } 这种写法的寻找目录实际为 country/cn/cn/ ，这是我个人在一开始使用 Nginx 时经常犯的错误，如果要在 location 中使用 root ，最好在 root 这个目录下拥有和 location 匹配条件一致的同名子目录。 …","date":1622386800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"8d221a46635d5f6fbbb8d4ed140587e9","permalink":"https://yuweizzz.github.io/post/knowledge_about_nginx/","publishdate":"2021-05-30T15:00:00Z","relpermalink":"/post/knowledge_about_nginx/","section":"post","summary":"这篇笔记用来记录 Nginx 的编译安装和基本使用。\n","tags":["Nginx"],"title":"Nginx 使用笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记主要介绍如何为 CentOS 安装图形界面。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 图形界面的运行原理 Linux 各类发行版图形界面的理论支持就是大名鼎鼎的 X Window System ，它是基于 Server/Clinet 架构的一套复杂软件，主要的工作组件是 X Server 和 X Client 。\n# X Window System 架构: +----------+ +----------+ +----------+ | Hardware |\u0026lt;------\u0026gt;| X Server |\u0026lt;------\u0026gt;| X Client | +----------+ +----------+ +----------+ X Window System 的两个主要组件的工作内容如下：\nX Server：和硬件层面对接，从输入设备(键盘，鼠标)获取输入数据并告知 Client ，在输出设备(显示器)上绘制从 Client 获取的绘图数据。 X Client：从 Server 获取到硬件的输入数据，处理它们得出对应的绘图数据返回给 Server 。 X Server 是相对固定的一套软件，和硬件交互的主要是各类硬件驱动的使用和管理。\n而 X Client 就十分地自由，它可以是浏览器，办公软件，播放器等等。一个 X Client 可以认为是图形界面中的一个软件窗口。\n我们经常听到的 KDE ， GNOME ， Xfce 是特殊的 X Client 套件，通常认为是窗口管理器，因为它们可以统筹管理同一界面下的多个 X Client 。\n所以如果要为一台机器安装带图形界面的 Linux 系统，我们会需要安装 X Window System 组件以提供 X Server ，然后安装 KDE 这一类特殊的 X Client 套件，组成一套可用的图形界面。\n传统 Server/Clinet 架构和 X Window System 架构的区别 传统 Server/Clinet 架构 在传统的 Server/Clinet 架构中，最经典应用应该是 HTTP 服务。\n在 HTTP 服务中， Server 一般是运行在远端的 HTTP 服务器，比如 nginx 或者 Apache 。我们在 PC 端向 Server 发起请求， Server 在接受请求后响应并返回数据到 PC 端。\n从使用者的视角来看， Client 很好理解，无疑就是自己使用的 PC 设备，而 Server 就是远端提供服务的设备。\nX Window System 架构 当一台 Linux 系统的机器运行图形界面时，它的 X Server 和 X Client 都会运行在自己的本地机器上，这种应用情况在传统的 Server/Clinet 架构中比较少。\n此外还有一种情况就是和 HTTP 服务类似， X Server 和 X Client 不在同一台机器上运行，而是通过网络通信，但这种 X Window System 应用情况会比较少。\n# 运行在网络上的 X Window System 架构: +----------+ +----------+ | Input | | Input | +----------+ +----------+ | | | +---------------------+ | v | remote workstation | v +----------+ Network +----------+----------+ Network +----------+ | X Server |\u0026lt;---------\u0026gt;| X Client | X Client |\u0026lt;---------\u0026gt;| X Server | +----------+ +----------+----------+ +----------+ | | ........... | | | +---------------------+ | v v +----------+ +----------+ | Output | | Output | +----------+ +----------+ 在 X Window System 架构运行在网络上时， X Server 和 X Client 会通过网络传输数据。虽然 X Server 的服务对象依旧是 X Client ，但这时它们是分开在网络的两端的，站在使用者的角度上，远端不再是常见的 Server ，反而是各类 X Client ，它们会去完成软件的数据计算工作。而我们本地设备则成为了 X Server ，管理着本地硬件并且承担绘制图形的工作。\n这就是 X Window System 比较特殊的地方，但它本质上还是遵循着 Server 为 Client 提供服务的原则。\n使用 CentOS 图形界面 安装本地图形界面 在市面上的不同 Linux 发行版中，在个人用户中比较受欢迎的是 Ubuntu ，但是我个人习惯使用的 Linux 发行版是 CentOS ，它更多地用于服务器的场景，而且一般不会安装图形界面，所以需要额外去探究一下。\n对于发行版的选择，我认为贴近自己的工作场景是比较重要的参考。 CentOS 是稳定的服务器系统，而 Ubuntu 使用的内核比较新，图形界面的支持比较丰富。此外还可以 Debian 或者 Fedora 这些发行版可供选择。\n我个人推荐是 CentOS 或 Ubuntu 中任意一款，因为这些发行版的用户相对较多，各类支持和解决方案也相应地比其他发行版来的多。\n如前面所提到的，我们需要安装 Server 和 Client 两个组件。\nCentOS 如果没有安装过图形界面，需要先安装 X Window System ，它提供了 X Server 的功能。\n还需要安装 X Client ，可以选择只安装个人需要的 X Clinet，但一般会都会安装主流的桌面套件提供完整的支持，我的个人选择的桌面套件是 xfce 。\n# 前置的软件安装工作： $ yum group install \u0026#39;x window system\u0026#39; -y $ yum group install xfce -y # 这里可选择其他套件：GNOME Desktop or KDE Plasma Workspaces # 安装完成后，切换到图形界面的两种方式： $ init 5 $ systemctl isolate graphical.target # 安装完成后，可以设置默认启动环境为图形界面 $ systemctl get-default # 默认应该为多用户界面 multi-user.target $ systemctl set-default graphical.target $ reboot # 修改设置后重启生效 运行时分析 如果安装没有报错，我们在执行切换命令后就会自动切换到图形界面。\n在某些版本中，可能会出现无法立即切换图形的情况，解决办法一般是先切换为原来的运行等级，在终端界面会产生文本模式的会话，选择接受 licence 后再尝试切换。\n为了更好地了解运行的原理，我们可以抓取进程信息进行分析，看看服务是如何运行的。\n# 切换图形界面后的进程信息： $ ps aux | grep X root 1002 0.0 0.2 225840 4812 ? Ss 16:55 0:00 /usr/bin/abrt-watch-log -F Backtrace /var/log/Xorg.0.log -- /usr/bin/abrt-dump-xorg -xD root 1597 0.5 2.4 340104 49876 tty1 Ssl+ 16:56 0:01 /usr/bin/X :0 -background none -noreset -audit 4 -verbose -auth /run/gdm/auth-for-gdm-VsIE6S/database -seat seat0 -nolisten tcp vt1 # X server 的进程详情 root 2060 0.0 0.0 112812 948 pts/1 S+ 17:00 0:00 grep --color=auto X 如果我们已经启动了图形界面，那么这台机器上必然已有一个 X 进程，这时我们需要关注一个环境变量 DISPLAY ，它为所有的 X Client 提供 X Server 的通信地址。\n这里有个地方比较有意思，如果在图形界面下的终端打印这个变量，可以看到格式为 hostname:displaynumber.screennumber 的值，但是如果你切换到命令行界面并打印这个变量，会发现结果为空。那是因为这个变量在命令行界面是无意义的，而它对图形界面运行的 Client 却是不能缺少的，如果没有它， Server 和 Client 之间就无法通信，所以新的 Client 都会继承这个环境变量。\n关于 DISPLAY 这个环境变量，我们可以在 X Server 的文档中查到相关的信息：\nhostname 如果是为空，那么它默认会使用本地 X Server 中的最高效通信方式 Unix Socket ，实际使用中这个值经常留空。 displaynumber 用来界定显示器和输入设备的集合，用于区分不同的显示界面，是不能为空的。 screennumber 用来界定某个集合中不同的显示器，比如多显示器使用主副屏的情况。考虑到现实情况中，用户通常只有一个显示器， screennumber 留空则默认 0 。 所以可以看到实际中的 DISPLAY 一般是类似于 :0 ， :1 ， :10 这样的值，系统默认启动的 X 进程 DISPLAY 就是 :0 ，符合前面所描述的使用习惯。此外 displaynumber 一般还会作为这个 X Server 监听的端口号偏移量。\nX Server 的默认监听端口是 6000 ，如果是指定了 DISPLAY 的 X Server ，那么除了端口已被占用的情况，一般会在 6000 的 displaynumber 偏移端口上进行监听，当指定 DISPLAY 为 :1 会监听在 6001 端口，指定 DISPLAY 为 :10 会监听在 6010 端口，这个部分可以通过 netstat 查看系统的监听情况。\n再说回系统默认启动的 X 进程，我们可以在这个进程参数中看到 DISPLAY 的值为 :0 。如果依前面所述，此时应该可以查询到监听在 6000 端口的 X 进程。但是这时我们通过 netstat 检查端口，会发现这个端口并不会被占用。出现这种现象的原因是系统默认启 …","date":1617138000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"f8c45226fe2aaec617d0e93728b60f2c","permalink":"https://yuweizzz.github.io/post/install_gui_in_linux/","publishdate":"2021-03-30T21:00:00Z","relpermalink":"/post/install_gui_in_linux/","section":"post","summary":"这篇笔记主要介绍如何为 CentOS 安装图形界面。\n","tags":["Linux"],"title":"安装 Linux 图形界面","type":"post"},{"authors":null,"categories":null,"content":"通过 GitHub Actions 实现持续集成和持续部署。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 使用 GitHub Actions 之前，我们需要了解持续集成和持续部署的概念。\n持续集成 (Continuous integration, CI) 和持续部署 (Continuous deployment, CD) 是随着 DevOps 的兴起而出现的开发工作流程，使用 CI 和 CD 可以省去重复工作，提高工作效率。\nCI 指的是频繁地把代码更新提交到主干中，为了实现这一过程，代码的提交需要通过测试才行，而很多时候代码的构建和测试是重复的步骤，所以我们把构建环节和测试环节自动化，省去每次手动操作的麻烦。\nCD 建立在 CI 的基础上，将成功集成的主干代码部署到生产环境中，这个过程同样有一些重复性工作，所以这一环节也可以自动化实现。\n快速使用 GitHub Actions GitHub Actions 是 GitHub 提供的一项服务，可以通过自定义 workflow 实现代码仓库的 CI 和 CD 工作流程，使用起来快速简单，完成满足个人项目的使用。\n使用 GitHub Actions 大概有以下几个步骤：\n首先在 GitHub 上建立一个 repository 。 进入你的 repository ，点击 Actions ，直接点击 Setup up this workflow ， workflow 可以选择创建一个新的 workflow ，也可以选择使用 GitHub 社区提供的各种编程环境的 workflow 模板。 将 workflow 文件 commit 到分支中。 提交了 workflow 文件后， GitHub Actions 会根据定义的 workflow 执行相关操作，可以在 Actions 中查看 workflow 的运行结果和运行日志。\n编写 workflow 文件 除了 GitHub 社区提供的各种编程环境的 workflow ，我们还可以自行编写 workflow 文件。\nworkflow 文件是使用 YAML 编写的配置文件，它会涉及以下列出的名词：\nworkflow : 一个完整的 CI 和 CD 工作流程认为是一个 workflow ，每个 workflow 以后缀命名为 .yml 的文件保存在代码仓库的 .github/workflows 目录中。一个库允许拥有多个 workflow 。\njobs : 一个 workflow 由一个或多个 job 构成，job 是可以自由命名的，比较常见的命名有 lint ， test ， build ， deploy 等。 job 默认是并行运行的，可以使用 needs 来规定依赖关系以实现顺序运行。\nstep : 一个 job 由一个或多个 step 构成， step 是按照顺序关系执行的。\naction : 一个 step 由一个或多个 action 构成， action 也是按照顺序关系执行的。 action 规定了细节工作，基本上由 shell 命令构成，可以自行编写 action 或使用 GitHub 社区提供的 action 。\nYAML 语法比较简单，这里会跳过相关介绍，直接通过实例来学习 workflow 编写。\npython 自动测试实例 以下是 GitHub 社区提供的基于 python 环境的 CI 工作流程。\nname: Python package on: push: branches: [master] pull_request: branches: [master] jobs: build: runs-on: ubuntu-latest strategy: matrix: python-version: [3.5, 3.6, 3.7, 3.8] steps: - uses: actions/checkout@v2 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install dependencies run: | python -m pip install --upgrade pip pip install flake8 pytest if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name: Lint with flake8 run: | # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name: Test with pytest run: | pytest 可以看到，这个 workflow 比较简单，只定义了一个 job 。\n在 jobs 被定义之前的 on 是 workflow 的关键字，用来定义这个 workflow 的运行触发条件。对于这个文件来说，在分支 master 上的 push 或 pull 动作都会激活这个 workflow 。除了 push 和 pull 两种触发方式，还可以使用 crontab 定时调度触发。\n在 jobs 定义 step 之前，还有运行环境的相关定义。 runs-on 关键字用来定义 workflow 的主机工作环境，一般的工作环境有 Linux ， macOS 和 Windows ，这里使用的是 ubuntu 最新版本。 matrix 是构建矩阵，常用于定义 jobs 运行在多个不同版本的工作环境，这里定义了不同的 python 解释器版本，包括了3.5，3.6，3.7，3.8，所以这个 workflow 会在不同的解释器版本下都运行一次。\n主要部分 steps 列表定义了5个成员，在每个成员中， name 用来定义该 action 的名称， run 用来定义该 action 的具体内容， with 用来设置参数和变量， uses 用来调用社区公共 action ，它们是可以灵活组合的。\n这个 workflow 的具体 step 完成了以下工作：\n调用公共 action checkout@v2 ，我们可以在公共模板看到这个 action 的大量使用，它的工作内容是把 workflow 所在的仓库代码拉取到当前 runs-on 的工作环境中。 调用公共 action setup-python@v2 ，用来设置 python 的工作环境。 自定义 action ，升级 pip ，并且安装测试所需要的模块和代码仓库的依赖。 自定义 action ，使用 flake8 模块来检查代码语法和编写规范。 自定义 action ，使用 pytest 模块来进行单元测试。 除了 python 之外，社区还有对各类开发语言的 workflow 支持，可以根据自己的需要选用。\nHexo 自动构建部署实例 在很多情况下需要使用自定义 workflow ，以下是我使用过的工作流程，略微比 python 自动测试的 workflow 复杂，用来自动构建并部署静态博客。\nname: deploy blog on: push: branches: - master jobs: build: name: Build and deploy blog env: MY_SECRET: ${{secrets.commit_secret}} USER_NAME: username USER_EMAIL: username@mail.com PUBLISH_DIR: ./public runs-on: ubuntu-latest strategy: matrix: node-version: [10.x] steps: - uses: actions/checkout@v1 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v1 with: node-version: ${{ matrix.node-version }} - name: generate key and identify host run: | mkdir ~/.ssh/ echo \u0026#34;$MY_SECRET\u0026#34; | tr -d \u0026#39;\\r\u0026#39; \u0026gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa ssh-keyscan github.com \u0026gt;\u0026gt; ~/.ssh/known_hosts - name: install package and build run: | npm install npm run build - name: commit and push to remote repository run: | cd $PUBLISH_DIR git init git config --local user.name $USER_NAME git config --local user.email $USER_EMAIL git remote add origin your_remote_repository git add --all message=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) git commit -m \u0026#34;$message UTC\u0026#34; git push -f origin master echo deploy complete. workflow 使用 env 关键字在 job 中定义环境变量，在这个文件中，除了明文的环境变量，还使用了 GitHub 仓库中的 Actions secrets 来设置环境变量。\nActions secrets 是在每个 GitHub 仓库都可以进行设置，用来存放敏感信息的键值对。 …","date":1599426525,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"88fb95b7789802e240a33f5e4f2bd506","permalink":"https://yuweizzz.github.io/post/using_github_actions_to_deploy_applications/","publishdate":"2020-09-06T21:08:45Z","relpermalink":"/post/using_github_actions_to_deploy_applications/","section":"post","summary":"通过 GitHub Actions 实现持续集成和持续部署。\n","tags":["GitHub","GitHub Actions"],"title":"使用 GitHub Actions 部署应用","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些 Git 命令的进阶使用和部分原理知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 在本地环境的 Git 使用熟练后，我们会使用到一些更复杂的功能，协助我们更好地工作。\n了解 Git 原理 在使用更复杂的功能前，简单了解一下 Git 的原理会对后续使用有帮助。\nGit 本质上是一个内容寻址的文件系统，它会为对象计算一个索引值，并凭借这个唯一值去获取对象内容。 Git 对象一共有三种：\nblob object tree object commit object 当仓库初始化并提交第一次 commit ，Git 会根据仓库情况自动创建这些对象：\nblob object 对应着文件，用来保存代码的详细内容。 tree object 对应着文件目录，用来保存目录结构和对应目录下的 blob 对象， tree 对象可以拥有子 tree 对象和 blob 对象，但是 blob 对象只能拥有一个父 tree 对象。 commit object 对应着 commit ，用来保存本次 commit 对应的 tree object 和 parent commit object ，此外还会保存 commit 的用户配置和注释信息。 每个 commit object 和其他 object 的关系一般是这样的：\n+-------------+ +-\u0026gt;+ blob object | | +-------------+ +---------------+ +-------------+ | +-------------+ | commit object +---\u0026gt;+ tree object +----\u0026gt;+ blob object | +---------------+ +-------------+ | +-------------+ | +-------------+ +-------------+ +-\u0026gt;+ tree object |--\u0026gt;+ blob object | +-------------+ +-------------+ 每一次 commit 会产生新的 commit object ，通过各自的 parent commit object 可以形成一条 commit 链。\nfather children children +---------------+ +---------------+ +---------------+ | commit object +\u0026lt;---+ commit object +\u0026lt;---+ commit object | +---------------+ +---------------+ +---------------+ | | | v v v +-------------+ +-------------+ +-------------+ + tree object + + tree object + + tree object + +-------------+ +-------------+ +-------------+ | | | v v v ... ... ... 通过整个 commit 链，我们就可以追溯整个仓库的所有历史变动。\nbranch 的工作原理 在前面的使用过程中，我们接触了分支 branch 的概念了，也知道每个 Git 仓库会自动创建一个 master/main 分支，而它本质上是一个指向 commit object 的指针。\n虽然 Git 会默认创建 master/main 分支，但是对比其他分支，它并没有优先级的区别，只是在使用习惯上的差别。我们一般以 master/main 为主线，其他分支为辅来进行开发。\n+------------+ | new branch | +-----+------+ | father children v +---------------+ +---------------+ +-------+-------+ | commit object +\u0026lt;---+ commit object +\u0026lt;---+ commit object | +---------------+ +---------------+ +-------+-------+ ^ | +---+----+ +------+ | master +\u0026lt;---+ HEAD | +--------+ +------+ 分支在 Git 中的更底层描述是引用 reference ，一般简写为 refs 。经常和 refs 联系在一起的还有 HEAD 的这个概念，通过前面的使用我们知道 HEAD 用来指向当前分支的最新 commit ，这样的描述和 refs 非常接近，当分支切换时， HEAD 会自动指向切换后分支的最新 commit 。但实际上， HEAD 只是 refs 的符号引用，我们将通过实例来说明。\n# 以任意一个仓库为例，可以看到当前仓库有两个分支 $ git branch * branchA main # 查看仓库已拥有的 refs $ ls .git/refs/heads/ branchA main # 查看 refs 指向的 object 类型 $ git cat-file -t `cat .git/refs/heads/main` commit # 指向的正是 commit # 查看 refs 指向的 object 的具体内容 $ git cat-file -p `cat .git/refs/heads/main` # 命令应该会输出某个 commit 的具体信息，我们可以通过对比 git log 得到它是当前分支的最新 commit # 查看 HEAD 的内容 $ cat .git/HEAD ref: refs/heads/branchA # 可以看到它是 refs 的符号引用 branch ， refs ， HEAD 三者之间有一个共同点，那就是它们的本质都是用来指向 commit object ， refs 是 branch 的低级原语，而 HEAD 是 refs 的符号引用，它是为了使用方便引出的概念，每次进行 branch 切换， HEAD 也会自动更新。\n使用 remote 功能 除了在本地设备上使用之外， Git 更强大的地方在于远端功能的支持，我们可以把仓库托管到云端，这个云端可以是 GitHub 或者自建的 GitLab 平台，这样我们就可以随时获得最新版本的代码仓库，使得团队协作开发更方便。\n在使用远端功能之前，需要确认 GitHub 或者自建的 GitLab 平台的账号鉴权是否完成，可以参考之前的配置 GitHub 免密认证的方法和通用的账号配置方法。\n# 本地仓库和远端仓库的基本关系: # for new repository: full remote add empty +------------------+---------\u0026gt;+-------------------+ | local repository | | remote repository | +------------------+\u0026lt;---------+-------------------+ empty clone full # for existing repository: new commit push +------------------+---------\u0026gt;+-------------------+ | local repository | | remote repository | +------------------+\u0026lt;---------+-------------------+ pull new commit 在本地仓库和远端仓库的交互中，出现了几个新的概念： clone ， pull 和 push ，它们同时也是 Git 的子命令：\nclone ： 用来将远端仓库复制到本地。\npull ： 用来更新本地仓库，将远端仓库的新变动拉取到本地。\npush ： 用来更新远端仓库，将本地仓库的新变动推送到远端。\n我们通过具体实际操作来熟悉它们。\n# 查看已配置的远程仓库 $ git remote -v origin git@github.com:username/repository.git (fetch) origin git@github.com:username/repository.git (push) # 每个本地仓库允许配置多个远程仓库，默认的远程仓库会是 origin # 将远端仓库下载到本地 # 远端仓库的地址信息一般可以直接在对应网页复制 $ git clone git@github.com:username/repository.git # clone 可以指定明确的远端仓库中任一分支，不指定默认为 master/main $ git clone -b branchA git@github.com:username/repository.git # 将本地仓库关联到远端仓库并推送更新 # 将现存的本地仓库关联到全新的远程仓库 $ git remote add origin git@github.com:username/repository.git # 设置分支关联并推送本地仓库的 commit 到远端仓库中 $ git push -u origin master # -u 是关键的参数，它将 origin/master 分支和本地的 master 分支关联，它们才能使用 pull 和 push # 这一步可以根据情况使用下面命令进行替代 # 使用 checkout 创建全新分支并设置上游分支 $ git checkout -b mybranch origin/mybranch # 使用 set upstream 设置本地分支的上游分支 $ git checkout mybranch $ git branch -u/--set-upstream-to origin/mybranch # 完成后，后续直接使用 git push 即可 $ git push # 拉取远端仓库领先于本地仓库的 commit 同时合并到本地仓库的当前分支中 $ git pull # 一般情况下它的效果会相当于 git pull origin master:master ， git pull 正常工作的前提是 remote 和 upstream 已经正常设置 # 除了这种默认情况，也可以显式声明使用其他远端仓库和需要拉取合并的分支 # 实际上 pull …","date":1597356000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5346a32a37956c4e634464a0d751d6b6","permalink":"https://yuweizzz.github.io/post/more_tips_about_git/","publishdate":"2020-08-13T22:00:00Z","relpermalink":"/post/more_tips_about_git/","section":"post","summary":"这篇笔记用来记录一些 Git 命令的进阶使用和部分原理知识。\n","tags":["Git","GitHub"],"title":"Git 命令进阶使用笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录如何把宿主机的磁盘目录共享到 VMware Workstation 创建的虚拟机之中。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 首先在宿主机创建好需要的共享文件夹，然后启动 VMware Workstation 并选择对应的虚拟机，依次点击编辑虚拟机设置，选项，共享文件夹，选择创建好的主机目录进行添加，宿主机需要做的部分就完成了，可以根据需要设置读写模式为只读，这样虚拟机就只能读取而不能修改共享目录。\n接下来是虚拟机需要操作的部分：\n# 虚拟机需要安装 open-vm-tools $ yum install open-vm-tools # 安装完成后可以列出宿主机提供的共享目录 $ vmware-hgfsclient Share # 在宿主机指定共享目录的时候，还需要自定义名称，会在这里显示 # 挂载目录 $ mkdir /mnt/share $ vmhgfs-fuse .host:/Share /mnt/share # 开机自动挂载 $ echo .host:/Share /mnt/share fuse.vmhgfs-fuse allow_other 0 0 \u0026gt;\u0026gt; /etc/fstab ","date":1595541600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"567e8d0c9c13d76e9fa1c52f1b6ec662","permalink":"https://yuweizzz.github.io/post/share_folder_in_vmware_workstation/","publishdate":"2020-07-23T22:00:00Z","relpermalink":"/post/share_folder_in_vmware_workstation/","section":"post","summary":"这篇笔记用来记录如何把宿主机的磁盘目录共享到 VMware Workstation 创建的虚拟机之中。\n","tags":["VMware Workstation"],"title":"VMware Workstation 虚拟机共享宿主机磁盘目录","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录一些常用的 Git 命令。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 使用 Git 可以有条理地管理代码，尤其是在大型项目和团队开发的场合，熟练使用 Git 是开发人员的基本功。\n想要熟练使用 Git 的最好办法是了解 Git 的基本原理和实践。\n快速了解 Git 术语 首先了解几个基本概念：\n仓库 (repository) ：仓库是一个被 Git 管控的目录。 分支 (branch) ：每个仓库一般有一个默认分支 master/main ，并且可以拥有多个分支。 提交 (commit) ：可以视为整个代码仓库的不同版本，提交会推进仓库的版本更新。 创建并配置本地仓库 cd repository git init 这样 repository 目录就是一个 Git 本地仓库，后续用来添加项目代码文件。\n初始化仓库后，仓库会带有一些默认的配置，我们可以根据需要去查询或者修改。\n# config -l用来查看仓库属性 # 如果要使用 github 的远程仓库，至少应该为仓库配置用户名和邮箱 # --global 和 --local 可以用来设定属性是 git 全局属性还是单一仓库属性 $ git config -l $ git config [--global|--local] user.name \u0026#34;your name\u0026#34; $ git config [--global|--local] user.email \u0026#34;your@email.com\u0026#34; 转换仓库中文件的状态 下面是一个 Git 仓库实例，基本覆盖了文件状态之间的转换：\n$ git status # On branch master # Changes to be committed: # (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) # # modified: fileA # # Changes not staged for commit: # (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) # (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) # (commit or discard the untracked or modified content in submodules) # # modified: fileB # # Untracked files: # (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) # # fileC 这里引入了 Git 仓库工作区和暂存区的概念：\n上面的仓库包含了多种文件，FileA ，FileB ，FileC 处于同一个工作区，工作区 (workspace) 可以简单认为是仓库所在目录，其中存放了各种不同状态的文件。\n暂存区 (index) 是一个抽象概念，它的信息会保存在实际文件 (.git/index) 中，我们修改过的文件，就可以暂存 (staged) 到暂存区中，也是 Changes to be committed 这一部分，它的下一步操作通常会是提交 (commit) 。\n下面进入实例的分析：\nChanges to be committed 列出的是已经被暂存的 FileA ，它已经由 not staged 转换为 staged 。下一步它可以提交到分支中，或者使用 reset 重置掉这次 modified 。 Changes not staged for commit 列出的是在工作区中被修改过，但是尚未被 staged 的 FileB 。下一步它可以使用 add 更新到暂存区，或者使用 checkout 丢弃本次修改，恢复到上次 add 的暂存区版本或 commit 保存的版本，恢复的版本由最近的一次的修改情况决定。 Untracked 的 FileC 目前是不受 Git 管制的，但是可以通过 add 转换状态。 以下是实际命令的使用过程：\n添加和删除文件 添加文件和暂存文件都使用了 add 命令，删除文件使用 rm 命令。\n通过删除文件，我们可以把文件排除出工作区 (untracked) ，如果是某些特定种类的文件需要排除，推荐使用 .gitignore 。\n$ git add \u0026lt;file\u0026gt; # add 用于 tracked 和 staged $ git rm [-f] [--cached] [--] \u0026lt;file\u0026gt; # rm 用来移除文件 # rm 带 -f 选项会删除工作区和暂存区文件，即本地文件也会被删除 # rm 带 --cached 选项会删除暂存区文件，工作区文件不会被改变 # -- 选项用来把操作限制在当前分支 文件的重置 需要丢弃不想要的修改，可以使用 reset 命令或者 checkout 命令。\n$ git reset HEAD \u0026lt;file\u0026gt; # reset 指令涉及 git 的实现原理，HEAD 是指向当前分支最新 commit 的指针 # 这里只能用于回退暂存区文件，后面会有涉及这一步的解释 $ git checkout -- \u0026lt;file\u0026gt; # checkout 其实是用于分支的命令，-- 用来把操作限制在当前分支 # 这里用于将文件恢复到上次 add 的暂存区版本或 commit 保存的版本 # 这个命令会改变实际文件，有一定风险 提交到新版本 文件修改，重置，暂存一系列操作，都是为了将代码修改到我们认可的程度，然后 commit 到分支中，产生新的版本库，一次或多次的 commit 可以视为一个新的版本库。\n$ git commit -m \u0026lt;msg\u0026gt; # 提交暂存区的文件到版本库， -m 为提交的说明信息 $ git commit --amend [\u0026lt;msg\u0026gt;] # 修改上次 commit ，可以添加提交暂存区文件并修改上次 commit 说明信息 这样所有 staged 的文件就会被提交到一个新的版本库中了。我们还会得到一个 commit ID ，这是非常重要的依据，后续版本库的管理都是基于 commit ID 来实现的。\n带 –amend 的 commit 指令可以在一次提交后，追加位于暂存区的文件变动。使用这个命令后只会有一个提交，本次带 –amend 的提交将代替上一次提交的结果。如果暂存区是干净的，可以使用这个命令修改上次 commit 的说明信息。\n对 commit 进行检查 在经过一定的开发时间后，仓库必然会产生很多 commit ，可以使用 log 和 diff 进行检查。\n$ git log # 查看 commit 历史记录，可以看到简要的提交信息 $ git show \u0026lt;object\u0026gt; # 用来查看 commit 的具体变动，默认为最近一次 commit 的详细信息 # 可以通过指定 commit id 来查看不同 commit $ git reflog # 类似于 bash history，它会输出 commit 操作的历史记录，可以作为版本回退和误操作恢复的依据 $ git diff \u0026lt;commit\u0026gt; \u0026lt;commit\u0026gt; [--] [\u0026lt;path\u0026gt;...] # 用来查看不同 commit 之间的所有文件的变动，可以指定单个文件 $ git diff --cached [\u0026lt;commit\u0026gt;] [--] [\u0026lt;path\u0026gt;...] # 除了用于 commit 外，还可以用来查看暂存区和工作区的文件变动 回退 commit 如果需要撤消 commit ，恢复原有的代码版本，需要用到 reset 指令。\n在 reset 命令中，我们经常会看到 HEAD 的使用，它用来指向当前分支的最新 commit ，所以最新 commit 的上一个版本使用 HEAD^ 表示，上上一个版本使用 HEAD^^ 表示，这是一种便捷写法，使用对应 commit ID 也是一样的效果。\n# reset 的使用参数 $ git reset [--mixed | --soft | --hard | --merge | --keep] [-q] [\u0026lt;commit\u0026gt;] # --mixed reset HEAD and index # --soft reset only HEAD # --hard reset HEAD, index and working tree # --merge reset HEAD, index and working tree # --keep reset HEAD but keep local changes # -q, --quiet be quiet, only report errors 可以看到 reset 有五种工作模式，但我在使用的时候一般只用到 mixed ， soft 和 hard 三种：\nmixed ：默认的工作模式，这个模式会影响 commit 和暂存区保存的文件变动。 soft ：这个模式的影响对象是 commit ，所以实际效果看起來就只有 commit 的消失，也就是 HEAD 指向了更旧的 commit 。 commit 回退后，文件变化会存放在暂存区。 hard ：这个模式下， commit ，工作区，暂存区的文件变动都会丢失。 在前面的实例中 git reset HEAD \u0026lt;file\u0026gt; 就是 mixed 模式的应用，它用来回退 staged 文件到 not staged 的状态。因为指定了 HEAD ，所以文件变化还是留存在工作区，暂存区被清空， commit 无变化。\nsoft 模式是最安全的，它不会影响实际文件，而 hard 模式是最为危险，在没有确定你需要丢弃你新写的代码，最好不要使用。\n此外还有一个值得注意的地方， reset 命令可以应用于单个文件，但仅限于 mixed 模式， hard 模式和 soft 模式均无法对单文件作用。\n如果需要撤销 commit ，但不具体恢复到某个原有的代码版本，需要用到 revert 指令。\nrevert 的使用方法类似于 reset ，但它不会改变 commit 的历史记录，而是在最新 commit 的基础上，将所指定 commit 的文件变动还原，并将这个过程提交为一个新的 commit 。\n# revert 的使用参数 $ git revert \u0026lt;commit\u0026gt; # 执行后将会产生新的 commit 的提交，默认将以 revert 信息作为 commit 信息 分支管理 简单项目可能没有使用到多分支，但如果是大型项目和 …","date":1595541600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"5a08d20327798cba79feeb99f4e18099","permalink":"https://yuweizzz.github.io/post/tips_about_git/","publishdate":"2020-07-23T22:00:00Z","relpermalink":"/post/tips_about_git/","section":"post","summary":"这篇笔记用来记录一些常用的 Git 命令。\n","tags":["Git"],"title":"常用 Git 命令笔记","type":"post"},{"authors":null,"categories":null,"content":"通过配置 GitHub SSH Key 实现 GitHub 免密推送，并且使用 GPG 密钥对 commit 进行签名。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 我们通过本地 Git 来远程管理 GitHub 上的仓库，在每次推送都需要输入账户和密码，可以通过配置 GitHub 公钥，省去重复输入账户密码的麻烦。\n创建秘钥对 GitHub 的推送和 SSH 的登陆原理非常相似，通常是本地使用私钥，GitHub 端保存公钥信息，我们使用 Git 推送时，以私钥加密的信息可以被对应的公钥解析，通信就能够建立了。\n我们需要在本地先生成一个新的秘钥对。\n$ ssh-keygen -t rsa -C \u0026#34;注册 GitHub 时的邮箱\u0026#34; # -t 代表秘钥对的加密算法，rsa是最常见的非对称加密算法 # -C 可以指定公钥的文本信息，这一步是 GitHub 要求的 上述的命令会产生一个交互对话，按照对话提示输入对应信息即可。\n在交互式生成模式下，会提示我们指定生成秘钥对的文件名和路径，通常在 Linux 环境下，保持默认则为 /home/user/.ssh/id_rsa ，user 为当前使用的用户。\n此外，在交互式生成模式下，还会提示使用 passphrase 再度加密，这个功能一般不使用，否则每次使用密钥都需要再次输入 passphrase 。\n所以，我比较推荐使用非交互式的命令生成秘钥对。\n$ ssh-keygen -t rsa -C \u0026#34;注册 GitHub 时的邮箱\u0026#34; -P \u0026#34;\u0026#34; -f ~/.ssh/id_rsa # 非交互生成的参考命令 # -P passphrase为空 # -f 指定路径名称 执行完命令后，我们可以得到 id_rsa 和 id_rsa.pub 两个文件，其中 id_rsa 是私钥，id_rsa.pub 是公钥。\n在 GitHub 账户中添加公钥 我们需要把公钥信息上传到自己的 GitHub 账户。\n打开自己的GitHub 账户，点击 New SSH key ，复制 id_rsa.pub 的内容到 Key 中，Title 按照自己的需求命名，然后 Add SSH Key ，完成公钥添加。\n测试公钥 完成云端公钥信息的添加后，我们可以在本地测试公钥是否正确设置。\n# 使用生成秘钥的账户测试 $ ssh git@github.com # 出现 kex_exchange_identification: Connection closed by remote host 报错的解决方法： # 通过 ~/.ssh/config 重新指定 ssh 信息 # 应该尽量通过写入 ssh_config 的方式实现，尤其是私钥不是以默认方式命名的情况 $ cat \u0026gt; ~/.ssh/config \u0026lt;\u0026lt; EOF Host github.com HostName ssh.github.com User git Port 443 IdentityFile ~/.ssh/id_rsa EOF # 出现报错并且尚未写入 ssh_config 时可以通过以下的命令行测试 $ ssh git@ssh.github.com -i ~/.ssh/id_rsa -p 443 在第一次使用免密登录时会需要我们确认 GitHub 的主机信息，输入 yes 即可。\n如果一切正常，会返回下面的信息：\n$ ssh git@github.com PTY allocation request failed on channel 0 Hi yuweizzz! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. Connection to github.com closed. 其实从这部分可以得出 GitHub 推送仓库是基于 OpenSSH 来实现的。\n更多信息可以参考 GitHub 的官方文档。\n利用 GPG 密钥对 commit 进行签名 # 生成 GPG 密钥 $ gpg --full-generate-key # 查看生成后的密钥信息 $ gpg --list-keys # 导出对应的公钥，并将它添加到对应的 Github 账户中 $ gpg --armor --export \u0026lt;key-id\u0026gt; # 在对应仓库中设置 signingkey 后，就可以在 commit 时进行签名 # GPG_TTY 用于交互式获取 GPG 密钥的 passphrase ，没有正常设置时可能会无法进行签名 $ git config user.signingkey \u0026lt;key-id\u0026gt; $ export GPG_TTY=$(tty) $ git commit -S -m \u0026#34;...\u0026#34; ","date":1595286000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"6c288741c3aedbb6edb1c83c41d5bdc3","permalink":"https://yuweizzz.github.io/post/authenticating_to_github/","publishdate":"2020-07-20T23:00:00Z","relpermalink":"/post/authenticating_to_github/","section":"post","summary":"通过配置 GitHub SSH Key 实现 GitHub 免密推送，并且使用 GPG 密钥对 commit 进行签名。\n","tags":["GitHub","SSH","Git"],"title":"配置 GitHub 认证密钥","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录配置 Linux 环境变量的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 背景 最近经常需要自行编译安装一些开源软件，它们有着各自的运行方式，经常需要定义一些环境变量，那么在何处定义这些变量成了需要探究的问题。\n设置即时生效的环境变量 我们可以从最常用的环境变量 PATH 开始，它是寻找解释器或者命令的默认路径，以 : 分割不同路径的形式出现。在这个变量定义的路径中，可执行文件可以被快速搜索并直接执行，而且路径的顺序是有意义的，越靠前的搜索优先级越高。经常可以在一些软件的使用文档中看到 export PATH=/path/to/bin:$PATH 的类似用法。\n除了 export 之外，还有 source 也可以用来设置一些环境变量， python 的虚拟环境 venv 就用到了 source 来设置一些环境变量，实际上 export 和 source 都是 bash 的内置命令。\n# 以下出自 bash 的相关文档 $ man bash ... SHELL BUILTIN COMMANDS ... export [-fn] [name[=word]] ... export -p The supplied names are marked for automatic export to the environment of subsequently executed commands. If the -f option is given, the names refer to functions.If no names are given, or if the -p option is supplied, a list of all names that are exported in this shell is printed. The -n option causes the export property to be removed from each name. If a variable name is followed by =word, the value of the variable is set to word. export returns an exit status of 0 unless an invalid option is encountered, one of the names is not a valid shell variable name, or -f is supplied with a name that is not a function. ... . filename [arguments] source filename [arguments] Read and execute commands from filename in the current shell environment and return the exit status of the last command executed from filename. If filename does not contain a slash, file names in PATH are used to find the directory containing filename. The file searched for in PATH need not be executable. When bash is not in posix mode, the current directory is searched if no file is found in PATH. If the sourcepath option to the shopt builtin command is turned off, the PATH is not searched.If any arguments are supplied, they become the positional parameters when filename is executed. Otherwise the positional parameters are unchanged.The return status is the status of the last command exited within the script (0 if no commands are executed), and false if filename is not found or cannot be read. ... 从文档可以看到， export 被明确定位用来快速设置环境变量。而 source 则要特殊一些，可以看到它是对文件级进行操作的，实际上它并不明确涉及任何变量改动。但是它们有一个相同点在于它们的生效范围都是当前 shell 。所以当你使用了 export 设置了一些变量，或者使用 source 执行了某个文件，那么它们涉及的变量改动就影响了当前 shell ，起到了改变环境变量的效果。\n设置可持久化的环境变量 由于前面所述的方法只对当前 shell 生效，那么持久方案也值得我们额外关注。\n这里只讨论 Linux 环境下的 bash ，其他类型的 shell 和 bash 可能略有差别，但它们的机制应该是相近的。\nshell 的属性 bash 的机制比表面看上去的复杂得多，它隐藏了很多的属性设置。可以根据我们比较关系的属性，来简单划分 shell 的种类。\n影响环境变量的属性有比较关键的两个，分别是登录相关的属性和交互相关的属性。\n根据登录相关的属性分为登录式 login shell 和非登录式 non-login shell ，根据交互相关的属性分为交互式 interactive shell 和非交互式 non-interactive shell ，将两种属性组合起来可以将 shell 分为四个种类。\n# 以下结果由直接登录系统后的 shell 中执行得出 # 是否为登录式 shell $ shopt | grep login login_shell on # 是否为交互式 shell $ echo $- himBH # 以脚本文件执行检查 $ cat test.sh shopt | grep login echo $- $ bash test.sh login_shell off hB 关于登录属性的判断比较直观，关于交互属性则需要通过特殊变量去获取， - 变量可以获取 shell 的部分设置选项，每个字母代表一种属性，其中 i 正是关于交互属性的字段。 在 - 变量包含的各字段意义如下：\n$ man bash ... OPTIONS -l Make bash act as if it had been invoked as a login shell (see INVOCATION below). # 以登录模式启动 shell -i If the -i option is present, the shell is interactive. # 以交互模式启动 shell ... SHELL BUILTIN COMMANDS set [--abefhkmnptuvxBCEHPT] [-o option-name] [arg ...] set [+abefhkmnptuvxBCEHPT] [+o option-name] [arg ...] Without options, the name and value of each shell variable are displayed in a format that can be reused as input for setting or resetting the currently-set variables.Read-only variables cannot be reset. In posix mode, only shell variables are listed. The output is sorted according to the current locale. When options are specified, they set or unset shell attributes. Any arguments remaining after option processing are treated as values for the positional parameters and are assigned, in order, to $1, $2, ... $n. Options, if specified, have the following meanings: ... -h Remember the location of commands as they are looked up for execution. This is enabled by default. # 缓存执行过的二进制命令的路径，以便下次调用省去搜索时间 -B The shell performs brace expansion (see Brace Expansion above). This is on by default. # 允许使用 shell 的花括号扩展 -m Monitor mode. Job control is enabled. This option is on by default for interactive shells on systems that support it (see JOB CONTROL above). Back‐ground processes run in a separate process group and a line containing their exit status is printed upon their completion. # 允许作业控制和后台运行 -H Enable ! style history substitution. This option is on by default when the shell is interactive. # 允许快速索引存在历史记录中的命令 ... profile 和 rc bash 使用了 profile 和 rc 来完成一 …","date":1585514505,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"96696f62764611b30919c81eaab52dca","permalink":"https://yuweizzz.github.io/post/tips_about_environment_variable/","publishdate":"2020-03-29T20:41:45Z","relpermalink":"/post/tips_about_environment_variable/","section":"post","summary":"这篇笔记用来记录配置 Linux 环境变量的相关知识。\n","tags":["Linux","Environment Variable"],"title":"配置 Linux 环境变量","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 DNS 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ 前言 网络服务是一般是基于 IP 地址提供的，但是 IP 地址比较难记，所以现实中使用了人类便于记忆的域名来访问网络服务。\n而 DNS 服务器负责提供域名和 IP 地址映射关系查询的服务，一般叫做域名解析。\nDNS 配置文件 在使用网络服务时，域名解析的工作由操作系统的全局配置的 DNS 服务器负责。\nDNS 带有缓存机制，短时间的相同请求并不会多次请求 DNS 服务器，而会直接使用缓存，直到缓存过期后发起新的查询请求。\n除了自身的 DNS 缓存和发起新的查询请求，本地 hosts 也可以提供类似于 DNS 的服务，它是可以自行定义的域名和 IP 地址的映射表，能够一定程度上优化 DNS 解析的进行，但现实中它多用于开发过程中的调试。\n# 全局配置的 DNS 服务器 $ cat /etc/resolv.conf ; generated by /sbin/dhclient-script nameserver 192.168.0.1 # 本地 hosts 定义的映射关系 $ cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 # 在这个文件中写入 IP 地址和对应域名，在访问对应域名时会直接使用文件中给定的 IP 地址 DNS 查询规则 域名解析记录类型 发起域名查询的工具有 nslookup 和 dig ，使用 dig 可以输出详细的域名解析记录。\ndig 查询返回的域名解析记录的类型比较多，下面列出常见的域名解析记录类型：\nA ：解析对应域名的 IPv4 地址。 AAAA ：解析对应域名的 IPv6 地址。 CNAME ：全称为 Canonical Name ，用它来将为一个域名设置多个别名，使得不同域名最终解析到相同的 IP 地址。 SOA ：用来表示此指明域名的主要 DNS 服务器，并设置记录的过期时间和版本信息。 NS ：用来指明 DNS 服务器的 IP 地址， NS 记录可以有多个。 MX ：全称为 Mail Exchange ，用来指定邮件服务器的 IP 地址。 PTR ：反向解析记录，用来记录 IP 地址对应的域名信息。 SRV ：用来记录提供特定服务的域名。 TXT ：用来设置域名的说明信息。 # 使用 dig 对域名发起解析请求 $ dig github.com +noauthority +noadditional ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.4 \u0026lt;\u0026lt;\u0026gt;\u0026gt; github.com +noauthority +noadditional ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 8528 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;github.com. IN A ;; ANSWER SECTION: github.com. 572 IN A 20.205.243.166 ;; Query time: 41 msec ;; SERVER: 192.168.0.1#53(192.168.0.1) ;; WHEN: Sat Jan 08 16:24:44 CST 2022 ;; MSG SIZE rcvd: 44 访问站点需要的是 A 记录，这里省去了 dig 输出的 authority 和 additional 部分， authority 包含了所有的权威 DNS 服务器， additional 包含了 authority 中服务器的 A 记录。\n根服务器和权威服务器 实际中使用的域名通过 . 来划分不同的域，例如 github.com 这个域名就可以划分为二级域 github 和一级域 com 。\n在前面使用 dig 进行域名查询时，可以看到在 A 记录中，它的域名显示为 github.com. ，这个 . 代表根域，它是所有域名的起源。\n$ dig . -t soa +noadditional ;; Warning: Message parser reports malformed message packet. ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.4 \u0026lt;\u0026lt;\u0026gt;\u0026gt; . -t soa +noadditional ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 32440 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 13, ADDITIONAL: 26 ;; WARNING: Message has 7 extra bytes at end ;; QUESTION SECTION: ;. IN SOA ;; ANSWER SECTION: . 3595 IN SOA a.root-servers.net. nstld.verisign-grs.com. 2022010700 1800 900 604800 86400 ;; AUTHORITY SECTION: . 260739 IN NS j.root-servers.net. . 260739 IN NS f.root-servers.net. . 260739 IN NS c.root-servers.net. . 260739 IN NS i.root-servers.net. . 260739 IN NS h.root-servers.net. . 260739 IN NS d.root-servers.net. . 260739 IN NS k.root-servers.net. . 260739 IN NS l.root-servers.net. . 260739 IN NS a.root-servers.net. . 260739 IN NS b.root-servers.net. . 260739 IN NS g.root-servers.net. . 260739 IN NS e.root-servers.net. . 260739 IN NS m.root-servers.net. ;; Query time: 11 msec ;; SERVER: 192.168.0.1#53(192.168.0.1) ;; WHEN: Sat Jan 08 16:50:46 CST 2022 ;; MSG SIZE rcvd: 512 根域一共有 13 台 DNS 服务器，由于根域的需要处理的请求数量比较大，所以根域 DNS 服务器中只保存解析一级域的相关记录。\n# 使用 dig 获取域名的权威 DNS dig github.com +noadditional -t soa ;; Warning: Message parser reports malformed message packet. ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.4 \u0026lt;\u0026lt;\u0026gt;\u0026gt; github.com +noadditional -t soa ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 61395 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 8, ADDITIONAL: 17 ;; QUESTION SECTION: ;github.com. IN SOA ;; ANSWER SECTION: github.com. 3478 IN SOA dns1.p08.nsone.net. hostmaster.nsone.net. 1641583307 43200 7200 1209600 3600 ;; AUTHORITY SECTION: github.com. 520 IN NS dns2.p08.nsone.net. github.com. 520 IN NS ns-421.awsdns-52.com. github.com. 520 IN NS dns4.p08.nsone.net. github.com. 520 IN NS ns-1283.awsdns-32.org. github.com. 520 IN NS ns-520.awsdns-01.net. github.com. 520 IN NS dns1.p08.nsone.net. github.com. 520 IN NS ns-1707.awsdns-21.co.uk. github.com. 520 IN NS dns3.p08.nsone.net. ;; Query time: 16 msec ;; SERVER: 192.168.0.1#53(192.168.0.1) ;; WHEN: Sat Jan 08 22:08:46 CST 2022 ;; MSG SIZE rcvd: 512 权威 DNS 服务器是域名的可信解析来源，一般来说 SOA 记录指定的 DNS 服务器就是权威 DNS ，但是为了保证域名查询的可用性，一般会设置多个 NS 记录，它们会在 dig 输出的 authority 部分，它们也属于权威 DNS 服务器。\n递归查询和迭代查询 一次 DNS 查询行为一般由递归查询和迭代查询协作完成。\n我们在浏览器上访问一个新的网站，这个时候一般需要向全局设置的 DNS 发起一个新的 DNS 递归查询。\n假设全局设置的 DNS 服务器没有所请求域名的缓存，并且不是所请求域名的权威 DNS ，那么在它允许接受递归查询的前提下，会进行如下工作：\n本地全局 DNS 服务器向根服务器发起目标一级域的查询请求。 本地全局 DNS 服务器接收到目标 …","date":1584890145,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"38c13186a41487b3933be9b71e2329bb","permalink":"https://yuweizzz.github.io/post/knowledge_about_dns/","publishdate":"2020-03-22T15:15:45Z","relpermalink":"/post/knowledge_about_dns/","section":"post","summary":"这篇笔记用来记录 DNS 的相关知识。\n","tags":["Linux","DNS","BIND"],"title":"DNS 知识笔记","type":"post"},{"authors":null,"categories":null,"content":"这篇笔记用来记录 Linux 中 iptables 的相关知识。\n(@@) ( ) (@) ( ) @@ () @ O @ O @ ( ) (@@@@) ( ) (@@@) ==== ________ ___________ _D _| |_______/ \\__I_I_____===__|_________| |(_)--- | H\\________/ | | =|___ ___| _________________ / | | H | | | | ||_| |_|| _| \\_____A | | | H |__--------------------| [___] | =| | | ________|___H__/__|_____/[][]~\\_______| | -| | |/ | |-----------I_____I [][] [] D |=======|____|________________________|_ __/ =| o |=-~O=====O=====O=====O\\ ____Y___________|__|__________________________|_ |/-=|___|= || || || |_____/~\\___/ |_D__D__D_| |_D__D__D_| \\_/ \\__/ \\__/ \\__/ \\__/ \\_/ \\_/ \\_/ \\_/ \\_/ Iptables 的规则链 在 Linux 的网络协议栈中，通过内核组件 netfilter 来控制网络数据包的来往，对主机进行网络防护。而 iptables 是用来配置 netfilter 内核组件规则的用户层工具。\n在 iptables 中，将所有网络数据包抽象为几条规则链，首先数据包会从外界流入，内核也会发出数据包，它们就区分两条规则链 INPUT 和 OUTPUT 。这里考虑的情况比较简单，相当于认为系统只是局域网络下的一个普通用户，但如果系统作为了局域网的网关时，两条规则链的管理能力是明显不够的。这时 INPUT 之前会诞生两条新的规则链 PREROUTING 和 FORWORD ，其中 PREROUTING 作用于路由前的数据包，即未分辨数据包发往何处， FORWORD 作用于需要转发的数据包，其实在 PREROUTING 中的数据包一般就只有进入内核，被转发或者丢弃这几种处理办法，正好对应了 INPUT 和 FORWORD 。\n由于 FORWORD 的诞生，会额外在 OUTPUT 延申新的规则链 POSTROUTING ，从 FORWARD 中流出的数据包可以直接进入 POSTROUTING ，并且内核中 OUTPUT 的数据包也要经过 POSTROUTING 。\n+---------------+ +---------------+ | | | | | INPUT | | OUTPUT | | | | | +------+--------+ +------+--------+ ^ | | | | | | +---------------+ | | ROUTING | | | +---------------\u0026gt;+ FORWARD +------------+ | | | | | +---------------+ v +------+--------+ +------+--------+ | | | | | PREROUTING | | POSTROUTING | | | | | +---------------+ +---------------+ 所以一共有 INPUT ， OUTPUT ， PREROUTING ， FORWORD ， POSTROUTING 五条规则链控制进出的网络数据包。在作为局域网络内单一主机时， PREROUTING ， FORWORD 和 POSTROUTING 是不起作用的，只由 INPUT 和 OUTPUT 负责本机的数据包管控。在作为网络入口或中转设备时， PREROUTING ， FORWORD 和 POSTROUTING 起到主要作用，INPUT 和 OUTPUT 起到辅助作用，如果想要将 Linux 作为网关来使用，还需要通过 echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forword 启动内核的路由转发功能，这样上述的对应规则链才有意义。\nIptables 的规则表 在前面的五条规则链的基础上，每条规则链之中具有相同功能的规则会划分到四张表中，四个表分别是 filter ， mangle ， nat 和 raw 。并且四个表有优先级之分，从高到低依次是 raw ， mangle ， nat ， filter ：\nraw 是跟踪数据表规则表，一般用来控制数据包的连接追踪机制。\nmangle 是修改数据标记位规则表，负责对数据包进行修改，并重新封装。\nnat 是地址转换规则表，负责地址转换和端口映射。\nfilter 是过滤规则表，负责过滤，是使用最多的表。\n在四张表中， raw 表比较特殊，它可以用于关闭数据的的追踪处理，因为 iptables 的数据追踪相关信息会记录在 /proc/net/nf_conntrack 中，并且 /proc/sys/net/netfilter/nf_conntrack_max 控制了数据包最大追踪数量，在流量过大时可能会出现 ip_conntrack: table full, dropping packet 这类日志，这时可以通过将这类数据包加入到 raw 表的规则中以关闭 iptables 对数据包状态的追踪，关闭了数据追踪的数据包就可以跳过后续低级别的规则表处理。\nnat 表和 filter 表是使用频繁的规则表， filter 可以用来过滤攻击数据包，而 nat 在虚拟化网络和容器网络等场景都有相关的应用。 mangle 表则可以修改相关的数据包内容，复杂度更高。\nIptables 的规则管理 在实际应用中，我们直接使用 iptables 添加需要的规则动作。\n# 查看 iptables 规则 # 不指定 -t 选项，默认查询 filter 表的规则 $ iptables -L -n --line-numbers Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 2 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 3 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 4 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:22 5 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT) num target prot opt source destination 1 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT) num target prot opt source destination 规则是以 IP 地址为基本匹配条件的，比较常见的响应动作有 ACCEPT ， REJECT ， DROP ， LOG ， DNAT ， SNAT ， REDIRECT 和 MASQUERADE 这几种，它们基本都出现在 nat 表和 filter 表中：\nACCEPT ， REJECT ， DROP 是最基本的三种动作， ACCEPT 为接收， REJECT 为拒绝接收， DROP 为丢弃。\nDNAT ， SNAT ， MASQUERADE 和 REDIRECT 是 nat 表的专属动作，其中 SNAT 为源地址转换， DNAT 为目的地址转换， REDIRECT 为端口映射， MASQUERADE 为源地址转换的特殊形式，是一种动态转换源地址的 NAT ，它会自动获取地址作为 SNAT 的转换地址，而不是使用固定的地址进行转换。\nLOG 是用于日志记录的动作，它会在 /var/log/message 中记录一条信息，然后把数据包交由下一规则匹配。\n顺序是 iptables 的工作准则，在各个表中会按照规则的顺序进行匹配，匹配成功就执行对应的动作，后面的规则不会被使用，除了特殊动作 LOG 。如果流过某个规则链的所有表格都没有对应的匹配规则，就会按照这个规则链的默认策略来响应。\n# iptables 具体使用例子 # 新增规则，新规则将会是表中最后一条规则 $ iptables -t filter -A INPUT -s 192.168.1.222 -j REJECT # 插入规则，新规则的序号将会是 1 $ iptables -t filter -I INPUT -s 192.168.1.222 -j REJECT # 使用具体序号插入规则 $ iptables -t filter -I INPUT 4 -s 192.168.1.222 -j REJECT # 通过详细条件匹配规则并进行删除 $ iptables -t filter -D INPUT -s 192.168.1.222 -j REJECT # 通过规则序号匹配规则并进行删除 $ iptables -t filter -D INPUT 4 # 通过规则序号匹配规则并进行规则内容替换 $ iptables -t filter -R INPUT 4 -s 192.168.1.222 -j REJECT # 清除所有规则 $ iptables -F # 设置规则链的默认策略 $ iptables -t filter -P INPUT DROP # 输出规则表中的所有规则 $ iptables -t filter -S # 将当前所有规则保存到文件中 $ iptables-save \u0026gt; /etc/sysconfig/iptables # 使用文件中保存的规则覆盖当前所有规则 $ iptables-restore \u0026lt; /etc/sysconfig/iptables # 通过 iptables 设置端口转发 # 源地址为 192.168.1.222:9999 ，目标地址为 192.168.1.233:10000 ，则应该在 192.168.1.222 上添加转发规则 $ iptables -t nat -A PREROUTING -p tcp -m tcp --dport 9999 -j DNAT --to-destination 192.168.1.233:10000 $ iptables -t nat -A POSTROUTING -d 192.168.1.233 -p tcp -m tcp --dport 10000 -j SNAT --to-source 192.168.1.222 Iptables 规则的多种匹配条件 在 iptables 中规则是以 IP 地址为基本匹配条件的，其中 -s 表示源地址，-d 表示目的地址，除了 IP 地址， iptables 还有很多匹配条件，包括端口，传输协议，数据包状态等。比如可以使用 -p 指定传输协议；使用 -i 指定流入数据的网卡接口；使用 -o 指定流出数据的网卡接口。\n以上还是比较单一的匹配条件，可以通过扩展模块来加强这些单一条件，常用的模块有 iprange ，string ，time ，limit ，connlimit ，state 等。\niprange 模块用来指定一定范围内的 IP 地址，具体用例为 -m iprange …","date":1583011425,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728727189,"objectID":"879cf8189578d207c22ae83d0ea6fc22","permalink":"https://yuweizzz.github.io/post/knowledge_about_iptables/","publishdate":"2020-02-29T21:23:45Z","relpermalink":"/post/knowledge_about_iptables/","section":"post","summary":"这篇笔记用来记录 Linux 中 iptables 的相关知识。\n","tags":["iptables","Linux"],"title":"Iptables 知识笔记","type":"post"}]